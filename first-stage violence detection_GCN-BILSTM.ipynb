{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1OJegEOhX5oaXTQ0WYxUrq9kRYet2IIje","timestamp":1760263765733},{"file_id":"1qq1o_yePawM04BgalHbjkOydlQJngBFi","timestamp":1759729626377},{"file_id":"1GRjhnros6tWcFOIfIiyFxIe-d-leVOn3","timestamp":1759709927615},{"file_id":"1YmHMYA580VdOcUW-NtR-fvrDjQyHK0Y8","timestamp":1759668927803},{"file_id":"18-Rxc55UNYYy2I9TZYtTkKgI4GKJZaJ3","timestamp":1759631300669},{"file_id":"1neVWGgqwbfX4L7K47xXUTgkbNEiN8fg4","timestamp":1759064353164}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"OEDRhuTlUEJ6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764561824210,"user_tz":-540,"elapsed":17480,"user":{"displayName":"우수정","userId":"10597797283932636405"}},"outputId":"3116ea36-9369-4ee3-e0f9-59b105abf07e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","source":["!pip install kaggle\n"],"metadata":{"id":"SkG_NWPWXwo4","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","os.environ['KAGGLE_CONFIG_DIR'] = '/content/drive/MyDrive/kaggle'\n"],"metadata":{"id":"rtsj9ONVYFBG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 데이터셋 준비"],"metadata":{"id":"hu7P_m_uOhIt"}},{"cell_type":"code","source":["# # !kaggle datasets download -d\n","# toluwaniaremu/smartcity-cctv-violence-detection-dataset-scvd -p\n","# /content/drive/MyDrive/datasets\n"],"metadata":{"id":"6X-GFTq_YtfF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # !unzip\n","# /content/drive/MyDrive/datasets/smartcity-cctv-violence-detection-dataset-scvd.zip\n","# -d /content/drive/MyDrive/datasets/scvd\n"],"metadata":{"id":"C98TVhVk3GTl","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dir = \"/content/drive/MyDrive/datasets/scvd/SCVD/SCVD_converted/Train\"\n","test_dir = \"/content/drive/MyDrive/datasets/scvd/SCVD/SCVD_converted/Test\"\n","\n","import os\n","print(\"Train 폴더:\", os.listdir(train_dir))\n","print(\"Test 폴더:\", os.listdir(test_dir))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yc0qeMpXNsQ8","executionInfo":{"status":"ok","timestamp":1764561825764,"user_tz":-540,"elapsed":1524,"user":{"displayName":"우수정","userId":"10597797283932636405"}},"outputId":"c977d75f-3a9e-4b59-d37a-5595a6ea7ba1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train 폴더: ['Normal', 'Violence', 'Weaponized']\n","Test 폴더: ['Normal', 'Violence', 'Weaponized']\n"]}]},{"cell_type":"code","source":["import os\n","\n","folder_path = '/content/drive/MyDrive/datasets'\n","print(os.listdir(folder_path))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9v6J3Bwf78Ur","executionInfo":{"status":"ok","timestamp":1764561831556,"user_tz":-540,"elapsed":19,"user":{"displayName":"우수정","userId":"10597797283932636405"}},"outputId":"e8c3b721-a20e-49d4-f3bf-3592fc0beede"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['smartcity-cctv-violence-detection-dataset-scvd.zip', 'scvd', 'skeleton_dataset', 'UCF_Crime_Dataset.zip', 'ucf_crime', 'skeleton_dataset_hybrid']\n"]}]},{"cell_type":"code","source":["!unzip -o \"/content/drive/MyDrive/datasets/UCF_Crime_Dataset.zip\" \"Test/Abuse/*\" -d \"/content/drive/MyDrive/datasets/ucf_crime\"\n","!unzip -o \"/content/drive/MyDrive/datasets/UCF_Crime_Dataset.zip\" \"Test/Assault/*\" -d \"/content/drive/MyDrive/datasets/ucf_crime\"\n","!unzip -o \"/content/drive/MyDrive/datasets/UCF_Crime_Dataset.zip\" \"Test/Fighting/*\" -d \"/content/drive/MyDrive/datasets/ucf_crime\"\n"],"metadata":{"collapsed":true,"id":"Nj_rc9DqEcdP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!unzip -o \"/content/drive/MyDrive/datasets/UCF_Crime_Dataset.zip\" \"Train/Abuse/*\" -d \"/content/drive/MyDrive/datasets/ucf_crime\"\n","!unzip -o \"/content/drive/MyDrive/datasets/UCF_Crime_Dataset.zip\" \"Train/Assault/*\" -d \"/content/drive/MyDrive/datasets/ucf_crime\"\n","!unzip -o \"/content/drive/MyDrive/datasets/UCF_Crime_Dataset.zip\" \"Train/Fighting/*\" -d \"/content/drive/MyDrive/datasets/ucf_crime\"\n"],"metadata":{"collapsed":true,"id":"d1EfkKUnEdIv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dir = \"/content/drive/MyDrive/datasets/ucf_crime/Train\"\n","test_dir = \"/content/drive/MyDrive/datasets/ucf_crime/Test\"\n","\n","import os\n","print(\"Train 폴더:\", os.listdir(train_dir))\n","print(\"Test 폴더:\", os.listdir(test_dir))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LMYnaqOlJzNP","executionInfo":{"status":"ok","timestamp":1764561834766,"user_tz":-540,"elapsed":405,"user":{"displayName":"우수정","userId":"10597797283932636405"}},"outputId":"c5485a94-1e8a-4f06-c387-8322a25eda35"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train 폴더: ['Abuse', 'Assault', 'Fighting']\n","Test 폴더: ['Abuse', 'Assault', 'Fighting']\n"]}]},{"cell_type":"markdown","source":["# 스켈레톤 추출"],"metadata":{"id":"f_CrbxtpSoyW"}},{"cell_type":"code","source":["!pip install mediapipe\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"yV3J0cOYQ3Oq","executionInfo":{"status":"ok","timestamp":1764561871814,"user_tz":-540,"elapsed":34888,"user":{"displayName":"우수정","userId":"10597797283932636405"}},"outputId":"f6cb4ba4-58f4-46f1-ee75-507253d4aab1","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting mediapipe\n","  Downloading mediapipe-0.10.21-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from mediapipe) (1.4.0)\n","Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.4.0)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.9.23)\n","Requirement already satisfied: jax in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.7.2)\n","Requirement already satisfied: jaxlib in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.7.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from mediapipe) (3.10.0)\n","Collecting numpy<2 (from mediapipe)\n","  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.12/dist-packages (from mediapipe) (4.12.0.88)\n","Collecting protobuf<5,>=4.25.3 (from mediapipe)\n","  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n","Collecting sounddevice>=0.4.4 (from mediapipe)\n","  Downloading sounddevice-0.5.3-py3-none-any.whl.metadata (1.6 kB)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.2.1)\n","Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.12/dist-packages (from sounddevice>=0.4.4->mediapipe) (2.0.0)\n","Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (0.5.4)\n","INFO: pip is looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n","Collecting jax (from mediapipe)\n","  Downloading jax-0.8.1-py3-none-any.whl.metadata (13 kB)\n","Collecting jaxlib (from mediapipe)\n","  Downloading jaxlib-0.8.1-cp312-cp312-manylinux_2_27_x86_64.whl.metadata (1.3 kB)\n","Collecting jax (from mediapipe)\n","  Downloading jax-0.8.0-py3-none-any.whl.metadata (13 kB)\n","Collecting jaxlib (from mediapipe)\n","  Downloading jaxlib-0.8.0-cp312-cp312-manylinux_2_27_x86_64.whl.metadata (1.3 kB)\n","Collecting jax (from mediapipe)\n","  Downloading jax-0.7.1-py3-none-any.whl.metadata (13 kB)\n","Collecting jaxlib (from mediapipe)\n","  Downloading jaxlib-0.7.1-cp312-cp312-manylinux_2_27_x86_64.whl.metadata (1.3 kB)\n","Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (3.4.0)\n","Requirement already satisfied: scipy>=1.12 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (1.16.3)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (1.4.9)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (25.0)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (3.2.5)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (2.9.0.post0)\n","INFO: pip is looking at multiple versions of opencv-contrib-python to determine which version is compatible with other requirements. This could take a while.\n","Collecting opencv-contrib-python (from mediapipe)\n","  Downloading opencv_contrib_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.23)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n","Downloading mediapipe-0.10.21-cp312-cp312-manylinux_2_28_x86_64.whl (35.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sounddevice-0.5.3-py3-none-any.whl (32 kB)\n","Downloading jax-0.7.1-py3-none-any.whl (2.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m109.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jaxlib-0.7.1-cp312-cp312-manylinux_2_27_x86_64.whl (81.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.2/81.2 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opencv_contrib_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (69.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: protobuf, numpy, sounddevice, opencv-contrib-python, jaxlib, jax, mediapipe\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 5.29.5\n","    Uninstalling protobuf-5.29.5:\n","      Successfully uninstalled protobuf-5.29.5\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 2.0.2\n","    Uninstalling numpy-2.0.2:\n","      Successfully uninstalled numpy-2.0.2\n","  Attempting uninstall: opencv-contrib-python\n","    Found existing installation: opencv-contrib-python 4.12.0.88\n","    Uninstalling opencv-contrib-python-4.12.0.88:\n","      Successfully uninstalled opencv-contrib-python-4.12.0.88\n","  Attempting uninstall: jaxlib\n","    Found existing installation: jaxlib 0.7.2\n","    Uninstalling jaxlib-0.7.2:\n","      Successfully uninstalled jaxlib-0.7.2\n","  Attempting uninstall: jax\n","    Found existing installation: jax 0.7.2\n","    Uninstalling jax-0.7.2:\n","      Successfully uninstalled jax-0.7.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n","opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n","pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n","ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n","opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n","opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\n","shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed jax-0.7.1 jaxlib-0.7.1 mediapipe-0.10.21 numpy-1.26.4 opencv-contrib-python-4.11.0.86 protobuf-4.25.8 sounddevice-0.5.3\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["google","numpy"]},"id":"a9327dafdef7434e937a0f08bb8bf09f"}},"metadata":{}}]},{"cell_type":"code","source":["!pip install mediapipe opencv-python tqdm\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"-EeJNxXIRv01","outputId":"0da102d8-fd3a-422a-fc71-19f6c01ed240"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: mediapipe in /usr/local/lib/python3.12/dist-packages (0.10.21)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from mediapipe) (1.4.0)\n","Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.4.0)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.9.23)\n","Requirement already satisfied: jax in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.7.1)\n","Requirement already satisfied: jaxlib in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.7.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from mediapipe) (3.10.0)\n","Requirement already satisfied: numpy<2 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (1.26.4)\n","Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.12/dist-packages (from mediapipe) (4.11.0.86)\n","Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (4.25.8)\n","Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.5.3)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.2.1)\n","INFO: pip is looking at multiple versions of opencv-python to determine which version is compatible with other requirements. This could take a while.\n","Collecting opencv-python\n","  Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n","Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.12/dist-packages (from sounddevice>=0.4.4->mediapipe) (2.0.0)\n","Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (0.5.4)\n","Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (3.4.0)\n","Requirement already satisfied: scipy>=1.12 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (1.16.3)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (1.4.9)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (25.0)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (3.2.5)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (2.9.0.post0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.23)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n","Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (63.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n","\u001b[0m^C\n"]}]},{"cell_type":"code","source":["!pip install ultralytics\n"],"metadata":{"collapsed":true,"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"uNFZtHuu9Wg3","executionInfo":{"status":"ok","timestamp":1764561904708,"user_tz":-540,"elapsed":20635,"user":{"displayName":"우수정","userId":"10597797283932636405"}},"outputId":"1d6ab5f3-1498-49ba-ace2-75d326e5794f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ultralytics\n","  Downloading ultralytics-8.3.233-py3-none-any.whl.metadata (37 kB)\n","Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.26.4)\n","Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n","Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.12.0.88)\n","Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.3)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.3)\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.9.0+cu126)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.24.0+cu126)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: polars>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.31.0)\n","Collecting ultralytics-thop>=2.0.18 (from ultralytics)\n","  Downloading ultralytics_thop-2.0.18-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.5)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n","Collecting numpy>=1.23.0 (from ultralytics)\n","  Downloading numpy-2.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2025.11.12)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.20.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.6)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.3)\n","Downloading ultralytics-8.3.233-py3-none-any.whl (1.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading numpy-2.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ultralytics_thop-2.0.18-py3-none-any.whl (28 kB)\n","Installing collected packages: numpy, ultralytics-thop, ultralytics\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.26.4\n","    Uninstalling numpy-1.26.4:\n","      Successfully uninstalled numpy-1.26.4\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","mediapipe 0.10.21 requires numpy<2, but you have numpy 2.2.6 which is incompatible.\n","numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\n","tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\n","ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed numpy-2.2.6 ultralytics-8.3.233 ultralytics-thop-2.0.18\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy"]},"id":"07bca47225814f7d8c1ea41325c23e05"}},"metadata":{}}]},{"cell_type":"code","source":["import mediapipe as mp\n","import numpy as np\n","\n","# Mediapipe와 NumPy 버전 출력\n","print(f\"Mediapipe 버전: {mp.__version__}\")\n","print(f\"NumPy 버전: {np.__version__}\")\n"],"metadata":{"id":"z0JKt99FPRus","executionInfo":{"status":"ok","timestamp":1764561929289,"user_tz":-540,"elapsed":13903,"user":{"displayName":"우수정","userId":"10597797283932636405"}},"outputId":"ecc297b7-0104-40e7-8dd7-ba596bf06a34","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/jaxlib/plugin_support.py:71: RuntimeWarning: JAX plugin jax_cuda12_plugin version 0.7.2 is installed, but it is not compatible with the installed jaxlib version 0.7.1, so it will not be used.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Mediapipe 버전: 0.10.21\n","NumPy 버전: 2.2.6\n"]}]},{"cell_type":"code","source":["import os\n","import cv2\n","import numpy as np\n","from tqdm import tqdm\n","import mediapipe as mp\n","from ultralytics import YOLO\n","\n","# ==========================================\n","#설정\n","# ==========================================\n","input_dirs = {\n","    \"Normal\": \"/content/drive/MyDrive/datasets/scvd/SCVD/SCVD_converted/Train/Normal\",\n","    \"Violence\": \"/content/drive/MyDrive/datasets/scvd/SCVD/SCVD_converted/Train/Violence\",\n","}\n","\n","output_root = \"/content/drive/MyDrive/datasets/skeleton_dataset_hybrid/Train\"\n","os.makedirs(output_root, exist_ok=True)\n","\n","# ==========================================\n","# 모델 초기화\n","# ==========================================\n","yolo = YOLO(\"yolov8n.pt\")  # 작은 모델 (빠름)\n","mp_pose = mp.solutions.pose\n","pose_detector = mp_pose.Pose(\n","    static_image_mode=False,\n","    model_complexity=2,                 # 정확도 높임\n","    enable_segmentation=False,\n","    min_detection_confidence=0.2,\n","    min_tracking_confidence=0.2\n",")\n","\n","# ==========================================\n","# 전처리 함수 (어두운 프레임용)\n","# ==========================================\n","def enhance_brightness(frame):\n","    # YCrCb 변환으로 밝기 조절\n","    ycrcb = cv2.cvtColor(frame, cv2.COLOR_BGR2YCrCb)\n","    y, cr, cb = cv2.split(ycrcb)\n","    y = cv2.equalizeHist(y)  # 밝기 균일화\n","    merged = cv2.merge([y, cr, cb])\n","    frame_enhanced = cv2.cvtColor(merged, cv2.COLOR_YCrCb2BGR)\n","\n","    # 대비·밝기 미세 조정\n","    frame_enhanced = cv2.convertScaleAbs(frame_enhanced, alpha=1.4, beta=20)\n","    return frame_enhanced\n","\n","# ==========================================\n","# 스켈레톤 추출 함수\n","# ==========================================\n","def extract_skeleton_from_video(video_path, num_frames=32):\n","    cap = cv2.VideoCapture(video_path)\n","    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","    if total_frames == 0:\n","        print(f\"영상 손상 또는 프레임 없음: {video_path}\")\n","        return np.zeros((num_frames, 33, 4), dtype=np.float32)\n","\n","    frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=np.int32)\n","    sequence = []\n","\n","    for idx in frame_indices:\n","        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n","        ret, frame = cap.read()\n","        if not ret or frame is None:\n","            sequence.append(np.zeros((33, 4)))\n","            continue\n","\n","        # 어두운 장면 보정\n","        frame = enhance_brightness(frame)\n","\n","        # YOLO 탐지\n","        results = yolo(frame, verbose=False)\n","        if len(results[0].boxes) == 0:\n","            sequence.append(np.zeros((33, 4)))  # 사람 없음\n","            continue\n","\n","        # 가장 큰 사람 선택\n","        boxes = results[0].boxes.xyxy.cpu().numpy()\n","        areas = [(x2 - x1) * (y2 - y1) for x1, y1, x2, y2 in boxes]\n","        main_box = boxes[np.argmax(areas)]\n","        x1, y1, x2, y2 = map(int, main_box)\n","\n","        # crop & 보정\n","        person_crop = frame[max(0, y1):y2, max(0, x1):x2]\n","        if person_crop.size == 0:\n","            sequence.append(np.zeros((33, 4)))\n","            continue\n","\n","        # MediaPipe Pose 처리\n","        rgb = cv2.cvtColor(person_crop, cv2.COLOR_BGR2RGB)\n","        result = pose_detector.process(rgb)\n","\n","        if result.pose_landmarks:\n","            landmarks = result.pose_landmarks.landmark\n","            coords = np.array([[lm.x, lm.y, lm.z, lm.visibility] for lm in landmarks], dtype=np.float32)\n","            # 정규화된 좌표를 [-1, 1]로 스케일링\n","            coords[:, 0] = (coords[:, 0] - 0.5) * 2\n","            coords[:, 1] = (coords[:, 1] - 0.5) * 2\n","        else:\n","            coords = np.zeros((33, 4), dtype=np.float32)\n","\n","        sequence.append(coords)\n","\n","    cap.release()\n","    return np.array(sequence, dtype=np.float32)\n","\n","# ==========================================\n","# 전체 처리 루프\n","# ==========================================\n","for label, input_dir in input_dirs.items():\n","    output_dir = os.path.join(output_root, label)\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    video_files = [f for f in os.listdir(input_dir) if f.lower().endswith(('.mp4', '.avi', '.mov'))]\n","    print(f\"\\n=== [{label}] {len(video_files)}개 영상 처리 시작 ===\")\n","\n","    for video_file in tqdm(video_files, desc=f\"{label} 스켈레톤 추출\"):\n","        video_path = os.path.join(input_dir, video_file)\n","        save_name = os.path.splitext(video_file)[0] + \".npy\"\n","        save_path = os.path.join(output_dir, save_name)\n","\n","        if os.path.exists(save_path):\n","            continue\n","\n","        skeleton = extract_skeleton_from_video(video_path, num_frames=32)\n","        np.save(save_path, skeleton)\n","\n","    print(f\"{label} 완료 → {output_dir}\")\n","\n","pose_detector.close()\n","print(\"\\nYOLO+MediaPipe 하이브리드 스켈레톤 추출 완료!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vxFfF4DRRjuG","executionInfo":{"status":"ok","timestamp":1764561978626,"user_tz":-540,"elapsed":8275,"user":{"displayName":"우수정","userId":"10597797283932636405"}},"outputId":"ff92320f-a11d-45cb-e15f-325f5af22a2f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Creating new Ultralytics Settings v0.0.6 file ✅ \n","View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n","Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n","\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt': 100% ━━━━━━━━━━━━ 6.2MB 157.3MB/s 0.0s\n","Downloading model to /usr/local/lib/python3.12/dist-packages/mediapipe/modules/pose_landmark/pose_landmark_heavy.tflite\n","\n","=== [Normal] 200개 영상 처리 시작 ===\n"]},{"output_type":"stream","name":"stderr","text":["Normal 스켈레톤 추출: 100%|██████████| 200/200 [00:01<00:00, 198.60it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Normal 완료 → /content/drive/MyDrive/datasets/skeleton_dataset_hybrid/Train/Normal\n","\n","=== [Violence] 99개 영상 처리 시작 ===\n"]},{"output_type":"stream","name":"stderr","text":["Violence 스켈레톤 추출: 100%|██████████| 99/99 [00:00<00:00, 4028.56it/s]"]},{"output_type":"stream","name":"stdout","text":["Violence 완료 → /content/drive/MyDrive/datasets/skeleton_dataset_hybrid/Train/Violence\n","\n","YOLO+MediaPipe 하이브리드 스켈레톤 추출 완료!\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["import os\n","import cv2\n","import numpy as np\n","from tqdm import tqdm\n","import mediapipe as mp\n","from ultralytics import YOLO\n","\n","# ==========================================\n","# 설정\n","# ==========================================\n","input_dirs = {\n","    \"Normal\": \"/content/drive/MyDrive/datasets/scvd/SCVD/SCVD_converted/Test/Normal\",\n","    \"Violence\": \"/content/drive/MyDrive/datasets/scvd/SCVD/SCVD_converted/Test/Violence\",\n","}\n","\n","output_root = \"/content/drive/MyDrive/datasets/skeleton_dataset_hybrid/Test\"\n","os.makedirs(output_root, exist_ok=True)\n","\n","# ==========================================\n","# 모델 초기화\n","# ==========================================\n","yolo = YOLO(\"yolov8n.pt\")  # 작은 모델 (빠름)\n","mp_pose = mp.solutions.pose\n","pose_detector = mp_pose.Pose(\n","    static_image_mode=False,\n","    model_complexity=2,                 # 정확도 높임\n","    enable_segmentation=False,\n","    min_detection_confidence=0.2,\n","    min_tracking_confidence=0.2\n",")\n","\n","# ==========================================\n","# 전처리 함수 (어두운 프레임용)\n","# ==========================================\n","def enhance_brightness(frame):\n","    # YCrCb 변환으로 밝기 조절\n","    ycrcb = cv2.cvtColor(frame, cv2.COLOR_BGR2YCrCb)\n","    y, cr, cb = cv2.split(ycrcb)\n","    y = cv2.equalizeHist(y)  # 밝기 균일화\n","    merged = cv2.merge([y, cr, cb])\n","    frame_enhanced = cv2.cvtColor(merged, cv2.COLOR_YCrCb2BGR)\n","\n","    # 대비·밝기 미세 조정\n","    frame_enhanced = cv2.convertScaleAbs(frame_enhanced, alpha=1.4, beta=20)\n","    return frame_enhanced\n","\n","# ==========================================\n","# 스켈레톤 추출 함수\n","# ==========================================\n","def extract_skeleton_from_video(video_path, num_frames=32):\n","    cap = cv2.VideoCapture(video_path)\n","    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","    if total_frames == 0:\n","        print(f\"영상 손상 또는 프레임 없음: {video_path}\")\n","        return np.zeros((num_frames, 33, 4), dtype=np.float32)\n","\n","    frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=np.int32)\n","    sequence = []\n","\n","    for idx in frame_indices:\n","        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n","        ret, frame = cap.read()\n","        if not ret or frame is None:\n","            sequence.append(np.zeros((33, 4)))\n","            continue\n","\n","        # 어두운 장면 보정\n","        frame = enhance_brightness(frame)\n","\n","        # YOLO 탐지\n","        results = yolo(frame, verbose=False)\n","        if len(results[0].boxes) == 0:\n","            sequence.append(np.zeros((33, 4)))  # 사람 없음\n","            continue\n","\n","        # 가장 큰 사람 선택\n","        boxes = results[0].boxes.xyxy.cpu().numpy()\n","        areas = [(x2 - x1) * (y2 - y1) for x1, y1, x2, y2 in boxes]\n","        main_box = boxes[np.argmax(areas)]\n","        x1, y1, x2, y2 = map(int, main_box)\n","\n","        # crop & 보정\n","        person_crop = frame[max(0, y1):y2, max(0, x1):x2]\n","        if person_crop.size == 0:\n","            sequence.append(np.zeros((33, 4)))\n","            continue\n","\n","        # MediaPipe Pose 처리\n","        rgb = cv2.cvtColor(person_crop, cv2.COLOR_BGR2RGB)\n","        result = pose_detector.process(rgb)\n","\n","        if result.pose_landmarks:\n","            landmarks = result.pose_landmarks.landmark\n","            coords = np.array([[lm.x, lm.y, lm.z, lm.visibility] for lm in landmarks], dtype=np.float32)\n","            # 정규화된 좌표를 [-1, 1]로 스케일링\n","            coords[:, 0] = (coords[:, 0] - 0.5) * 2\n","            coords[:, 1] = (coords[:, 1] - 0.5) * 2\n","        else:\n","            coords = np.zeros((33, 4), dtype=np.float32)\n","\n","        sequence.append(coords)\n","\n","    cap.release()\n","    return np.array(sequence, dtype=np.float32)\n","\n","# ==========================================\n","# 전체 처리 루프\n","# ==========================================\n","for label, input_dir in input_dirs.items():\n","    output_dir = os.path.join(output_root, label)\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    video_files = [f for f in os.listdir(input_dir) if f.lower().endswith(('.mp4', '.avi', '.mov'))]\n","    print(f\"\\n=== [{label}] {len(video_files)}개 영상 처리 시작 ===\")\n","\n","    for video_file in tqdm(video_files, desc=f\"{label} 스켈레톤 추출\"):\n","        video_path = os.path.join(input_dir, video_file)\n","        save_name = os.path.splitext(video_file)[0] + \".npy\"\n","        save_path = os.path.join(output_dir, save_name)\n","\n","        if os.path.exists(save_path):\n","            continue\n","\n","        skeleton = extract_skeleton_from_video(video_path, num_frames=32)\n","        np.save(save_path, skeleton)\n","\n","    print(f\"{label} 완료 → {output_dir}\")\n","\n","pose_detector.close()\n","print(\"\\nYOLO+MediaPipe 하이브리드 스켈레톤 추출 완료!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z3u4kgIkWM65","executionInfo":{"status":"ok","timestamp":1764561989742,"user_tz":-540,"elapsed":576,"user":{"displayName":"우수정","userId":"10597797283932636405"}},"outputId":"c463abdd-115f-4b40-dcac-62e31cda2dae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== [Normal] 46개 영상 처리 시작 ===\n"]},{"output_type":"stream","name":"stderr","text":["Normal 스켈레톤 추출: 100%|██████████| 46/46 [00:00<00:00, 151.75it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Normal 완료 → /content/drive/MyDrive/datasets/skeleton_dataset_hybrid/Test/Normal\n","\n","=== [Violence] 12개 영상 처리 시작 ===\n"]},{"output_type":"stream","name":"stderr","text":["Violence 스켈레톤 추출: 100%|██████████| 12/12 [00:00<00:00, 2689.66it/s]"]},{"output_type":"stream","name":"stdout","text":["Violence 완료 → /content/drive/MyDrive/datasets/skeleton_dataset_hybrid/Test/Violence\n","\n","YOLO+MediaPipe 하이브리드 스켈레톤 추출 완료!\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["import os\n","import cv2\n","import numpy as np\n","import mediapipe as mp\n","from tqdm import tqdm\n","from collections import defaultdict\n","from ultralytics import YOLO\n","\n","# ======================================\n","# 설정\n","# ======================================\n","base_input = \"/content/drive/MyDrive/datasets/ucf_crime\"\n","base_output = \"/content/drive/MyDrive/datasets/skeleton_dataset_hybrid\"\n","splits = [\"Train\", \"Test\"]\n","classes = [\"Fighting\", \"Abuse\", \"Assault\"]\n","num_frames = 32\n","\n","# ======================================\n","# 모델 초기화\n","# ======================================\n","yolo = YOLO(\"yolov8n.pt\")  # 작은 YOLOv8 모델\n","mp_pose = mp.solutions.pose\n","pose_detector = mp_pose.Pose(\n","    static_image_mode=True,     # 정지 이미지에 최적화\n","    model_complexity=2,\n","    enable_segmentation=False,\n","    min_detection_confidence=0.2,\n","    min_tracking_confidence=0.2\n",")\n","\n","# ======================================\n","# 어두운 프레임 밝기/대비 보정 함수\n","# ======================================\n","def enhance_brightness(frame):\n","    ycrcb = cv2.cvtColor(frame, cv2.COLOR_BGR2YCrCb)\n","    y, cr, cb = cv2.split(ycrcb)\n","    y = cv2.equalizeHist(y)\n","    frame = cv2.merge([y, cr, cb])\n","    frame = cv2.cvtColor(frame, cv2.COLOR_YCrCb2BGR)\n","    frame = cv2.convertScaleAbs(frame, alpha=1.4, beta=20)\n","    return frame\n","\n","# ======================================\n","# 스켈레톤 추출 함수\n","# ======================================\n","def extract_from_images(img_paths, num_frames=32):\n","    img_paths = sorted(img_paths)\n","    if len(img_paths) == 0:\n","        return None\n","\n","    indices = np.linspace(0, len(img_paths) - 1, num_frames, dtype=int)\n","    selected = [img_paths[i] for i in indices]\n","    sequence = []\n","\n","    for img_path in selected:\n","        image = cv2.imread(img_path)\n","        if image is None:\n","            sequence.append(np.zeros((33, 4)))\n","            continue\n","\n","        # 어두운 이미지 보정\n","        image = enhance_brightness(image)\n","\n","        # YOLO로 사람 탐지\n","        results = yolo(image, verbose=False)\n","        if len(results[0].boxes) == 0:\n","            sequence.append(np.zeros((33, 4)))\n","            continue\n","\n","        # 가장 큰 사람 bbox 선택\n","        boxes = results[0].boxes.xyxy.cpu().numpy()\n","        areas = [(x2 - x1) * (y2 - y1) for x1, y1, x2, y2 in boxes]\n","        x1, y1, x2, y2 = map(int, boxes[np.argmax(areas)])\n","        crop = image[max(0, y1):y2, max(0, x1):x2]\n","\n","        if crop.size == 0:\n","            sequence.append(np.zeros((33, 4)))\n","            continue\n","\n","        # MediaPipe Pose 처리\n","        rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n","        result = pose_detector.process(rgb)\n","\n","        if result.pose_landmarks:\n","            lm = result.pose_landmarks.landmark\n","            coords = np.array([[p.x, p.y, p.z, p.visibility] for p in lm], dtype=np.float32)\n","            # 정규화된 좌표를 [-1, 1]로 맞춤\n","            coords[:, 0] = (coords[:, 0] - 0.5) * 2\n","            coords[:, 1] = (coords[:, 1] - 0.5) * 2\n","        else:\n","            coords = np.zeros((33, 4), dtype=np.float32)\n","\n","        sequence.append(coords)\n","\n","    return np.array(sequence, dtype=np.float32)\n","\n","# ======================================\n","# 실행 루프 (prefix 그룹 기준)\n","# ======================================\n","for split in splits:\n","    for cls in classes:\n","        class_path = os.path.join(base_input, split, cls)\n","        out_dir = os.path.join(base_output, split, cls)\n","        os.makedirs(out_dir, exist_ok=True)\n","\n","        # PNG 파일 그룹화\n","        files = [f for f in os.listdir(class_path) if f.endswith(\".png\")]\n","        groups = defaultdict(list)\n","        for f in files:\n","            prefix = \"_\".join(f.split(\"_\")[:-1])  # ex) Fighting002_x264\n","            groups[prefix].append(os.path.join(class_path, f))\n","\n","        print(f\"\\n{split}/{cls}: {len(groups)}개 영상 샘플 처리 중...\")\n","\n","        for prefix, paths in tqdm(groups.items(), desc=f\"{split}-{cls}\"):\n","            save_path = os.path.join(out_dir, prefix + \".npy\")\n","            if os.path.exists(save_path):\n","                continue\n","\n","            skeleton = extract_from_images(paths, num_frames=num_frames)\n","            if skeleton is None:\n","                continue\n","\n","            np.save(save_path, skeleton)\n","\n","        print(f\"{split}/{cls} 완료 → {out_dir}\")\n","\n","pose_detector.close()\n","print(\"\\nPNG 기반 YOLO+MediaPipe 하이브리드 스켈레톤 추출 완료!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vxVdf5J2S8_7","outputId":"531ccf38-8219-4ca0-ef93-39ee2028e0e2","executionInfo":{"status":"ok","timestamp":1764562004877,"user_tz":-540,"elapsed":4057,"user":{"displayName":"우수정","userId":"10597797283932636405"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Train/Fighting: 6개 영상 샘플 처리 중...\n"]},{"output_type":"stream","name":"stderr","text":["Train-Fighting: 100%|██████████| 6/6 [00:00<00:00, 827.69it/s]"]},{"output_type":"stream","name":"stdout","text":["Train/Fighting 완료 → /content/drive/MyDrive/datasets/skeleton_dataset_hybrid/Train/Fighting\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Train/Abuse: 48개 영상 샘플 처리 중...\n"]},{"output_type":"stream","name":"stderr","text":["Train-Abuse: 100%|██████████| 48/48 [00:00<00:00, 2732.48it/s]"]},{"output_type":"stream","name":"stdout","text":["Train/Abuse 완료 → /content/drive/MyDrive/datasets/skeleton_dataset_hybrid/Train/Abuse\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Train/Assault: 47개 영상 샘플 처리 중...\n"]},{"output_type":"stream","name":"stderr","text":["Train-Assault: 100%|██████████| 47/47 [00:00<00:00, 4403.71it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train/Assault 완료 → /content/drive/MyDrive/datasets/skeleton_dataset_hybrid/Train/Assault\n","\n","Test/Fighting: 5개 영상 샘플 처리 중...\n"]},{"output_type":"stream","name":"stderr","text":["Test-Fighting: 100%|██████████| 5/5 [00:00<00:00, 1634.82it/s]"]},{"output_type":"stream","name":"stdout","text":["Test/Fighting 완료 → /content/drive/MyDrive/datasets/skeleton_dataset_hybrid/Test/Fighting\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test/Abuse: 2개 영상 샘플 처리 중...\n"]},{"output_type":"stream","name":"stderr","text":["Test-Abuse: 100%|██████████| 2/2 [00:00<00:00, 832.20it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test/Abuse 완료 → /content/drive/MyDrive/datasets/skeleton_dataset_hybrid/Test/Abuse\n","\n","Test/Assault: 3개 영상 샘플 처리 중...\n"]},{"output_type":"stream","name":"stderr","text":["Test-Assault: 100%|██████████| 3/3 [00:00<00:00, 850.02it/s]"]},{"output_type":"stream","name":"stdout","text":["Test/Assault 완료 → /content/drive/MyDrive/datasets/skeleton_dataset_hybrid/Test/Assault\n","\n","PNG 기반 YOLO+MediaPipe 하이브리드 스켈레톤 추출 완료!\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","\n","base_path = \"/content/drive/MyDrive/datasets/skeleton_dataset_hybrid\"\n","splits = [\"Train\", \"Test\"]\n","violence_classes = [\"Violence\", \"Fighting\", \"Abuse\", \"Assault\"]\n","\n","rows = []\n","\n","for split in splits:\n","    for cls in os.listdir(os.path.join(base_path, split)):\n","        label = 1 if cls in violence_classes else 0\n","        cls_path = os.path.join(base_path, split, cls)\n","        files = [f for f in os.listdir(cls_path) if f.endswith(\".npy\")]\n","        for f in files:\n","            rows.append({\n","                \"path\": os.path.join(cls_path, f),\n","                \"label\": label,\n","                \"split\": split\n","            })\n","\n","df = pd.DataFrame(rows)\n","csv_path = \"/content/drive/MyDrive/datasets/skeleton_dataset_hybrid/metadata.csv\"\n","df.to_csv(csv_path, index=False)\n","\n","print(f\"메타데이터 CSV 생성 완료 → {csv_path}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LzrgpT1IMD7-","executionInfo":{"status":"ok","timestamp":1764562010929,"user_tz":-540,"elapsed":4249,"user":{"displayName":"우수정","userId":"10597797283932636405"}},"outputId":"a65fed3e-af7e-44c2-f19d-2c013435a18e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["메타데이터 CSV 생성 완료 → /content/drive/MyDrive/datasets/skeleton_dataset_hybrid/metadata.csv\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","# metadata.csv 로드\n","df = pd.read_csv(\"/content/drive/MyDrive/datasets/skeleton_dataset_hybrid/metadata.csv\")\n","\n","# 라벨별 샘플 개수 확인\n","label_counts = df[\"label\"].value_counts()\n","print(\"라벨별 개수:\")\n","print(label_counts)\n","\n","# 클래스 이름으로 보기 쉽게 출력 (0: Normal, 1: Violence)\n","print(\"\\nNormal:\", label_counts.get(0, 0))\n","print(\"Violence:\", label_counts.get(1, 0))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_-GeHeVD8ESb","executionInfo":{"status":"ok","timestamp":1764562012375,"user_tz":-540,"elapsed":420,"user":{"displayName":"우수정","userId":"10597797283932636405"}},"outputId":"8091a432-abf1-4452-86a6-b41fd5c3316c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["라벨별 개수:\n","label\n","0    246\n","1    222\n","Name: count, dtype: int64\n","\n","Normal: 246\n","Violence: 222\n"]}]},{"cell_type":"markdown","source":["# 모델학습(GCN_LSTM)"],"metadata":{"id":"COlSYU9lxACk"}},{"cell_type":"code","source":["import os\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n","from sklearn.metrics import classification_report"],"metadata":{"id":"yxY8HYUjcNaF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#관절 연결 정의\n","JOINT_CONNECTIONS = [\n","    (0, 1), (1, 2), (2, 3), (3, 7),\n","    (0, 4), (4, 5), (5, 6), (6, 8),\n","    (9, 10), (11, 12), (11, 13), (13, 15), (12, 14), (14, 16)\n","]\n","\n","def get_adjacency_matrix(num_joints=33):\n","    A = torch.eye(num_joints)\n","    for i, j in JOINT_CONNECTIONS:\n","        A[i, j] = 1\n","        A[j, i] = 1\n","    D_inv_sqrt = torch.diag(torch.pow(A.sum(1), -0.5))\n","    return D_inv_sqrt @ A @ D_inv_sqrt\n","\n","#GCN Layer\n","class GraphConv(nn.Module):\n","    def __init__(self, in_features, out_features, A):\n","        super().__init__()\n","        self.A = A\n","        self.fc = nn.Linear(in_features, out_features)\n","\n","    def forward(self, x):\n","        Ax = torch.einsum('ij,bjc->bic', self.A, x)\n","        return F.relu(self.fc(Ax))\n","\n","#GCN-LSTM 모델\n","class GCN_LSTM(nn.Module):\n","    def __init__(self, num_joints=33, in_features=4, gcn_hidden=64, lstm_hidden=128):\n","        super().__init__()\n","        self.A = get_adjacency_matrix(num_joints).to(torch.float32)\n","        self.gcn1 = GraphConv(in_features, gcn_hidden, self.A)\n","        self.gcn2 = GraphConv(gcn_hidden, gcn_hidden, self.A)\n","\n","        #BiLSTM\n","        self.lstm = nn.LSTM(\n","            gcn_hidden * num_joints, lstm_hidden,\n","            batch_first=True, bidirectional=True\n","        )\n","\n","        #출력 차원 2배 → 양방향\n","        self.fc = nn.Linear(lstm_hidden * 2, 1)\n","\n","    def forward(self, x):  # (B, T, J, C)\n","        B, T, J, C = x.shape\n","        feats = []\n","        for t in range(T):\n","            xt = x[:, t, :, :]  # (B, J, C)\n","            h = self.gcn1(xt)\n","            h = self.gcn2(h)\n","            feats.append(h.view(B, -1))  # (B, J*C)\n","        x_seq = torch.stack(feats, dim=1)  # (B, T, J*C)\n","\n","        _, (hn, _) = self.lstm(x_seq)  # hn: (2, B, H)\n","        hn_cat = torch.cat((hn[0], hn[1]), dim=1)  # (B, H*2)\n","        return self.fc(hn_cat).squeeze(1)  # logit\n"],"metadata":{"id":"78BMrX-f7At_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SkeletonDataset(Dataset):\n","    def __init__(self, dataframe):\n","        self.df = dataframe.reset_index(drop=True)\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        row = self.df.iloc[idx]\n","        data = np.load(row[\"path\"])  # (32, 33, 4)\n","        label = row[\"label\"]\n","        return torch.tensor(data, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)\n"],"metadata":{"id":"9wLU-VtsAlQR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Focal Loss 정의\n","import torch\n","import torch.nn as nn\n","\n","class FocalLoss(nn.Module):\n","    def __init__(self, alpha=0.25, gamma=2.0, reduction=\"mean\"):\n","        super(FocalLoss, self).__init__()\n","        self.alpha = alpha\n","        self.gamma = gamma\n","        self.reduction = reduction\n","\n","    def forward(self, logits, targets):\n","        BCE_loss = nn.functional.binary_cross_entropy_with_logits(logits, targets, reduction=\"none\")\n","        pt = torch.exp(-BCE_loss)\n","        focal_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n","\n","        if self.reduction == \"mean\":\n","            return focal_loss.mean()\n","        elif self.reduction == \"sum\":\n","            return focal_loss.sum()\n","        else:\n","            return focal_loss\n"],"metadata":{"id":"gfgp9wZ0Kd5W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#CSV 로드\n","df = pd.read_csv(\"/content/drive/MyDrive/datasets/skeleton_dataset_hybrid/metadata.csv\")\n","train_df = df[df[\"split\"] == \"Train\"]\n","test_df = df[df[\"split\"] == \"Test\"]\n","\n","#pos_weight 계산\n","pos = train_df[\"label\"].sum()\n","neg = len(train_df) - pos\n","pos_weight = torch.tensor([neg / pos], dtype=torch.float32)\n","\n","#샘플링 가중치\n","counts = train_df[\"label\"].value_counts().to_dict()\n","weights = [1.0 / counts[l] for l in train_df[\"label\"]]\n","sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n","\n","#DataLoader\n","train_loader = DataLoader(SkeletonDataset(train_df), batch_size=16, sampler=sampler)\n","test_loader = DataLoader(SkeletonDataset(test_df), batch_size=16, shuffle=False)\n","\n","#학습 준비\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = GCN_LSTM().to(device)\n","# GraphConv 계층의 인접 행렬 A를 장치로 이동\n","model.gcn1.A = model.gcn1.A.to(device)\n","model.gcn2.A = model.gcn2.A.to(device)\n","criterion = FocalLoss(alpha=0.5, gamma=1.0)\n","\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n","\n","#학습 루프\n","best_loss = float('inf')  # 초기에 큰 값으로 설정\n","\n","for epoch in range(1, 31):\n","    model.train()\n","    running_loss = 0\n","\n","    for data, labels in train_loader:\n","        data, labels = data.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        logits = model(data)\n","        loss = criterion(logits, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","\n","    avg_loss = running_loss / len(train_loader)\n","    print(f\"Epoch {epoch:02d} | Loss: {avg_loss:.4f}\")\n","\n","    #현재까지 중 가장 낮은 Loss일 경우 저장\n","    if avg_loss < best_loss:\n","        best_loss = avg_loss\n","        torch.save(model.state_dict(), \"best_model_GCN-BILSTM.pth\")\n","        print(f\"Best model saved at Epoch {epoch:02d} | Loss: {avg_loss:.4f}\")\n"],"metadata":{"id":"kNh-cR48Amgo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764562334668,"user_tz":-540,"elapsed":163709,"user":{"displayName":"우수정","userId":"10597797283932636405"}},"outputId":"2401dc3a-1859-4ad1-c9ee-249db9e8ac88"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 01 | Loss: 0.1628\n","Best model saved at Epoch 01 | Loss: 0.1628\n","Epoch 02 | Loss: 0.1515\n","Best model saved at Epoch 02 | Loss: 0.1515\n","Epoch 03 | Loss: 0.1400\n","Best model saved at Epoch 03 | Loss: 0.1400\n","Epoch 04 | Loss: 0.1372\n","Best model saved at Epoch 04 | Loss: 0.1372\n","Epoch 05 | Loss: 0.1194\n","Best model saved at Epoch 05 | Loss: 0.1194\n","Epoch 06 | Loss: 0.1105\n","Best model saved at Epoch 06 | Loss: 0.1105\n","Epoch 07 | Loss: 0.1133\n","Epoch 08 | Loss: 0.1081\n","Best model saved at Epoch 08 | Loss: 0.1081\n","Epoch 09 | Loss: 0.1041\n","Best model saved at Epoch 09 | Loss: 0.1041\n","Epoch 10 | Loss: 0.0995\n","Best model saved at Epoch 10 | Loss: 0.0995\n","Epoch 11 | Loss: 0.0936\n","Best model saved at Epoch 11 | Loss: 0.0936\n","Epoch 12 | Loss: 0.0856\n","Best model saved at Epoch 12 | Loss: 0.0856\n","Epoch 13 | Loss: 0.0942\n","Epoch 14 | Loss: 0.0928\n","Epoch 15 | Loss: 0.0834\n","Best model saved at Epoch 15 | Loss: 0.0834\n","Epoch 16 | Loss: 0.0736\n","Best model saved at Epoch 16 | Loss: 0.0736\n","Epoch 17 | Loss: 0.0670\n","Best model saved at Epoch 17 | Loss: 0.0670\n","Epoch 18 | Loss: 0.0896\n","Epoch 19 | Loss: 0.0638\n","Best model saved at Epoch 19 | Loss: 0.0638\n","Epoch 20 | Loss: 0.0646\n","Epoch 21 | Loss: 0.0674\n","Epoch 22 | Loss: 0.0532\n","Best model saved at Epoch 22 | Loss: 0.0532\n","Epoch 23 | Loss: 0.0554\n","Epoch 24 | Loss: 0.0577\n","Epoch 25 | Loss: 0.0444\n","Best model saved at Epoch 25 | Loss: 0.0444\n","Epoch 26 | Loss: 0.0472\n","Epoch 27 | Loss: 0.0476\n","Epoch 28 | Loss: 0.0444\n","Best model saved at Epoch 28 | Loss: 0.0444\n","Epoch 29 | Loss: 0.0402\n","Best model saved at Epoch 29 | Loss: 0.0402\n","Epoch 30 | Loss: 0.0411\n"]}]},{"cell_type":"code","source":["import shutil\n","import os\n","\n","source_path = \"best_model_GCN-BILSTM.pth\"\n","\n","\n","target_dir = \"/content/drive/MyDrive/datasets/skeleton_dataset_hybrid\"\n","target_path = os.path.join(target_dir, \"best_model_GCN-BILSTM.pth\")\n","\n","if os.path.exists(source_path):\n","    os.makedirs(target_dir, exist_ok=True)\n","    shutil.copy(source_path, target_path)\n","    print(f\"모델을 드라이브로 옮겼습니다.\")\n","    print(f\"저장 위치: {target_path}\")\n","else:\n","    print(\"'best_model_GCN-BILSTM.pth' 파일을 찾을 수 없습니다. 학습이 제대로 돌았는지 확인해주세요.\")"],"metadata":{"id":"b17mkqjt1Q8i","executionInfo":{"status":"ok","timestamp":1764562339207,"user_tz":-540,"elapsed":985,"user":{"displayName":"우수정","userId":"10597797283932636405"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"5141d1e9-4fc7-4dc7-b7fd-1e4c66ce7b1e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["모델을 드라이브로 옮겼습니다.\n","저장 위치: /content/drive/MyDrive/datasets/skeleton_dataset_hybrid/best_model_GCN-BILSTM.pth\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import classification_report\n","\n","model.load_state_dict(torch.load(\"best_model_GCN-BILSTM.pth\"))\n","model.eval()\n","all_preds, all_labels = [], []\n","\n","with torch.no_grad():\n","    for data, labels in test_loader:\n","        data = data.to(device)\n","        logits = model(data)\n","        probs = torch.sigmoid(logits)\n","        preds = (probs > 0.5).long().cpu().numpy()\n","        all_preds.extend(preds)\n","        all_labels.extend(labels.numpy())\n","\n","print(\"\\nClassification Report:\")\n","print(classification_report(all_labels, all_preds, target_names=[\"Normal\", \"Violence\"]))\n"],"metadata":{"id":"w8tB4suEBLsP","executionInfo":{"status":"ok","timestamp":1764562374540,"user_tz":-540,"elapsed":33507,"user":{"displayName":"우수정","userId":"10597797283932636405"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"5b557ec6-0f68-41d8-cdf1-39a2294c173d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","      Normal       0.95      0.85      0.90        46\n","    Violence       0.74      0.91      0.82        22\n","\n","    accuracy                           0.87        68\n","   macro avg       0.85      0.88      0.86        68\n","weighted avg       0.88      0.87      0.87        68\n","\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","\n","cm = confusion_matrix(all_labels, all_preds)\n","ConfusionMatrixDisplay(cm, display_labels=[\"Normal\", \"Violence\"]).plot()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":467},"id":"Y0HqEW8wn226","executionInfo":{"status":"ok","timestamp":1764562374840,"user_tz":-540,"elapsed":302,"user":{"displayName":"우수정","userId":"10597797283932636405"}},"outputId":"ec6fd9c2-59aa-471c-c583-a1a291dee295"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7bf0d726c500>"]},"metadata":{},"execution_count":16},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 2 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":["# 단일 영상 테스트"],"metadata":{"id":"ArstEE9Syl1p"}},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","import mediapipe as mp\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from google.colab import files\n","from ultralytics import YOLO\n","\n","# 사용자로부터 영상 업로드\n","uploaded = files.upload()\n","\n","# 파일이 업로드되었는지 확인\n","if not uploaded:\n","    print(\"파일 업로드가 취소되었거나 파일이 선택되지 않았습니다. 다시 시도해주세요.\")\n","else:\n","    video_path = list(uploaded.keys())[0]\n","    print(f\"업로드 완료: {video_path}\")\n","\n","    # 관절 연결 정의 (GCN_LSTM 모델에 필요)\n","    JOINT_CONNECTIONS = [\n","        (0, 1), (1, 2), (2, 3), (3, 7),\n","        (0, 4), (4, 5), (5, 6), (6, 8),\n","        (9, 10), (11, 12), (11, 13), (13, 15), (12, 14), (14, 16)\n","    ]\n","\n","    def get_adjacency_matrix(num_joints=33):\n","        A = torch.eye(num_joints)\n","        for i, j in JOINT_CONNECTIONS:\n","            A[i, j] = 1\n","            A[j, i] = 1\n","        D_inv_sqrt = torch.diag(torch.pow(A.sum(1), -0.5))\n","        return D_inv_sqrt @ A @ D_inv_sqrt\n","\n","    # GCN Layer (GCN_LSTM 모델에 필요)\n","    class GraphConv(nn.Module):\n","        def __init__(self, in_features, out_features, A):\n","            super().__init__()\n","            self.A = A\n","            self.fc = nn.Linear(in_features, out_features)\n","\n","        def forward(self, x):\n","            Ax = torch.einsum('ij,bjc->bic', self.A, x)\n","            return F.relu(self.fc(Ax))\n","\n","    # GCN-LSTM 모델 (훈련된 모델과 동일한 아키텍처)\n","    class GCN_LSTM(nn.Module):\n","        def __init__(self, num_joints=33, in_features=4, gcn_hidden=64, lstm_hidden=128):\n","            super().__init__()\n","            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","            self.A = get_adjacency_matrix(num_joints).to(torch.float32).to(device)\n","            self.gcn1 = GraphConv(in_features, gcn_hidden, self.A)\n","            self.gcn2 = GraphConv(gcn_hidden, gcn_hidden, self.A)\n","            self.lstm = nn.LSTM(\n","                gcn_hidden * num_joints, lstm_hidden,\n","                batch_first=True, bidirectional=True\n","            )\n","            self.fc = nn.Linear(lstm_hidden * 2, 1)\n","\n","        def forward(self, x):  # (B, T, J, C)\n","            B, T, J, C = x.shape\n","            feats = []\n","            for t in range(T):\n","                xt = x[:, t, :, :]  # (B, J, C)\n","                h = self.gcn1(xt)\n","                h = self.gcn2(h)\n","                feats.append(h.view(B, -1))  # (B, J*C)\n","            x_seq = torch.stack(feats, dim=1)  # (B, T, J*C)\n","\n","            _, (hn, _) = self.lstm(x_seq)  # hn: (2, B, H)\n","            hn_cat = torch.cat((hn[0], hn[1]), dim=1)  # (B, H*2)\n","            return self.fc(hn_cat).squeeze(1)  # logit\n","\n","    # 전처리 함수 (어두운 프레임용 - 훈련 시와 동일)\n","    def enhance_brightness(frame):\n","        ycrcb = cv2.cvtColor(frame, cv2.COLOR_BGR2YCrCb)\n","        y, cr, cb = cv2.split(ycrcb)\n","        y = cv2.equalizeHist(y)  # 밝기 균일화\n","        merged = cv2.merge([y, cr, cb])\n","        frame_enhanced = cv2.cvtColor(merged, cv2.COLOR_YCrCb2BGR)\n","\n","        # 대비·밝기 미세 조정\n","        frame_enhanced = cv2.convertScaleAbs(frame_enhanced, alpha=1.4, beta=20)\n","        return frame_enhanced\n","\n","    # 스켈레톤 추출 함수 (32프레임 - 훈련 시와 동일하게 YOLO 및 밝기 보정 포함)\n","    def extract_skeleton_from_video(video_path, num_frames=32):\n","        cap = cv2.VideoCapture(video_path)\n","        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","        if total_frames == 0:\n","            print(f\"영상 손상 또는 프레임 없음: {video_path}\")\n","            return np.zeros((num_frames, 33, 4), dtype=np.float32)\n","\n","        frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=np.int32)\n","        sequence = []\n","\n","        yolo = YOLO(\"yolov8n.pt\") # YOLO 모델 로드 (훈련 시와 동일)\n","        mp_pose = mp.solutions.pose\n","        pose_detector = mp_pose.Pose(\n","            static_image_mode=False,\n","            model_complexity=2,                 # 정확도 높임\n","            enable_segmentation=False,\n","            min_detection_confidence=0.2,\n","            min_tracking_confidence=0.2\n","        )\n","\n","        for idx in frame_indices:\n","            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n","            ret, frame = cap.read()\n","            if not ret or frame is None:\n","                sequence.append(np.zeros((33, 4)))\n","                continue\n","\n","            # 어두운 장면 보정 (훈련 시와 동일)\n","            frame = enhance_brightness(frame)\n","\n","            # YOLO 탐지 (훈련 시와 동일)\n","            results = yolo(frame, verbose=False)\n","            if len(results[0].boxes) == 0:\n","                sequence.append(np.zeros((33, 4)))  # 사람 없음\n","                continue\n","\n","            # 가장 큰 사람 선택 (훈련 시와 동일)\n","            boxes = results[0].boxes.xyxy.cpu().numpy()\n","            areas = [(x2 - x1) * (y2 - y1) for x1, y1, x2, y2 in boxes]\n","            main_box = boxes[np.argmax(areas)]\n","            x1, y1, x2, y2 = map(int, main_box)\n","\n","            # crop & 보정 (훈련 시와 동일)\n","            person_crop = frame[max(0, y1):y2, max(0, x1):x2]\n","            if person_crop.size == 0:\n","                sequence.append(np.zeros((33, 4)))\n","                continue\n","\n","            # MediaPipe Pose 처리 (훈련 시와 동일)\n","            rgb = cv2.cvtColor(person_crop, cv2.COLOR_BGR2RGB)\n","            result = pose_detector.process(rgb)\n","\n","            if result.pose_landmarks:\n","                landmarks = result.pose_landmarks.landmark\n","                coords = np.array([[lm.x, lm.y, lm.z, lm.visibility] for lm in landmarks], dtype=np.float32)\n","                # 정규화된 좌표를 [-1, 1]로 스케일링 (훈련 시와 동일)\n","                coords[:, 0] = (coords[:, 0] - 0.5) * 2\n","                coords[:, 1] = (coords[:, 1] - 0.5) * 2\n","            else:\n","                coords = np.zeros((33, 4), dtype=np.float32)\n","\n","            sequence.append(coords)\n","\n","        cap.release()\n","        pose_detector.close()\n","        return np.array(sequence, dtype=np.float32) # shape: (32, 33, 4)\n","\n","    # 스켈레톤 추출\n","    skeleton = extract_skeleton_from_video(video_path)  # (32, 33, 4)\n","    tensor = torch.tensor(skeleton, dtype=torch.float32).unsqueeze(0) # 배치 차원 추가 (1, 32, 33, 4)\n","\n","    # 모델 불러오기 & 추론\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model = GCN_LSTM().to(device) # GCN_LSTM 모델 사용\n","    model.load_state_dict(torch.load(\"best_model_GCN-BILSTM.pth\")) # 올바른 모델 파일명 사용\n","    model.eval()\n","\n","    with torch.no_grad():\n","        logits = model(tensor.to(device))\n","        probs = torch.sigmoid(logits)\n","        prob = probs.item()\n","        label = 1 if prob > 0.5 else 0\n","\n","    # 출력\n","    print(f\"\\n예측 결과: {'폭력' if label == 1 else '정상'}\")\n","    print(f\"예측 확률: {prob:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":150},"id":"tNV4RVEhXjgp","outputId":"9d9a3f80-4cc5-45d3-fd03-36776ea7ac2b","executionInfo":{"status":"ok","timestamp":1764562429334,"user_tz":-540,"elapsed":45887,"user":{"displayName":"우수정","userId":"10597797283932636405"}}},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-151f894e-1dfa-430e-9228-36831f699c3a\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-151f894e-1dfa-430e-9228-36831f699c3a\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving dancing.mp4 to dancing.mp4\n","업로드 완료: dancing.mp4\n","\n","예측 결과: 정상\n","예측 확률: 0.0190\n"]}]}]}
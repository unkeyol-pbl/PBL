{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"559cbe04ee0e4c79a6cb24295e7ba27b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d6c13ceec2f64b838bc91a6ce99cda35","IPY_MODEL_c281d37569f54bc2beb63a8bbab73066","IPY_MODEL_8ccdeceb2e5e495c854e9b864cfedad5"],"layout":"IPY_MODEL_299258b6537647248084d9aa4b039002"}},"d6c13ceec2f64b838bc91a6ce99cda35":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0476ed0f5cae4c79b89131503975e43e","placeholder":"â€‹","style":"IPY_MODEL_6222ee79d0c94179948c29c889363338","value":"100%"}},"c281d37569f54bc2beb63a8bbab73066":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6ed4a2d18caa43f79ac6a244cf899b56","max":132,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9660b7104ff143739eea36e9ff6cc857","value":132}},"8ccdeceb2e5e495c854e9b864cfedad5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7bdd2ca21f9e4cb7be2d93d6414ce6a0","placeholder":"â€‹","style":"IPY_MODEL_0e07a4520c2a4c55a7dbfdcdcbb8619d","value":"â€‡132/132â€‡[03:41&lt;00:00,â€‡â€‡1.67s/it]"}},"299258b6537647248084d9aa4b039002":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0476ed0f5cae4c79b89131503975e43e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6222ee79d0c94179948c29c889363338":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6ed4a2d18caa43f79ac6a244cf899b56":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9660b7104ff143739eea36e9ff6cc857":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7bdd2ca21f9e4cb7be2d93d6414ce6a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e07a4520c2a4c55a7dbfdcdcbb8619d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# **í•„ìˆ˜ í•­ëª©**\n","-ì¬ì‹œì‘ í•˜ë¼ëŠ” ë©”ì‹œì§€ ëœ¨ë©´ ê·¸ëƒ¥ ì¬ì‹œì‘ í›„ ë‹¤ìŒ ë¸”ë¡ìœ¼ë¡œ ë„˜ì–´ê°€ì£¼ì„¸ìš”"],"metadata":{"id":"PD_h4irf2fEL"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"JkVtbQlrxWDV","outputId":"aa4e4f35-22be-4679-b9d1-be6090e8e1b6","executionInfo":{"status":"ok","timestamp":1764556170311,"user_tz":-540,"elapsed":82869,"user":{"displayName":"ì½”ì½”ëª½","userId":"10518974845406570474"}},"collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting numpy==1.26.4\n","  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n","\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/61.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting mediapipe==0.10.21\n","  Downloading mediapipe-0.10.21-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n","Collecting absl-py (from mediapipe==0.10.21)\n","  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n","Collecting attrs>=19.1.0 (from mediapipe==0.10.21)\n","  Downloading attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n","Collecting flatbuffers>=2.0 (from mediapipe==0.10.21)\n","  Downloading flatbuffers-25.9.23-py2.py3-none-any.whl.metadata (875 bytes)\n","Collecting jax (from mediapipe==0.10.21)\n","  Downloading jax-0.8.1-py3-none-any.whl.metadata (13 kB)\n","Collecting jaxlib (from mediapipe==0.10.21)\n","  Downloading jaxlib-0.8.1-cp312-cp312-manylinux_2_27_x86_64.whl.metadata (1.3 kB)\n","Collecting matplotlib (from mediapipe==0.10.21)\n","  Downloading matplotlib-3.10.7-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n","Collecting opencv-contrib-python (from mediapipe==0.10.21)\n","  Downloading opencv_contrib_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n","Collecting protobuf<5,>=4.25.3 (from mediapipe==0.10.21)\n","  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n","Collecting sounddevice>=0.4.4 (from mediapipe==0.10.21)\n","  Downloading sounddevice-0.5.3-py3-none-any.whl.metadata (1.6 kB)\n","Collecting sentencepiece (from mediapipe==0.10.21)\n","  Downloading sentencepiece-0.2.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n","Collecting CFFI>=1.0 (from sounddevice>=0.4.4->mediapipe==0.10.21)\n","  Downloading cffi-2.0.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.6 kB)\n","Collecting ml_dtypes>=0.5.0 (from jax->mediapipe==0.10.21)\n","  Downloading ml_dtypes-0.5.4-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n","INFO: pip is looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n","Collecting jax (from mediapipe==0.10.21)\n","  Downloading jax-0.8.0-py3-none-any.whl.metadata (13 kB)\n","Collecting jaxlib (from mediapipe==0.10.21)\n","  Downloading jaxlib-0.8.0-cp312-cp312-manylinux_2_27_x86_64.whl.metadata (1.3 kB)\n","Collecting jax (from mediapipe==0.10.21)\n","  Downloading jax-0.7.2-py3-none-any.whl.metadata (13 kB)\n","Collecting jaxlib (from mediapipe==0.10.21)\n","  Downloading jaxlib-0.7.2-cp312-cp312-manylinux_2_27_x86_64.whl.metadata (1.3 kB)\n","Collecting jax (from mediapipe==0.10.21)\n","  Downloading jax-0.7.1-py3-none-any.whl.metadata (13 kB)\n","Collecting jaxlib (from mediapipe==0.10.21)\n","  Downloading jaxlib-0.7.1-cp312-cp312-manylinux_2_27_x86_64.whl.metadata (1.3 kB)\n","Collecting opt_einsum (from jax->mediapipe==0.10.21)\n","  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n","Collecting scipy>=1.12 (from jax->mediapipe==0.10.21)\n","  Downloading scipy-1.16.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting contourpy>=1.0.1 (from matplotlib->mediapipe==0.10.21)\n","  Downloading contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n","Collecting cycler>=0.10 (from matplotlib->mediapipe==0.10.21)\n","  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n","Collecting fonttools>=4.22.0 (from matplotlib->mediapipe==0.10.21)\n","  Downloading fonttools-4.61.0-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (113 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m113.2/113.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib->mediapipe==0.10.21)\n","  Downloading kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n","Collecting packaging>=20.0 (from matplotlib->mediapipe==0.10.21)\n","  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n","Collecting pillow>=8 (from matplotlib->mediapipe==0.10.21)\n","  Downloading pillow-12.0.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.8 kB)\n","Collecting pyparsing>=3 (from matplotlib->mediapipe==0.10.21)\n","  Downloading pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n","Collecting python-dateutil>=2.7 (from matplotlib->mediapipe==0.10.21)\n","  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n","INFO: pip is looking at multiple versions of opencv-contrib-python to determine which version is compatible with other requirements. This could take a while.\n","Collecting opencv-contrib-python (from mediapipe==0.10.21)\n","  Downloading opencv_contrib_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n","Collecting pycparser (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe==0.10.21)\n","  Downloading pycparser-2.23-py3-none-any.whl.metadata (993 bytes)\n","Collecting six>=1.5 (from python-dateutil>=2.7->matplotlib->mediapipe==0.10.21)\n","  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n","Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading mediapipe-0.10.21-cp312-cp312-manylinux_2_28_x86_64.whl (35.6 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading attrs-25.4.0-py3-none-any.whl (67 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n","Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sounddevice-0.5.3-py3-none-any.whl (32 kB)\n","Downloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jax-0.7.1-py3-none-any.whl (2.8 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jaxlib-0.7.1-cp312-cp312-manylinux_2_27_x86_64.whl (81.2 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m81.2/81.2 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading matplotlib-3.10.7-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m111.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opencv_contrib_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (69.1 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sentencepiece-0.2.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading cffi-2.0.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (219 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m219.6/219.6 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (362 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m362.6/362.6 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n","Downloading fonttools-4.61.0-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (4.9 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m104.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.5 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ml_dtypes-0.5.4-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (5.0 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m101.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading packaging-25.0-py3-none-any.whl (66 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pillow-12.0.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (7.0 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m112.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m113.9/113.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading scipy-1.16.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.7 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m35.7/35.7 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n","Downloading pycparser-2.23-py3-none-any.whl (118 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m118.1/118.1 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: flatbuffers, six, sentencepiece, pyparsing, pycparser, protobuf, pillow, packaging, opt_einsum, numpy, kiwisolver, fonttools, cycler, attrs, absl-py, scipy, python-dateutil, opencv-contrib-python, ml_dtypes, contourpy, CFFI, sounddevice, matplotlib, jaxlib, jax, mediapipe\n","  Attempting uninstall: flatbuffers\n","    Found existing installation: flatbuffers 25.9.23\n","    Uninstalling flatbuffers-25.9.23:\n","      Successfully uninstalled flatbuffers-25.9.23\n","  Attempting uninstall: six\n","    Found existing installation: six 1.17.0\n","    Uninstalling six-1.17.0:\n","      Successfully uninstalled six-1.17.0\n","  Attempting uninstall: sentencepiece\n","    Found existing installation: sentencepiece 0.2.1\n","    Uninstalling sentencepiece-0.2.1:\n","      Successfully uninstalled sentencepiece-0.2.1\n","  Attempting uninstall: pyparsing\n","    Found existing installation: pyparsing 3.2.5\n","    Uninstalling pyparsing-3.2.5:\n","      Successfully uninstalled pyparsing-3.2.5\n","  Attempting uninstall: pycparser\n","    Found existing installation: pycparser 2.23\n","    Uninstalling pycparser-2.23:\n","      Successfully uninstalled pycparser-2.23\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 5.29.5\n","    Uninstalling protobuf-5.29.5:\n","      Successfully uninstalled protobuf-5.29.5\n","  Attempting uninstall: pillow\n","    Found existing installation: pillow 11.3.0\n","    Uninstalling pillow-11.3.0:\n","      Successfully uninstalled pillow-11.3.0\n","  Attempting uninstall: packaging\n","    Found existing installation: packaging 25.0\n","    Uninstalling packaging-25.0:\n","      Successfully uninstalled packaging-25.0\n","  Attempting uninstall: opt_einsum\n","    Found existing installation: opt_einsum 3.4.0\n","    Uninstalling opt_einsum-3.4.0:\n","      Successfully uninstalled opt_einsum-3.4.0\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 2.0.2\n","    Uninstalling numpy-2.0.2:\n","      Successfully uninstalled numpy-2.0.2\n","  Attempting uninstall: kiwisolver\n","    Found existing installation: kiwisolver 1.4.9\n","    Uninstalling kiwisolver-1.4.9:\n","      Successfully uninstalled kiwisolver-1.4.9\n","  Attempting uninstall: fonttools\n","    Found existing installation: fonttools 4.60.1\n","    Uninstalling fonttools-4.60.1:\n","      Successfully uninstalled fonttools-4.60.1\n","  Attempting uninstall: cycler\n","    Found existing installation: cycler 0.12.1\n","    Uninstalling cycler-0.12.1:\n","      Successfully uninstalled cycler-0.12.1\n","  Attempting uninstall: attrs\n","    Found existing installation: attrs 25.4.0\n","    Uninstalling attrs-25.4.0:\n","      Successfully uninstalled attrs-25.4.0\n","  Attempting uninstall: absl-py\n","    Found existing installation: absl-py 1.4.0\n","    Uninstalling absl-py-1.4.0:\n","      Successfully uninstalled absl-py-1.4.0\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.16.3\n","    Uninstalling scipy-1.16.3:\n","      Successfully uninstalled scipy-1.16.3\n","  Attempting uninstall: python-dateutil\n","    Found existing installation: python-dateutil 2.9.0.post0\n","    Uninstalling python-dateutil-2.9.0.post0:\n","      Successfully uninstalled python-dateutil-2.9.0.post0\n","  Attempting uninstall: opencv-contrib-python\n","    Found existing installation: opencv-contrib-python 4.12.0.88\n","    Uninstalling opencv-contrib-python-4.12.0.88:\n","      Successfully uninstalled opencv-contrib-python-4.12.0.88\n","  Attempting uninstall: ml_dtypes\n","    Found existing installation: ml_dtypes 0.5.4\n","    Uninstalling ml_dtypes-0.5.4:\n","      Successfully uninstalled ml_dtypes-0.5.4\n","  Attempting uninstall: contourpy\n","    Found existing installation: contourpy 1.3.3\n","    Uninstalling contourpy-1.3.3:\n","      Successfully uninstalled contourpy-1.3.3\n","  Attempting uninstall: CFFI\n","    Found existing installation: cffi 2.0.0\n","    Uninstalling cffi-2.0.0:\n","      Successfully uninstalled cffi-2.0.0\n","  Attempting uninstall: matplotlib\n","    Found existing installation: matplotlib 3.10.0\n","    Uninstalling matplotlib-3.10.0:\n","      Successfully uninstalled matplotlib-3.10.0\n","  Attempting uninstall: jaxlib\n","    Found existing installation: jaxlib 0.7.2\n","    Uninstalling jaxlib-0.7.2:\n","      Successfully uninstalled jaxlib-0.7.2\n","  Attempting uninstall: jax\n","    Found existing installation: jax 0.7.2\n","    Uninstalling jax-0.7.2:\n","      Successfully uninstalled jax-0.7.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n","opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n","opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\n","gradio 5.50.0 requires pillow<12.0,>=8.0, but you have pillow 12.0.0 which is incompatible.\n","shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n","ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n","grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n","pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed CFFI-2.0.0 absl-py-2.3.1 attrs-25.4.0 contourpy-1.3.3 cycler-0.12.1 flatbuffers-25.9.23 fonttools-4.61.0 jax-0.7.1 jaxlib-0.7.1 kiwisolver-1.4.9 matplotlib-3.10.7 mediapipe-0.10.21 ml_dtypes-0.5.4 numpy-1.26.4 opencv-contrib-python-4.11.0.86 opt_einsum-3.4.0 packaging-25.0 pillow-12.0.0 protobuf-4.25.8 pycparser-2.23 pyparsing-3.2.5 python-dateutil-2.9.0.post0 scipy-1.16.3 sentencepiece-0.2.1 six-1.17.0 sounddevice-0.5.3\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["PIL","cycler","dateutil","google","kiwisolver","matplotlib","mpl_toolkits","numpy","packaging","pyparsing","six"]},"id":"d3100d08c86f4cc5baac2a39aeeb3433"}},"metadata":{}}],"source":["\"\"\"mediapipe ì„¤ì¹˜\"\"\"\n","\n","# MediaPipeì™€ í˜¸í™˜ë˜ëŠ” numpy ë²„ì „ì„ ê°•ì œ ì¬ì„¤ì¹˜\n","!pip install numpy==1.26.4 mediapipe==0.10.21 --force-reinstall"]},{"cell_type":"code","source":["!pip install tqdm"],"metadata":{"id":"lAxU7YtLyJXz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764556193798,"user_tz":-540,"elapsed":15180,"user":{"displayName":"ì½”ì½”ëª½","userId":"10518974845406570474"}},"outputId":"a0968f94-1f87-4b53-81f2-597cd29b9472"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n"]}]},{"cell_type":"code","source":["!pip install ultralytics"],"metadata":{"id":"hVFoRnRYyLt5","collapsed":true,"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1764556225864,"user_tz":-540,"elapsed":32063,"user":{"displayName":"ì½”ì½”ëª½","userId":"10518974845406570474"}},"outputId":"77f0662c-f650-4ee6-f669-4ae0fee28e09"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ultralytics\n","  Downloading ultralytics-8.3.233-py3-none-any.whl.metadata (37 kB)\n","Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.26.4)\n","Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.7)\n","Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.12.0.88)\n","Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (12.0.0)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.3)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.3)\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.9.0+cu126)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.24.0+cu126)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: polars>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.31.0)\n","Collecting ultralytics-thop>=2.0.18 (from ultralytics)\n","  Downloading ultralytics_thop-2.0.18-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.61.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n","Requirement already satisfied: pyparsing>=3 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.5)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n","Collecting numpy>=1.23.0 (from ultralytics)\n","  Downloading numpy-2.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2025.11.12)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.20.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.6)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.3)\n","Downloading ultralytics-8.3.233-py3-none-any.whl (1.1 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading numpy-2.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.5 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ultralytics_thop-2.0.18-py3-none-any.whl (28 kB)\n","Installing collected packages: numpy, ultralytics-thop, ultralytics\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.26.4\n","    Uninstalling numpy-1.26.4:\n","      Successfully uninstalled numpy-1.26.4\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","mediapipe 0.10.21 requires numpy<2, but you have numpy 2.2.6 which is incompatible.\n","gradio 5.50.0 requires pillow<12.0,>=8.0, but you have pillow 12.0.0 which is incompatible.\n","tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\n","numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\n","ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed numpy-2.2.6 ultralytics-8.3.233 ultralytics-thop-2.0.18\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy"]},"id":"670512e83d914932aaed9a7f9c0b305c"}},"metadata":{}}]},{"cell_type":"code","source":["## ì„¤ì¹˜ í™•ì¸\n","import mediapipe as mp\n","import numpy as np\n","\n","print(\"âœ… Mediapipeì™€ NumPy ë¡œë“œ ì„±ê³µ!\")\n","print(f\"Mediapipe ë²„ì „: {mp.__version__}\")\n","print(f\"NumPy ë²„ì „: {np.__version__}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"qS_8esPhyNvO","executionInfo":{"status":"ok","timestamp":1764553097988,"user_tz":-540,"elapsed":5869,"user":{"displayName":"ì½”ì½”ëª½","userId":"10518974845406570474"}},"outputId":"115e9c16-0c2d-493e-8aa6-4a2867de2e0c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/jaxlib/plugin_support.py:71: RuntimeWarning: JAX plugin jax_cuda12_plugin version 0.7.2 is installed, but it is not compatible with the installed jaxlib version 0.7.1, so it will not be used.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["âœ… Mediapipeì™€ NumPy ë¡œë“œ ì„±ê³µ!\n","Mediapipe ë²„ì „: 0.10.21\n","NumPy ë²„ì „: 2.2.6\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import os\n","import cv2\n","import math\n","import random\n","import tensorflow as tf\n","from tensorflow.keras.models import load_model, Sequential\n","from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n","from tqdm import tqdm\n","from ultralytics import YOLO\n","import mediapipe as mp\n","from mediapipe.framework.formats import landmark_pb2\n","from google.colab import drive\n","\n","# 1. Google Drive ë§ˆìš´íŠ¸\n","drive.mount('/content/drive')\n","print(\"âœ… Drive ë§ˆìš´íŠ¸ ë° í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ\")\n","\n","# GPU/CPU ì„¤ì • (â­ ì´ ì½”ë“œê°€ device ë³€ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤!)\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"âœ… ì‚¬ìš© ì¥ì¹˜: {device}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PJNwmQz9x96N","executionInfo":{"status":"ok","timestamp":1764556309266,"user_tz":-540,"elapsed":63542,"user":{"displayName":"ì½”ì½”ëª½","userId":"10518974845406570474"}},"outputId":"96ce753e-2172-4263-a96b-95d6c1e9f3a7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/jaxlib/plugin_support.py:71: RuntimeWarning: JAX plugin jax_cuda12_plugin version 0.7.2 is installed, but it is not compatible with the installed jaxlib version 0.7.1, so it will not be used.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Creating new Ultralytics Settings v0.0.6 file âœ… \n","View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n","Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n","Mounted at /content/drive\n","âœ… Drive ë§ˆìš´íŠ¸ ë° í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ\n","âœ… ì‚¬ìš© ì¥ì¹˜: cpu\n"]}]},{"cell_type":"code","source":["# ==========================================\n","# ëª¨ë¸ ë¡œë“œ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ\n","# ==========================================\n","\n","LOAD_MODEL_PATH = '/content/drive/MyDrive/hand_project/Final_code/skeleton_pinch_v3.keras'\n","X_TEST_PATH = '/content/drive/MyDrive/hand_project/Final_code/X_test_v3.npy'\n","Y_TEST_PATH = '/content/drive/MyDrive/hand_project/Final_code/Y_test_v3.npy'\n","\n","loaded_model = None\n","try:\n","    loaded_model = load_model(LOAD_MODEL_PATH)\n","    print(f\"âœ… Pinch ëª¨ë¸ V3 ë¡œë“œ ì„±ê³µ: {os.path.basename(LOAD_MODEL_PATH)}\")\n","except Exception as e:\n","    print(f\"âš ï¸ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {e}\")\n","\n","X_test = None\n","Y_test = None\n","try:\n","    X_test = np.load(X_TEST_PATH)\n","    Y_test = np.load(Y_TEST_PATH)\n","    print(f\"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¡œë“œ ì„±ê³µ: (X: {X_test.shape}, Y: {Y_test.shape})\")\n","\n","except FileNotFoundError:\n","    print(\"âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨. 2ë‹¨ê³„ë¥¼ ë¨¼ì € ì™„ë£Œí•˜ì„¸ìš”.\")\n","\n","print(\"\\nâœ… ë¶„ì„ ì´ˆê¸°í™” ì™„ë£Œ. 4ë‹¨ê³„ F1 ìŠ¤ì½”ì–´ ê³„ì‚°ì„ ì‹œì‘í•˜ì„¸ìš”.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kLMhZ-ha2Vdg","executionInfo":{"status":"ok","timestamp":1764556315068,"user_tz":-540,"elapsed":5805,"user":{"displayName":"ì½”ì½”ëª½","userId":"10518974845406570474"}},"outputId":"947c9230-909a-409c-c116-eb0cc2e012a3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Pinch ëª¨ë¸ V3 ë¡œë“œ ì„±ê³µ: skeleton_pinch_v3.keras\n","âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¡œë“œ ì„±ê³µ: (X: (129, 126), Y: (129,))\n","\n","âœ… ë¶„ì„ ì´ˆê¸°í™” ì™„ë£Œ. 4ë‹¨ê³„ F1 ìŠ¤ì½”ì–´ ê³„ì‚°ì„ ì‹œì‘í•˜ì„¸ìš”.\n"]}]},{"cell_type":"code","source":["# ==========================================\n","# 2. ST-GCN ê´€ë ¨ í´ë˜ìŠ¤ ë° í•¨ìˆ˜ ì •ì˜ (ì›ë³¸ AdaptiveSTGCN êµ¬ì¡°)\n","# ğŸš¨ 3ï¸âƒ£ ë¸”ë¡ ì´ì „ì— ë°˜ë“œì‹œ ì‹¤í–‰ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.\n","# ==========================================\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import torch # PyTorchê°€ 1-2ì—ì„œ ì„í¬íŠ¸ë˜ì–´ ìˆì§€ë§Œ, ì•ˆì „ì„ ìœ„í•´ ë‹¤ì‹œ ëª…ì‹œí•©ë‹ˆë‹¤.\n","\n","# --- [2-1. ê·¸ë˜í”„(A) ì •ì˜] ---\n","# MediaPipe ì† ê´€ì ˆ ì—°ê²° êµ¬ì¡° ì •ì˜ (21ê°œ ê´€ì ˆ)\n","mediapipe_edges = [\n","    (0,1), (1,2), (2,3), (3,4), (0,5), (5,6), (6,7), (7,8),\n","    (0,9), (9,10), (10,11), (11,12), (0,13),(13,14),(14,15),(15,16),\n","    (0,17),(17,18),(18,19),(19,20)\n","]\n","NUM_JOINTS = 21 # ê´€ì ˆ ìˆ˜\n","\n","# ì¸ì ‘ í–‰ë ¬(Adjacency Matrix) A ìƒì„±\n","A = np.zeros((NUM_JOINTS, NUM_JOINTS))\n","for i, j in mediapipe_edges:\n","    A[i, j] = A[j, i] = 1 # ì—°ê²°ëœ ê´€ì ˆì€ 1\n","for i in range(NUM_JOINTS):\n","    A[i, i] = 1 # ìê¸° ìì‹ ê³¼ì˜ ì—°ê²° (self-loop)ì€ 1\n","A = np.array([A], dtype=np.float32)\n","\n","\n","# --- [2-2. Adaptive ST-GCN ëª¨ë¸ ì •ì˜] ---\n","class AdaptiveSTGCNBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, A, stride=1, residual=True):\n","        super().__init__()\n","        self.A = torch.tensor(A, dtype=torch.float32, requires_grad=False)\n","        self.B = nn.Parameter(torch.zeros(A.shape, dtype=torch.float32)) # Adaptive Matrix B\n","\n","        self.gcn = nn.Conv2d(in_channels, out_channels, kernel_size=(1,1))\n","        self.tcn = nn.Sequential(\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(out_channels, out_channels, kernel_size=(3,1), stride=(stride,1), padding=(1,0)),\n","            nn.BatchNorm2d(out_channels),\n","        )\n","        if not residual:\n","            self.residual = lambda x: 0\n","        elif (in_channels == out_channels) and (stride == 1):\n","            self.residual = lambda x: x\n","        else:\n","            self.residual = nn.Sequential(\n","                nn.Conv2d(in_channels, out_channels, kernel_size=(1,1), stride=(stride,1)),\n","                nn.BatchNorm2d(out_channels),\n","            )\n","        self.relu = nn.ReLU(inplace=True)\n","\n","    def forward(self, x):\n","        A_adaptive = self.A.to(x.device) + self.B.to(x.device)\n","        # Spatial Graph Convolution\n","        x_gcn = torch.einsum('nctv,vw->nctw', x, A_adaptive.squeeze(0))\n","        x_gcn = self.gcn(x_gcn)\n","        # Temporal Convolution\n","        x_tcn = self.tcn(x_gcn)\n","        return self.relu(x_tcn + self.residual(x))\n","\n","class AdaptiveSTGCN(nn.Module):\n","    def __init__(self, A, num_classes=2, in_channels=7):\n","        super().__init__()\n","        # 7ch * 21 joints * 2 hands -> 294 features\n","        self.data_bn = nn.BatchNorm1d(in_channels * 21 * 2)\n","        self.layer1 = AdaptiveSTGCNBlock(in_channels, 64, A)\n","        self.layer2 = AdaptiveSTGCNBlock(64, 128, A, stride=2)\n","        self.layer3 = AdaptiveSTGCNBlock(128, 256, A, stride=2)\n","        self.fc = nn.Linear(256, num_classes)\n","\n","    def forward(self, x):\n","        N, C, T, V, M = x.shape\n","        # Input: (N, C, T, V, M) -> (N, M*V*C, T)\n","        x = x.permute(0, 4, 3, 1, 2).contiguous().view(N, M * V * C, T)\n","        x = self.data_bn(x)\n","        # Back to (N*M, C, T, V)\n","        x = x.view(N, M, V, C, T).permute(0, 1, 3, 4, 2).contiguous().view(N * M, C, T, V)\n","\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","\n","        # Global Average Pooling (T, V ì°¨ì› ì¶•ì†Œ)\n","        x = F.avg_pool2d(x, x.size()[2:])\n","        # (N, M, Channel) -> ë‘ ì†ì˜ í‰ê· ì„ ì·¨í•¨\n","        x = x.view(N, M, -1).mean(dim=1)\n","        x = self.fc(x)\n","        return x\n","\n","print(\"âœ… ST-GCN ê´€ë ¨ í´ë˜ìŠ¤ ë° í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zozmJQsoajVs","executionInfo":{"status":"ok","timestamp":1764556317655,"user_tz":-540,"elapsed":18,"user":{"displayName":"ì½”ì½”ëª½","userId":"10518974845406570474"}},"outputId":"150c1ee5-6586-40b3-9c40-14c3e0e863cc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… ST-GCN ê´€ë ¨ í´ë˜ìŠ¤ ë° í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n"]}]},{"cell_type":"markdown","source":["# **í•™ìŠµ ìˆ˜í–‰**\n","-ì‹¤ìŠµí•  ë•ŒëŠ” ê·¸ëƒ¥ ìœ„ì— ë¶ˆëŸ¬ì˜¤ê¸°ë§Œ í•´ì£¼ì„¸ìš”! -> ë°”ë¡œ ì½”ë“œ í•©ì²´ ë¸”ë¡ìœ¼ë¡œ"],"metadata":{"id":"F5iIXmNT1k0A"}},{"cell_type":"code","source":["# ==========================================\n","# 2. ëª¨ë¸ V3 í•™ìŠµ ë° ì €ì¥ (ğŸš¨ ì˜¤íƒ€ ìˆ˜ì • ì™„ë£Œë¨)\n","# ==========================================\n","import os\n","import cv2\n","import numpy as np\n","import random\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","import mediapipe as mp\n","from tqdm import tqdm\n","\n","# ğŸš¨ğŸš¨ ê²½ë¡œ ì„¤ì •\n","BASE_PATH = '/content/drive/MyDrive/hand_project/Dataset'\n","PINCH_PATH = os.path.join(BASE_PATH, 'pinch')\n","NON_PINCH_PATH = os.path.join(BASE_PATH, 'train')\n","MODEL_SAVE_DIR = '/content/drive/MyDrive/hand_project/Final_code'\n","MODEL_PATH = os.path.join(MODEL_SAVE_DIR, 'skeleton_pinch_v3.keras')\n","\n","# ğŸš¨ğŸš¨ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì €ì¥ ê²½ë¡œ\n","X_TEST_PATH = os.path.join(MODEL_SAVE_DIR, 'X_test_v3.npy')\n","Y_TEST_PATH = os.path.join(MODEL_SAVE_DIR, 'Y_test_v3.npy') # ì˜¤íƒ€ ìˆ˜ì • ë°˜ì˜\n","\n","os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n","print(f\"âœ… ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {MODEL_SAVE_DIR}\")\n","\n","# MediaPipe ì„¤ì •\n","mp_hands = mp.solutions.hands\n","hands = mp_hands.Hands(static_image_mode=True, max_num_hands=2, min_detection_confidence=0.5)\n","\n","# --- 2.1. íŒŒì¼ ë¡œë“œ ë° ì¶”ì¶œ í•¨ìˆ˜ (ì´ì „ê³¼ ë™ì¼) ---\n","def get_safe_image_files(folder_path):\n","    valid_extensions = ('.jpg', '.jpeg', '.png', '.bmp')\n","    safe_files = []\n","    if os.path.exists(folder_path):\n","        for root, dirs, files in os.walk(folder_path):\n","            for filename in files:\n","                if filename.lower().endswith(valid_extensions):\n","                    safe_files.append(os.path.join(root, filename))\n","    return safe_files\n","\n","def extract_keypoints(file_path):\n","    image = cv2.imread(file_path)\n","    if image is None: return np.zeros(126)\n","    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","    results = hands.process(image_rgb)\n","    keypoints = []\n","    if results.multi_hand_landmarks:\n","        for hand_landmarks in results.multi_hand_landmarks:\n","            for landmark in hand_landmarks.landmark:\n","                keypoints.extend([landmark.x, landmark.y, landmark.z])\n","    final_kp = np.zeros(126)\n","    final_kp[:len(keypoints)] = keypoints[:126]\n","    return final_kp\n","\n","# --- 2.2. ë°ì´í„° ë¡œë“œ ë° ì¦ê°• (ì¦ê°• ì œê±°) ---\n","print(\"ğŸš€ ë°ì´í„° ë¡œë“œ ë° ì²˜ë¦¬ ì‹œì‘...\")\n","pinch_files = get_safe_image_files(PINCH_PATH)\n","X_pinch = []\n","print(\" Â  - Pinch ì¢Œí‘œ ì¶”ì¶œ (ì¦ê°• ì—†ìŒ) ì¤‘...\")\n","for file_path in tqdm(pinch_files):\n","    kp = extract_keypoints(file_path)\n","    if np.any(kp):\n","        X_pinch.append(kp)\n","print(f\" Â  âœ… Pinch ë°ì´í„° í™•ë³´: {len(X_pinch)}ê°œ (ìµœì¢…)\")\n","\n","non_pinch_files = get_safe_image_files(NON_PINCH_PATH)\n","NON_PINCH_TARGET_RATIO = 1.5\n","target_non_pinch_count = int(len(X_pinch) * NON_PINCH_TARGET_RATIO)\n","if len(non_pinch_files) > target_non_pinch_count:\n","    random.seed(42)\n","    selected_non_pinch = random.sample(non_pinch_files, target_non_pinch_count)\n","else:\n","    selected_non_pinch = non_pinch_files\n","    target_non_pinch_count = len(selected_non_pinch)\n","\n","print(f\" Â  - Non-Pinch {len(non_pinch_files)}ê°œ ì¤‘ {target_non_pinch_count}ê°œ ìƒ˜í”Œë§ ì¤‘...\")\n","X_non_pinch = []\n","for file_path in tqdm(selected_non_pinch):\n","    kp = extract_keypoints(file_path)\n","    if np.any(kp):\n","        X_non_pinch.append(kp)\n","\n","# --- 2.3. í•™ìŠµ ë°ì´í„°ì…‹ êµ¬ì„± ë° í•™ìŠµ ---\n","X = np.array(X_pinch + X_non_pinch)\n","y = np.array([1]*len(X_pinch) + [0]*len(X_non_pinch))\n","\n","# í›ˆë ¨/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬\n","X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True, stratify=y)\n","# ğŸš¨ğŸš¨ y_test ë³€ìˆ˜ëª…ì€ ì†Œë¬¸ì yë¡œ ì •ì˜ë©ë‹ˆë‹¤. ğŸš¨ğŸš¨\n","X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n","\n","print(f\"\\nğŸ“Š ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: Train({len(X_train)}) / Val({len(X_val)}) / Test({len(X_test)})\")\n","\n","# ğŸš¨ğŸš¨ X_testì™€ y_testë¥¼ Driveì— ì €ì¥í•©ë‹ˆë‹¤! (ì˜¤íƒ€ ìˆ˜ì •ë¨) ğŸš¨ğŸš¨\n","np.save(X_TEST_PATH, X_test)\n","np.save(Y_TEST_PATH, y_test) # ğŸ‘ˆ ì—¬ê¸°ê°€ Y_testê°€ ì•„ë‹Œ y_testì—¬ì•¼ í•©ë‹ˆë‹¤.\n","print(f\"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì €ì¥ ì™„ë£Œ: {X_TEST_PATH}, {Y_TEST_PATH}\")\n","\n","\n","# 2.4. ëª¨ë¸ ì •ì˜ ë° í•™ìŠµ\n","input_shape = X_train.shape[1]\n","\n","model = Sequential([\n","    Input(shape=(126,)), Dense(512, activation='relu'), BatchNormalization(), Dropout(0.3),\n","    Dense(256, activation='relu'), BatchNormalization(), Dropout(0.3),\n","    Dense(128, activation='relu'), BatchNormalization(), Dropout(0.2),\n","    Dense(1, activation='sigmoid')\n","])\n","\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","callbacks = [\n","    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n","    ModelCheckpoint(filepath=MODEL_PATH, monitor='val_accuracy', save_best_only=True, verbose=1)\n","]\n","\n","print(f\"\\nğŸ”¥ í•™ìŠµ ì‹œì‘! (ì €ì¥ ê²½ë¡œ: {MODEL_PATH})\")\n","history = model.fit(\n","    X_train, y_train,\n","    epochs=100, batch_size=32, validation_data=(X_val, y_val), callbacks=callbacks, verbose=1\n",")\n","\n","print(f\"\\nğŸ‰ ëª¨ë¸ V3 í•™ìŠµ ë° ì €ì¥ ì™„ë£Œ: {MODEL_PATH}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"aonoVWawxvkZ","outputId":"0ac0ba46-12d9-4d49-b81c-16fa1f197eef","executionInfo":{"status":"ok","timestamp":1764512760133,"user_tz":-540,"elapsed":97741,"user":{"displayName":"ìœ¤ì±„ë¦¼","userId":"14851188995800772232"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… ëª¨ë¸ ì €ì¥ ê²½ë¡œ: /content/drive/MyDrive/hand_project/Final_code\n","ğŸš€ ë°ì´í„° ë¡œë“œ ë° ì²˜ë¦¬ ì‹œì‘...\n"," Â  - Pinch ì¢Œí‘œ ì¶”ì¶œ (ì¦ê°• ì—†ìŒ) ì¤‘...\n"]},{"output_type":"stream","name":"stderr","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 203/203 [00:35<00:00,  5.76it/s]\n"]},{"output_type":"stream","name":"stdout","text":[" Â  âœ… Pinch ë°ì´í„° í™•ë³´: 174ê°œ (ìµœì¢…)\n"," Â  - Non-Pinch 2155ê°œ ì¤‘ 261ê°œ ìƒ˜í”Œë§ ì¤‘...\n"]},{"output_type":"stream","name":"stderr","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 261/261 [00:42<00:00,  6.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","ğŸ“Š ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: Train(278) / Val(60) / Test(60)\n","âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì €ì¥ ì™„ë£Œ: /content/drive/MyDrive/hand_project/Final_code/X_test_v3.npy, /content/drive/MyDrive/hand_project/Final_code/Y_test_v3.npy\n","\n","ğŸ”¥ í•™ìŠµ ì‹œì‘! (ì €ì¥ ê²½ë¡œ: /content/drive/MyDrive/hand_project/Final_code/skeleton_pinch_v3.keras)\n","Epoch 1/100\n","\u001b[1m5/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5888 - loss: 0.8227\n","Epoch 1: val_accuracy improved from -inf to 0.61667, saving model to /content/drive/MyDrive/hand_project/Final_code/skeleton_pinch_v3.keras\n","\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 188ms/step - accuracy: 0.6577 - loss: 0.6950 - val_accuracy: 0.6167 - val_loss: 0.6356\n","Epoch 2/100\n","\u001b[1m8/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8999 - loss: 0.2651\n","Epoch 2: val_accuracy improved from 0.61667 to 0.65000, saving model to /content/drive/MyDrive/hand_project/Final_code/skeleton_pinch_v3.keras\n","\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - accuracy: 0.8969 - loss: 0.2760 - val_accuracy: 0.6500 - val_loss: 0.6163\n","Epoch 3/100\n","\u001b[1m7/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9060 - loss: 0.3418\n","Epoch 3: val_accuracy improved from 0.65000 to 0.91667, saving model to /content/drive/MyDrive/hand_project/Final_code/skeleton_pinch_v3.keras\n","\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 80ms/step - accuracy: 0.9098 - loss: 0.3226 - val_accuracy: 0.9167 - val_loss: 0.6248\n","Epoch 4/100\n","\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9566 - loss: 0.2010\n","Epoch 4: val_accuracy did not improve from 0.91667\n","\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.9566 - loss: 0.2003 - val_accuracy: 0.9167 - val_loss: 0.6187\n","Epoch 5/100\n","\u001b[1m8/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9697 - loss: 0.1490\n","Epoch 5: val_accuracy improved from 0.91667 to 0.93333, saving model to /content/drive/MyDrive/hand_project/Final_code/skeleton_pinch_v3.keras\n","\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - accuracy: 0.9671 - loss: 0.1499 - val_accuracy: 0.9333 - val_loss: 0.5927\n","Epoch 6/100\n","\u001b[1m7/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9336 - loss: 0.1624\n","Epoch 6: val_accuracy improved from 0.93333 to 0.95000, saving model to /content/drive/MyDrive/hand_project/Final_code/skeleton_pinch_v3.keras\n","\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 87ms/step - accuracy: 0.9317 - loss: 0.1651 - val_accuracy: 0.9500 - val_loss: 0.5787\n","Epoch 7/100\n","\u001b[1m7/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9779 - loss: 0.1119\n","Epoch 7: val_accuracy did not improve from 0.95000\n","\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.9801 - loss: 0.1071 - val_accuracy: 0.8167 - val_loss: 0.5777\n","Epoch 8/100\n","\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9764 - loss: 0.1069\n","Epoch 8: val_accuracy did not improve from 0.95000\n","\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9756 - loss: 0.1076 - val_accuracy: 0.7333 - val_loss: 0.5792\n","Epoch 9/100\n","\u001b[1m7/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9717 - loss: 0.0882 \n","Epoch 9: val_accuracy did not improve from 0.95000\n","\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9695 - loss: 0.0936 - val_accuracy: 0.5833 - val_loss: 0.5947\n","Epoch 10/100\n","\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9624 - loss: 0.0975\n","Epoch 10: val_accuracy did not improve from 0.95000\n","\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.9618 - loss: 0.0988 - val_accuracy: 0.5000 - val_loss: 0.6244\n","Epoch 11/100\n","\u001b[1m5/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9759 - loss: 0.1252\n","Epoch 11: val_accuracy did not improve from 0.95000\n","\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9799 - loss: 0.1054 - val_accuracy: 0.4667 - val_loss: 0.6750\n","Epoch 12/100\n","\u001b[1m6/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9701 - loss: 0.0783\n","Epoch 12: val_accuracy did not improve from 0.95000\n","\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9700 - loss: 0.0856 - val_accuracy: 0.4500 - val_loss: 0.6792\n","Epoch 13/100\n","\u001b[1m5/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9749 - loss: 0.0755 \n","Epoch 13: val_accuracy did not improve from 0.95000\n","\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9755 - loss: 0.0730 - val_accuracy: 0.4500 - val_loss: 0.7003\n","Epoch 14/100\n","\u001b[1m5/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9752 - loss: 0.0654\n","Epoch 14: val_accuracy did not improve from 0.95000\n","\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9771 - loss: 0.0612 - val_accuracy: 0.4833 - val_loss: 0.6901\n","Epoch 15/100\n","\u001b[1m7/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9686 - loss: 0.1204 \n","Epoch 15: val_accuracy did not improve from 0.95000\n","\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9710 - loss: 0.1130 - val_accuracy: 0.4500 - val_loss: 0.7148\n","Epoch 16/100\n","\u001b[1m1/9\u001b[0m \u001b[32mâ”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 1.0000 - loss: 0.0292\n","Epoch 16: val_accuracy did not improve from 0.95000\n","\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9790 - loss: 0.0479 - val_accuracy: 0.4667 - val_loss: 0.7443\n","Epoch 17/100\n","\u001b[1m1/9\u001b[0m \u001b[32mâ”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9688 - loss: 0.0730\n","Epoch 17: val_accuracy did not improve from 0.95000\n","\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9806 - loss: 0.0571 - val_accuracy: 0.4667 - val_loss: 0.7487\n","\n","ğŸ‰ ëª¨ë¸ V3 í•™ìŠµ ë° ì €ì¥ ì™„ë£Œ: /content/drive/MyDrive/hand_project/Final_code/skeleton_pinch_v3.keras\n"]}]},{"cell_type":"markdown","source":["ì¦ê°• ì ìš©"],"metadata":{"id":"PoloBFY4XVvq"}},{"cell_type":"code","source":["# ==========================================\n","# 2. ëª¨ë¸ V3 í•™ìŠµ ë° ì €ì¥ (ğŸš¨ ì¢Œìš° ë°˜ì „ ì¦ê°• ì¶”ê°€ ì™„ë£Œ)\n","# ==========================================\n","import os\n","import cv2\n","import numpy as np\n","import random\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","import mediapipe as mp\n","from tqdm import tqdm\n","\n","# ğŸš¨ğŸš¨ ê²½ë¡œ ì„¤ì •\n","BASE_PATH = '/content/drive/MyDrive/hand_project/Dataset'\n","PINCH_PATH = os.path.join(BASE_PATH, 'pinch')\n","NON_PINCH_PATH = os.path.join(BASE_PATH, 'train')\n","MODEL_SAVE_DIR = '/content/drive/MyDrive/hand_project/Final_code'\n","MODEL_PATH = os.path.join(MODEL_SAVE_DIR, 'skeleton_pinch_v3.keras')\n","\n","# ğŸš¨ğŸš¨ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì €ì¥ ê²½ë¡œ\n","X_TEST_PATH = os.path.join(MODEL_SAVE_DIR, 'X_test_v3.npy')\n","Y_TEST_PATH = os.path.join(MODEL_SAVE_DIR, 'Y_test_v3.npy')\n","\n","os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n","print(f\"âœ… ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {MODEL_SAVE_DIR}\")\n","\n","# MediaPipe ì„¤ì •\n","mp_hands = mp.solutions.hands\n","hands = mp_hands.Hands(static_image_mode=True, max_num_hands=2, min_detection_confidence=0.5)\n","\n","# --- 2.1. ì´ë¯¸ì§€ì—ì„œ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ í•¨ìˆ˜ (ë‚´ë¶€ í•¨ìˆ˜ ë³€ê²½) ---\n","def extract_keypoints_from_image(image, hands_processor):\n","    # ì´ë¯¸ì§€ì—ì„œ MediaPipeë¥¼ ì´ìš©í•´ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ\n","    if image is None: return np.zeros(126)\n","    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","    results = hands_processor.process(image_rgb)\n","    keypoints = []\n","    if results.multi_hand_landmarks:\n","        for hand_landmarks in results.multi_hand_landmarks:\n","            for landmark in hand_landmarks.landmark:\n","                # x, y, z ì¢Œí‘œ ì¶”ì¶œ (ì´ 126ê°œ)\n","                keypoints.extend([landmark.x, landmark.y, landmark.z])\n","    final_kp = np.zeros(126)\n","    final_kp[:len(keypoints)] = keypoints[:126]\n","    return final_kp\n","\n","def get_safe_image_files(folder_path):\n","    valid_extensions = ('.jpg', '.jpeg', '.png', '.bmp')\n","    safe_files = []\n","    if os.path.exists(folder_path):\n","        for root, dirs, files in os.walk(folder_path):\n","            for filename in files:\n","                if filename.lower().endswith(valid_extensions):\n","                    safe_files.append(os.path.join(root, filename))\n","    return safe_files\n","\n","# --- 2.2. ë°ì´í„° ë¡œë“œ ë° ì¦ê°• (ì¢Œìš° ë°˜ì „ ì¶”ê°€) ---\n","print(\"ğŸš€ ë°ì´í„° ë¡œë“œ ë° ì²˜ë¦¬ ì‹œì‘ (ì¢Œìš° ë°˜ì „ ì¦ê°• í¬í•¨)...\")\n","pinch_files = get_safe_image_files(PINCH_PATH)\n","X_pinch = []\n","print(\" Â  - Pinch ì¢Œí‘œ ì¶”ì¶œ ë° ì¢Œìš° ë°˜ì „ ì¦ê°• ì¤‘...\")\n","\n","for file_path in tqdm(pinch_files):\n","    image = cv2.imread(file_path)\n","    if image is None: continue\n","\n","    # 1. Original Keypoints\n","    kp_original = extract_keypoints_from_image(image, hands)\n","    if np.any(kp_original):\n","        X_pinch.append(kp_original)\n","\n","    # 2. Augmented (Horizontal Flip) Keypoints ğŸš¨ [NEW AUGMENTATION] ğŸš¨\n","    # ì´ë¯¸ì§€ë¥¼ ìˆ˜í‰ìœ¼ë¡œ ë°˜ì „(flip code 1) í›„ í‚¤í¬ì¸íŠ¸ ì¬ì¶”ì¶œ\n","    image_flipped = cv2.flip(image, 1)\n","    kp_flipped = extract_keypoints_from_image(image_flipped, hands)\n","    if np.any(kp_flipped):\n","        X_pinch.append(kp_flipped)\n","\n","# Non-pinch data loading and sampling (applied augmentation as well)\n","non_pinch_files = get_safe_image_files(NON_PINCH_PATH)\n","NON_PINCH_TARGET_RATIO = 1.5\n","\n","# í•„ìš”í•œ íŒŒì¼ ìˆ˜ ê³„ì‚°: ìµœì¢… ëª©í‘œ ë°ì´í„°ì…‹ í¬ê¸°ì— ë§ì¶° ìƒ˜í”Œë§\n","# X_pinchëŠ” ì¦ê°• í›„ 2ë°°ê°€ ë˜ë¯€ë¡œ, ìµœì¢… Non-Pinch ëª©í‘œ ê°œìˆ˜ë¥¼ 2ë¡œ ë‚˜ëˆˆ ê°’ ë§Œí¼ ì›ë³¸ íŒŒì¼ì„ ìƒ˜í”Œë§í•©ë‹ˆë‹¤.\n","expected_total_pinch = len(pinch_files) * 2\n","target_non_pinch_count_per_augmentation = int(expected_total_pinch * NON_PINCH_TARGET_RATIO / 2)\n","files_needed = target_non_pinch_count_per_augmentation\n","\n","if len(non_pinch_files) > files_needed:\n","    random.seed(42)\n","    selected_non_pinch = random.sample(non_pinch_files, files_needed)\n","else:\n","    selected_non_pinch = non_pinch_files\n","    files_needed = len(selected_non_pinch)\n","\n","target_non_pinch_count = len(selected_non_pinch) * 2 # ìµœì¢… ë°ì´í„° ê°œìˆ˜ (ì¦ê°• í›„)\n","\n","print(f\" Â  - Non-Pinch {len(non_pinch_files)}ê°œ ì¤‘ {len(selected_non_pinch)}ê°œ ìƒ˜í”Œë§ í›„ ì¦ê°• ì¤‘... (ìµœì¢… ëª©í‘œ: {target_non_pinch_count}ê°œ)\")\n","X_non_pinch = []\n","for file_path in tqdm(selected_non_pinch):\n","    image = cv2.imread(file_path)\n","    if image is None: continue\n","\n","    # 1. Original Keypoints\n","    kp_original = extract_keypoints_from_image(image, hands)\n","    if np.any(kp_original):\n","        X_non_pinch.append(kp_original)\n","\n","    # 2. Augmented (Horizontal Flip) Keypoints ğŸš¨ [NEW AUGMENTATION] ğŸš¨\n","    image_flipped = cv2.flip(image, 1)\n","    kp_flipped = extract_keypoints_from_image(image_flipped, hands)\n","    if np.any(kp_flipped):\n","        X_non_pinch.append(kp_flipped)\n","\n","\n","# --- 2.3. í•™ìŠµ ë°ì´í„°ì…‹ êµ¬ì„± ë° í•™ìŠµ ---\n","X = np.array(X_pinch + X_non_pinch)\n","y = np.array([1]*len(X_pinch) + [0]*len(X_non_pinch))\n","\n","# í›ˆë ¨/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬\n","X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True, stratify=y)\n","# ğŸš¨ğŸš¨ y_test ë³€ìˆ˜ëª…ì€ ì†Œë¬¸ì yë¡œ ì •ì˜ë©ë‹ˆë‹¤. ğŸš¨ğŸš¨\n","X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n","\n","print(f\"\\nğŸ“Š ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: Train({len(X_train)}) / Val({len(X_val)}) / Test({len(X_test)})\")\n","\n","# ğŸš¨ğŸš¨ X_testì™€ y_testë¥¼ Driveì— ì €ì¥í•©ë‹ˆë‹¤! (ì˜¤íƒ€ ìˆ˜ì •ë¨) ğŸš¨ğŸš¨\n","np.save(X_TEST_PATH, X_test)\n","np.save(Y_TEST_PATH, y_test) # ğŸ‘ˆ ì—¬ê¸°ê°€ Y_testê°€ ì•„ë‹Œ y_testì—¬ì•¼ í•©ë‹ˆë‹¤.\n","print(f\"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì €ì¥ ì™„ë£Œ: {X_TEST_PATH}, {Y_TEST_PATH}\")\n","\n","\n","# 2.4. ëª¨ë¸ ì •ì˜ ë° í•™ìŠµ\n","input_shape = X_train.shape[1]\n","\n","model = Sequential([\n","    Input(shape=(126,)), Dense(512, activation='relu'), BatchNormalization(), Dropout(0.3),\n","    Dense(256, activation='relu'), BatchNormalization(), Dropout(0.3),\n","    Dense(128, activation='relu'), BatchNormalization(), Dropout(0.2),\n","    Dense(1, activation='sigmoid')\n","])\n","\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","callbacks = [\n","    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n","    ModelCheckpoint(filepath=MODEL_PATH, monitor='val_accuracy', save_best_only=True, verbose=1)\n","]\n","\n","print(f\"\\nğŸ”¥ í•™ìŠµ ì‹œì‘! (ì €ì¥ ê²½ë¡œ: {MODEL_PATH})\")\n","history = model.fit(\n","    X_train, y_train,\n","    epochs=100, batch_size=32, validation_data=(X_val, y_val), callbacks=callbacks, verbose=1\n",")\n","\n","print(f\"\\nğŸ‰ ëª¨ë¸ V3 í•™ìŠµ ë° ì €ì¥ ì™„ë£Œ: {MODEL_PATH}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"hKmW8nL-XXRI","executionInfo":{"status":"ok","timestamp":1764549970210,"user_tz":-540,"elapsed":150379,"user":{"displayName":"ì½”ì½”ëª½","userId":"10518974845406570474"}},"outputId":"2c1263ef-43a5-40af-adc3-71d7e41e94fb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… ëª¨ë¸ ì €ì¥ ê²½ë¡œ: /content/drive/MyDrive/hand_project/Final_code\n","ğŸš€ ë°ì´í„° ë¡œë“œ ë° ì²˜ë¦¬ ì‹œì‘ (ì¢Œìš° ë°˜ì „ ì¦ê°• í¬í•¨)...\n"," Â  - Pinch ì¢Œí‘œ ì¶”ì¶œ ë° ì¢Œìš° ë°˜ì „ ì¦ê°• ì¤‘...\n"]},{"output_type":"stream","name":"stderr","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 203/203 [01:08<00:00,  2.96it/s]\n"]},{"output_type":"stream","name":"stdout","text":[" Â  - Non-Pinch 2155ê°œ ì¤‘ 304ê°œ ìƒ˜í”Œë§ í›„ ì¦ê°• ì¤‘... (ìµœì¢… ëª©í‘œ: 608ê°œ)\n"]},{"output_type":"stream","name":"stderr","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:55<00:00,  5.48it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","ğŸ“Š ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: Train(600) / Val(129) / Test(129)\n","âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì €ì¥ ì™„ë£Œ: /content/drive/MyDrive/hand_project/Final_code/X_test_v3.npy, /content/drive/MyDrive/hand_project/Final_code/Y_test_v3.npy\n","\n","ğŸ”¥ í•™ìŠµ ì‹œì‘! (ì €ì¥ ê²½ë¡œ: /content/drive/MyDrive/hand_project/Final_code/skeleton_pinch_v3.keras)\n","Epoch 1/100\n","\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.6845 - loss: 0.6047\n","Epoch 1: val_accuracy improved from -inf to 0.63566, saving model to /content/drive/MyDrive/hand_project/Final_code/skeleton_pinch_v3.keras\n","\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 237ms/step - accuracy: 0.6896 - loss: 0.5987 - val_accuracy: 0.6357 - val_loss: 0.6171\n","Epoch 2/100\n","\u001b[1m 1/19\u001b[0m \u001b[32mâ”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9062 - loss: 0.3202\n","Epoch 2: val_accuracy improved from 0.63566 to 0.67442, saving model to /content/drive/MyDrive/hand_project/Final_code/skeleton_pinch_v3.keras\n","\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9182 - loss: 0.2310 - val_accuracy: 0.6744 - val_loss: 0.6100\n","Epoch 3/100\n","\u001b[1m 1/19\u001b[0m \u001b[32mâ”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9062 - loss: 0.2189\n","Epoch 3: val_accuracy improved from 0.67442 to 0.79070, saving model to /content/drive/MyDrive/hand_project/Final_code/skeleton_pinch_v3.keras\n","\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9372 - loss: 0.1893 - val_accuracy: 0.7907 - val_loss: 0.5893\n","Epoch 4/100\n","\u001b[1m 1/19\u001b[0m \u001b[32mâ”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0726\n","Epoch 4: val_accuracy improved from 0.79070 to 0.85271, saving model to /content/drive/MyDrive/hand_project/Final_code/skeleton_pinch_v3.keras\n","\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9501 - loss: 0.1609 - val_accuracy: 0.8527 - val_loss: 0.5543\n","Epoch 5/100\n","\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9330 - loss: 0.1712 \n","Epoch 5: val_accuracy did not improve from 0.85271\n","\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9331 - loss: 0.1712 - val_accuracy: 0.8372 - val_loss: 0.5480\n","Epoch 6/100\n","\u001b[1m 1/19\u001b[0m \u001b[32mâ”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m2s\u001b[0m 164ms/step - accuracy: 0.8750 - loss: 0.2558\n","Epoch 6: val_accuracy did not improve from 0.85271\n","\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9369 - loss: 0.1640 - val_accuracy: 0.7907 - val_loss: 0.5394\n","Epoch 7/100\n","\u001b[1m 1/19\u001b[0m \u001b[32mâ”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0601\n","Epoch 7: val_accuracy did not improve from 0.85271\n","\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9592 - loss: 0.1215 - val_accuracy: 0.6744 - val_loss: 0.5576\n","Epoch 8/100\n","\u001b[1m18/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9616 - loss: 0.1110 \n","Epoch 8: val_accuracy did not improve from 0.85271\n","\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9616 - loss: 0.1109 - val_accuracy: 0.6822 - val_loss: 0.5448\n","Epoch 9/100\n","\u001b[1m 1/19\u001b[0m \u001b[32mâ”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0300\n","Epoch 9: val_accuracy did not improve from 0.85271\n","\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9773 - loss: 0.0800 - val_accuracy: 0.6202 - val_loss: 0.5573\n","Epoch 10/100\n","\u001b[1m 1/19\u001b[0m \u001b[32mâ”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9375 - loss: 0.1107\n","Epoch 10: val_accuracy did not improve from 0.85271\n","\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9559 - loss: 0.1093 - val_accuracy: 0.4031 - val_loss: 0.8596\n","Epoch 11/100\n","\u001b[1m 1/19\u001b[0m \u001b[32mâ”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9688 - loss: 0.0891\n","Epoch 11: val_accuracy did not improve from 0.85271\n","\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9487 - loss: 0.1343 - val_accuracy: 0.4574 - val_loss: 0.7172\n","Epoch 12/100\n","\u001b[1m 1/19\u001b[0m \u001b[32mâ”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.0236\n","Epoch 12: val_accuracy did not improve from 0.85271\n","\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9726 - loss: 0.0765 - val_accuracy: 0.6202 - val_loss: 0.5924\n","Epoch 13/100\n","\u001b[1m 1/19\u001b[0m \u001b[32mâ”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9688 - loss: 0.0788\n","Epoch 13: val_accuracy did not improve from 0.85271\n","\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9614 - loss: 0.1057 - val_accuracy: 0.4729 - val_loss: 0.7368\n","Epoch 14/100\n","\u001b[1m 1/19\u001b[0m \u001b[32mâ”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9375 - loss: 0.0731\n","Epoch 14: val_accuracy did not improve from 0.85271\n","\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9742 - loss: 0.0674 - val_accuracy: 0.5504 - val_loss: 0.6484\n","Epoch 15/100\n","\u001b[1m 1/19\u001b[0m \u001b[32mâ”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9688 - loss: 0.0802\n","Epoch 15: val_accuracy did not improve from 0.85271\n","\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9738 - loss: 0.0876 - val_accuracy: 0.6589 - val_loss: 0.5569\n","Epoch 16/100\n","\u001b[1m 1/19\u001b[0m \u001b[32mâ”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9688 - loss: 0.0919\n","Epoch 16: val_accuracy did not improve from 0.85271\n","\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9699 - loss: 0.1141 - val_accuracy: 0.6744 - val_loss: 0.5297\n","Epoch 17/100\n","\u001b[1m 1/19\u001b[0m \u001b[32mâ”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9375 - loss: 0.1906\n","Epoch 17: val_accuracy did not improve from 0.85271\n","\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9705 - loss: 0.0901 - val_accuracy: 0.8372 - val_loss: 0.3507\n","Epoch 18/100\n","\u001b[1m 1/19\u001b[0m \u001b[32mâ”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0150\n","Epoch 18: val_accuracy did not improve from 0.85271\n","\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9680 - loss: 0.0902 - val_accuracy: 0.8450 - val_loss: 0.3504\n","Epoch 19/100\n","\u001b[1m 1/19\u001b[0m \u001b[32mâ”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9688 - loss: 0.0619\n","Epoch 19: val_accuracy improved from 0.85271 to 0.86822, saving model to /content/drive/MyDrive/hand_project/Final_code/skeleton_pinch_v3.keras\n","\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9695 - loss: 0.0622 - val_accuracy: 0.8682 - val_loss: 0.2905\n","Epoch 20/100\n","\u001b[1m 1/19\u001b[0m \u001b[32mâ”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0165\n","Epoch 20: val_accuracy did not improve from 0.86822\n","\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9831 - loss: 0.0619 - val_accuracy: 0.8605 - val_loss: 0.2955\n","Epoch 21/100\n","\u001b[1m 1/19\u001b[0m \u001b[32mâ”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9375 - loss: 0.1430\n","Epoch 21: val_accuracy improved from 0.86822 to 0.88372, saving model to /content/drive/MyDrive/hand_project/Final_code/skeleton_pinch_v3.keras\n","\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9563 - loss: 0.0969 - val_accuracy: 0.8837 - val_loss: 0.2526\n","Epoch 22/100\n","\u001b[1m 1/19\u001b[0m \u001b[32mâ”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.0375\n","Epoch 22: val_accuracy improved from 0.88372 to 0.90698, saving model to /content/drive/MyDrive/hand_project/Final_code/skeleton_pinch_v3.keras\n","\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9769 - loss: 0.0745 - val_accuracy: 0.9070 - val_loss: 0.2105\n","Epoch 23/100\n","\u001b[1m 1/19\u001b[0m \u001b[32mâ”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9688 - loss: 0.0555\n","Epoch 23: val_accuracy improved from 0.90698 to 0.92248, saving model to /content/drive/MyDrive/hand_project/Final_code/skeleton_pinch_v3.keras\n","\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9774 - loss: 0.0809 - val_accuracy: 0.9225 - val_loss: 0.1691\n","Epoch 24/100\n","\u001b[1m16/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9704 - loss: 0.0801 \n","Epoch 24: val_accuracy improved from 0.92248 to 0.96899, saving model to /content/drive/MyDrive/hand_project/Final_code/skeleton_pinch_v3.keras\n","\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9709 - loss: 0.0795 - val_accuracy: 0.9690 - val_loss: 0.1078\n","Epoch 25/100\n","\u001b[1m 1/19\u001b[0m \u001b[32mâ”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0334\n","Epoch 25: val_accuracy did not improve from 0.96899\n","\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9745 - loss: 0.0767 - val_accuracy: 0.9690 - val_loss: 0.0917\n","Epoch 26/100\n","\u001b[1m 1/19\u001b[0m \u001b[32mâ”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9375 - loss: 0.1544\n","Epoch 26: val_accuracy did not improve from 0.96899\n","\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9672 - loss: 0.0794 - val_accuracy: 0.9690 - val_loss: 0.1014\n","Epoch 27/100\n","\u001b[1m 1/19\u001b[0m \u001b[32mâ”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9688 - loss: 0.0856\n","Epoch 27: val_accuracy did not improve from 0.96899\n","\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9800 - loss: 0.0558 - val_accuracy: 0.9535 - val_loss: 0.1182\n","Epoch 28/100\n","\u001b[1m 1/19\u001b[0m \u001b[32mâ”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.0325\n","Epoch 28: val_accuracy did not improve from 0.96899\n","\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9897 - loss: 0.0440 - val_accuracy: 0.9457 - val_loss: 0.1094\n","Epoch 29/100\n","\u001b[1m 1/19\u001b[0m \u001b[32mâ”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0111\n","Epoch 29: val_accuracy did not improve from 0.96899\n","\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9709 - loss: 0.0842 - val_accuracy: 0.9612 - val_loss: 0.1127\n","Epoch 30/100\n","\u001b[1m 1/19\u001b[0m \u001b[32mâ”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0153\n","Epoch 30: val_accuracy did not improve from 0.96899\n","\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9854 - loss: 0.0410 - val_accuracy: 0.9457 - val_loss: 0.0750\n","Epoch 31/100\n","\u001b[1m 1/19\u001b[0m \u001b[32mâ”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m3s\u001b[0m 185ms/step - accuracy: 1.0000 - loss: 0.0155\n","Epoch 31: val_accuracy did not improve from 0.96899\n","\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9839 - loss: 0.0528 - val_accuracy: 0.9535 - val_loss: 0.0929\n","Epoch 32/100\n","\u001b[1m 1/19\u001b[0m \u001b[32mâ”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0067\n","Epoch 32: val_accuracy did not improve from 0.96899\n","\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9807 - loss: 0.0427 - val_accuracy: 0.8992 - val_loss: 0.1674\n","Epoch 33/100\n","\u001b[1m 1/19\u001b[0m \u001b[32mâ”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0349\n","Epoch 33: val_accuracy did not improve from 0.96899\n","\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9922 - loss: 0.0437 - val_accuracy: 0.9690 - val_loss: 0.0914\n","Epoch 34/100\n","\u001b[1m 1/19\u001b[0m \u001b[32mâ”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0077\n","Epoch 34: val_accuracy did not improve from 0.96899\n","\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9810 - loss: 0.0486 - val_accuracy: 0.9612 - val_loss: 0.0798\n","Epoch 35/100\n","\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9903 - loss: 0.0348 \n","Epoch 35: val_accuracy did not improve from 0.96899\n","\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9900 - loss: 0.0352 - val_accuracy: 0.9535 - val_loss: 0.0790\n","Epoch 36/100\n","\u001b[1m 1/19\u001b[0m \u001b[32mâ”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0170\n","Epoch 36: val_accuracy improved from 0.96899 to 0.97674, saving model to /content/drive/MyDrive/hand_project/Final_code/skeleton_pinch_v3.keras\n","\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9854 - loss: 0.0487 - val_accuracy: 0.9767 - val_loss: 0.0873\n","Epoch 37/100\n","\u001b[1m 1/19\u001b[0m \u001b[32mâ”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0143\n","Epoch 37: val_accuracy did not improve from 0.97674\n","\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9787 - loss: 0.0611 - val_accuracy: 0.9767 - val_loss: 0.0838\n","Epoch 38/100\n","\u001b[1m 1/19\u001b[0m \u001b[32mâ”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0280\n","Epoch 38: val_accuracy did not improve from 0.97674\n","\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9849 - loss: 0.0519 - val_accuracy: 0.9535 - val_loss: 0.1362\n","Epoch 39/100\n","\u001b[1m 1/19\u001b[0m \u001b[32mâ”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9062 - loss: 0.1720\n","Epoch 39: val_accuracy did not improve from 0.97674\n","\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9571 - loss: 0.0977 - val_accuracy: 0.9457 - val_loss: 0.1201\n","Epoch 40/100\n","\u001b[1m 1/19\u001b[0m \u001b[32mâ”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0118\n","Epoch 40: val_accuracy did not improve from 0.97674\n","\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9939 - loss: 0.0409 - val_accuracy: 0.9690 - val_loss: 0.1139\n","\n","ğŸ‰ ëª¨ë¸ V3 í•™ìŠµ ë° ì €ì¥ ì™„ë£Œ: /content/drive/MyDrive/hand_project/Final_code/skeleton_pinch_v3.keras\n"]}]},{"cell_type":"markdown","source":["F1 ìŠ¤ì½”ì–´ ê³„ì‚°"],"metadata":{"id":"jx4wBbPp5NBH"}},{"cell_type":"code","source":["# ==========================================\n","# 3. ëª¨ë¸ ë¡œë“œ ë° ìµœì¢… ì„±ëŠ¥ í‰ê°€ (í˜¼ë™ í–‰ë ¬ í¬í•¨)\n","# ==========================================\n","import numpy as np\n","import os\n","from tensorflow.keras.models import load_model\n","# ğŸš¨ğŸš¨ í˜¼ë™ í–‰ë ¬ ê³„ì‚°ì„ ìœ„í•´ confusion_matrix ì„í¬íŠ¸\n","from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n","\n","# ğŸš¨ğŸš¨ íŒŒì¼ ê²½ë¡œ ì„¤ì • (2ë‹¨ê³„ì—ì„œ ì €ì¥í•œ ê²½ë¡œì™€ ë™ì¼í•´ì•¼ í•©ë‹ˆë‹¤)\n","LOAD_MODEL_PATH = '/content/drive/MyDrive/hand_project/Final_code/skeleton_pinch_v3.keras'\n","X_TEST_PATH = '/content/drive/MyDrive/hand_project/Final_code/X_test_v3.npy'\n","Y_TEST_PATH = '/content/drive/MyDrive/hand_project/Final_code/Y_test_v3.npy'\n","\n","# --- 3.1. ëª¨ë¸ ë¡œë“œ ---\n","loaded_model = None\n","try:\n","    loaded_model = load_model(LOAD_MODEL_PATH)\n","except Exception as e:\n","    print(f\"âš ï¸ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {e}\")\n","    exit()\n","\n","# --- 3.2. í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ---\n","X_test = None\n","Y_test = None\n","try:\n","    X_test = np.load(X_TEST_PATH)\n","    Y_test = np.load(Y_TEST_PATH)\n","except FileNotFoundError:\n","    print(\"âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨. 2ë‹¨ê³„ë¥¼ ë¨¼ì € ì™„ë£Œí•˜ì„¸ìš”.\")\n","    exit()\n","\n","# ==========================================\n","# 4. ëª¨ë¸ V3 ì„±ëŠ¥ í‰ê°€ (Loss, Accuracy ë° F1 ìŠ¤ì½”ì–´)\n","# ==========================================\n","print(\"\\nğŸš€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì— ëŒ€í•œ ëª¨ë¸ V3 ì„±ëŠ¥ í‰ê°€ ì‹œì‘...\")\n","\n","# 1. Keras evaluateë¥¼ í†µí•œ Lossì™€ Accuracy ê³„ì‚°\n","loss, accuracy = loaded_model.evaluate(X_test, Y_test, verbose=0)\n","\n","# 2. ëª¨ë¸ ì˜ˆì¸¡ (F1 ìŠ¤ì½”ì–´ ê³„ì‚°ì„ ìœ„í•¨)\n","Y_prob = loaded_model.predict(X_test)\n","Y_pred = (Y_prob > 0.5).astype(int)\n","\n","# 3. F1, Precision, Recall ê³„ì‚°\n","try:\n","    f1 = f1_score(Y_test, Y_pred, pos_label=1)\n","    precision = precision_score(Y_test, Y_pred, pos_label=1)\n","    recall = recall_score(Y_test, Y_pred, pos_label=1)\n","\n","    # ğŸš¨ğŸš¨ NEW: Confusion Matrix ê³„ì‚°\n","    cm = confusion_matrix(Y_test, Y_pred)\n","    tn, fp, fn, tp = cm.ravel() # TN, FP, FN, TP ê°’ì„ ì¶”ì¶œ\n","\n","except ValueError as e:\n","    print(f\"âŒ ì„±ëŠ¥ ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n","    exit()\n","\n","# 4. ê²°ê³¼ ì¶œë ¥\n","print(\"\\n==============================================\")\n","print(f\"ğŸ“Š ëª¨ë¸ V3 ìµœì¢… ì„±ëŠ¥ í‰ê°€ ê²°ê³¼ (ì´ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ: {len(X_test)})\")\n","print(\"==============================================\")\n","print(f\"ì†ì‹¤ (Loss):          {loss:.4f}\")\n","print(f\"ì •í™•ë„ (Accuracy):    {accuracy:.4f}\")\n","print(\"----------------------------------------------\")\n","print(f\"ì •ë°€ë„ (Precision): {precision:.4f}\")\n","print(f\"ì¬í˜„ìœ¨ (Recall):   {recall:.4f}\")\n","print(f\"F1 ìŠ¤ì½”ì–´:          {f1:.4f}\")\n","print(\"----------------------------------------------\")\n","print(\"\\n--- í˜¼ë™ í–‰ë ¬ (Confusion Matrix) ---\")\n","print(\"   [ì‹¤ì œ NonPinch] [ì‹¤ì œ Pinch]\")\n","print(f\"NonPinch ì˜ˆì¸¡: [{tn}            {fp}          ] (TN, FP)\")\n","print(f\"Pinch ì˜ˆì¸¡:    [{fn}            {tp}          ] (FN, TP)\")\n","print(\"\\n----------------------------------------------\")\n","print(\"  TN (True Negative): ë¹„ê¼¬ì§‘ê¸°ë¥¼ ë¹„ê¼¬ì§‘ê¸°ë¡œ ë§ê²Œ ì˜ˆì¸¡í•œ ìˆ˜\")\n","print(\"  FP (False Positive): ë¹„ê¼¬ì§‘ê¸°ë¥¼ ê¼¬ì§‘ê¸°ë¡œ ì˜ëª» ì˜ˆì¸¡í•œ ìˆ˜ (ì˜¤íƒì§€)\")\n","print(\"  FN (False Negative): ê¼¬ì§‘ê¸°ë¥¼ ë¹„ê¼¬ì§‘ê¸°ë¡œ ì˜ëª» ì˜ˆì¸¡í•œ ìˆ˜ (ë†“ì¹œ ìˆ˜)\")\n","print(\"  TP (True Positive): ê¼¬ì§‘ê¸°ë¥¼ ê¼¬ì§‘ê¸°ë¡œ ë§ê²Œ ì˜ˆì¸¡í•œ ìˆ˜\")\n","print(\"==============================================\")"],"metadata":{"collapsed":true,"id":"DKs712kNLeXj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764552842310,"user_tz":-540,"elapsed":6670,"user":{"displayName":"ì½”ì½”ëª½","userId":"10518974845406570474"}},"outputId":"60e4c3a2-77d0-4019-af6d-64d6e92e401e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","ğŸš€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì— ëŒ€í•œ ëª¨ë¸ V3 ì„±ëŠ¥ í‰ê°€ ì‹œì‘...\n","\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 247ms/step\n","\n","==============================================\n","ğŸ“Š ëª¨ë¸ V3 ìµœì¢… ì„±ëŠ¥ í‰ê°€ ê²°ê³¼ (ì´ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ: 129)\n","==============================================\n","ì†ì‹¤ (Loss):          0.1784\n","ì •í™•ë„ (Accuracy):    0.9457\n","----------------------------------------------\n","ì •ë°€ë„ (Precision): 0.9231\n","ì¬í˜„ìœ¨ (Recall):   0.9412\n","F1 ìŠ¤ì½”ì–´:          0.9320\n","----------------------------------------------\n","\n","--- í˜¼ë™ í–‰ë ¬ (Confusion Matrix) ---\n","   [ì‹¤ì œ NonPinch] [ì‹¤ì œ Pinch]\n","NonPinch ì˜ˆì¸¡: [74            4          ] (TN, FP)\n","Pinch ì˜ˆì¸¡:    [3            48          ] (FN, TP)\n","\n","----------------------------------------------\n","  TN (True Negative): ë¹„ê¼¬ì§‘ê¸°ë¥¼ ë¹„ê¼¬ì§‘ê¸°ë¡œ ë§ê²Œ ì˜ˆì¸¡í•œ ìˆ˜\n","  FP (False Positive): ë¹„ê¼¬ì§‘ê¸°ë¥¼ ê¼¬ì§‘ê¸°ë¡œ ì˜ëª» ì˜ˆì¸¡í•œ ìˆ˜ (ì˜¤íƒì§€)\n","  FN (False Negative): ê¼¬ì§‘ê¸°ë¥¼ ë¹„ê¼¬ì§‘ê¸°ë¡œ ì˜ëª» ì˜ˆì¸¡í•œ ìˆ˜ (ë†“ì¹œ ìˆ˜)\n","  TP (True Positive): ê¼¬ì§‘ê¸°ë¥¼ ê¼¬ì§‘ê¸°ë¡œ ë§ê²Œ ì˜ˆì¸¡í•œ ìˆ˜\n","==============================================\n"]}]},{"cell_type":"markdown","source":["# **ì½”ë“œ í•©ì²´**"],"metadata":{"id":"CtuGa0rPOiBJ"}},{"cell_type":"code","source":["# ==========================================\n","# 3-1. ëª¨ë¸ ë¡œë“œ ë° ìƒìˆ˜ ì •ì˜ (Fusion ì‹¤í–‰ ì¤€ë¹„)\n","# (CLS_ID_FOR_PERSON ì •ì˜ ì¶”ê°€ ë° WEIGHT_DNN/STGCN 0.5:0.5 ì ìš©)\n","# ==========================================\n","import os # os ì„í¬íŠ¸ (ì½”ë© í™˜ê²½ì— ë”°ë¼ í•„ìš”í•  ìˆ˜ ìˆìŒ)\n","import torch # torch ì„í¬íŠ¸ (STGCN ë¡œë“œ ì‹œ í•„ìš”)\n","from tensorflow.keras.models import load_model # load_model ì„í¬íŠ¸\n","from ultralytics import YOLO # YOLO ì„í¬íŠ¸\n","import mediapipe as mp # mediapipe ì„í¬íŠ¸\n","\n","# ğŸš¨ğŸš¨ğŸš¨ ê²½ë¡œ ì„¤ì • ë° ìƒìˆ˜ ì •ì˜ ğŸš¨ğŸš¨ğŸš¨\n","MODEL_SAVE_DIR = '/content/drive/MyDrive/hand_project/Final_code'\n","STGCN_MODEL_DIR = '/content/drive/MyDrive/Pinching_data/stgcn/'\n","LOAD_MODEL_PATH = os.path.join(MODEL_SAVE_DIR, 'skeleton_pinch_v3.keras')\n","STGCN_MODEL_PATH = os.path.join(STGCN_MODEL_DIR, 'best_model_adaptive.pth')\n","\n","# ----------------------------------------------------------------------\n","# ì°¸ê³ : INPUT_VIDEO_PATH ë³€ìˆ˜ëŠ” 3-A ì…€ì—ì„œ ì •ì˜ë©ë‹ˆë‹¤.\n","# ----------------------------------------------------------------------\n","\n","# Fusion ê°€ì¤‘ì¹˜ ë° ìƒìˆ˜\n","WEIGHT_DNN = 0.5\n","WEIGHT_STGCN = 0.5\n","SEQ_LEN = 30\n","NUM_CHANNELS_STGCN = 7\n","\n","# ë¶„ì„ ìƒìˆ˜ (DNN/Logicìš©)\n","DISTANCE_THRESHOLD = 0.05\n","FRAME_SKIP_INTERVAL = 3\n","MIN_CLOSING_RATE = 0.005\n","\n","# â­ ëˆ„ë½ëœ í•µì‹¬ ìƒìˆ˜ ì¶”ê°€ â­\n","# ğŸš¨ğŸš¨ ì´ ë³€ìˆ˜ëŠ” 4ë²ˆ ë¸”ë¡ ì‹¤í–‰ ì‹œ widthê°€ ì •ì˜ëœ í›„ ë‹¤ì‹œ ì‹¤í–‰ë˜ì–´ì•¼ í•©ë‹ˆë‹¤!\n","# ì„ì‹œ ì´ˆê¸°í™”: 4ë²ˆ ë¸”ë¡ ì‹¤í–‰ í›„ ì¬ì •ì˜ë˜ë„ë¡ 100ìœ¼ë¡œ ì„ì‹œ ì„¤ì •\n","width = 100 # ì„ì‹œ ì´ˆê¸°í™”\n","DISTANCE_FROM_CENTER_THRESHOLD = width * 0.15\n","width = None # ì‚¬ìš© í›„ ì´ˆê¸°í™”\n","\n","CLS_ID_FOR_HAND = 0 # YOLOv8ì—ì„œ ì†(hand)ì„ ë‚˜íƒ€ë‚´ëŠ” í´ë˜ìŠ¤ ID (ì¼ë°˜ YOLOv8nì—ì„œëŠ” ë³´í†µ 0ì´ 'person'ì´ì§€ë§Œ, hand-detection ëª¨ë¸ ì‚¬ìš©ì„ ê°€ì •)\n","CLS_ID_FOR_PERSON = 0 # ğŸš¨ NEW: YOLOv8 COCO ê¸°ì¤€ ì‚¬ëŒ í´ë˜ìŠ¤ ID (ëŒ€ë¶€ë¶„ 0)\n","SAVE_DIR = '/content/drive/MyDrive/hand_project/video_check' # ì¶œë ¥ ê²°ê³¼ ì €ì¥ ê²½ë¡œ\n","\n","# --- ëª¨ë¸ ë¡œë“œ ---\n","dnn_model = None\n","stgcn_model = None\n","\n","try:\n","    # 1. Keras DNN ëª¨ë¸ ë¡œë“œ\n","    dnn_model = load_model(LOAD_MODEL_PATH)\n","    print(f\"âœ… Pinch ëª¨ë¸ V3 (DNN) ë¡œë“œ ì„±ê³µ: {os.path.basename(LOAD_MODEL_PATH)}\")\n","\n","    # 2. PyTorch STGCN ëª¨ë¸ ë¡œë“œ\n","    # ğŸš¨ AdaptiveSTGCNê³¼ device ë³€ìˆ˜ê°€ ì•ì„  ë¸”ë¡ì—ì„œ ì •ì˜ë˜ì—ˆìŒì„ ê°€ì •\n","    stgcn_model = AdaptiveSTGCN(A=A, num_classes=2, in_channels=NUM_CHANNELS_STGCN).to(device)\n","    state_dict = torch.load(STGCN_MODEL_PATH, map_location=device)\n","\n","    if all(k.startswith('module.') for k in state_dict.keys()):\n","        state_dict = {k[7:]: v for k, v in state_dict.items()}\n","\n","    stgcn_model.load_state_dict(state_dict)\n","    stgcn_model.eval()\n","    print(f\"âœ… ST-GCN ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {os.path.basename(STGCN_MODEL_PATH)}\")\n","\n","except Exception as e:\n","    print(f\"âŒ ST-GCN ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}. Fusionì€ DNN ë‹¨ë…ìœ¼ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.\")\n","    stgcn_model = None\n","\n","# YOLO ëª¨ë¸ ë¡œë“œ\n","try:\n","    yolo_model = YOLO('yolov8n.pt')\n","    print(\"âœ… YOLOv8 ëª¨ë¸ ë¡œë“œ ì„±ê³µ!\")\n","except Exception as e:\n","    print(f\"âŒ YOLOv8 ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n","    exit()\n","\n","# MediaPipe ì´ˆê¸°í™” ë° ê²½ë¡œ ì„¤ì •\n","mp_hands = mp.solutions.hands\n","mp_drawing = mp.solutions.drawing_utils\n","mp_drawing_styles = mp.solutions.drawing_styles\n","hands = mp_hands.Hands(\n","    static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5, min_tracking_confidence=0.5\n",")\n","CAPTURED_FRAMES_DIR = '/content/drive/MyDrive/hand_project/Captured_Pinch_Frames_1'\n","os.makedirs(CAPTURED_FRAMES_DIR, exist_ok=True)\n","os.makedirs(SAVE_DIR, exist_ok=True)\n","print(f\"âœ… ìº¡ì²˜ í”„ë ˆì„ ì €ì¥ í´ë”: {CAPTURED_FRAMES_DIR}\")"],"metadata":{"id":"tj6kZVseSrp7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764556330372,"user_tz":-540,"elapsed":3172,"user":{"displayName":"ì½”ì½”ëª½","userId":"10518974845406570474"}},"outputId":"604874fb-1bce-4df5-ba0b-76fe50ffa2af"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Pinch ëª¨ë¸ V3 (DNN) ë¡œë“œ ì„±ê³µ: skeleton_pinch_v3.keras\n","âœ… ST-GCN ëª¨ë¸ ë¡œë“œ ì„±ê³µ: best_model_adaptive.pth\n","\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 6.2MB 222.0MB/s 0.0s\n","âœ… YOLOv8 ëª¨ë¸ ë¡œë“œ ì„±ê³µ!\n","âœ… ìº¡ì²˜ í”„ë ˆì„ ì €ì¥ í´ë”: /content/drive/MyDrive/hand_project/Captured_Pinch_Frames_1\n"]}]},{"cell_type":"markdown","source":["# **ì½”ë“œ í•©ì²´ í›„ í…ŒìŠ¤íŠ¸**\n","\n","*   ë“œë¼ì´ë¸Œì— ìˆëŠ” ì˜ìƒ -> input_data_pathì— ì§ì ‘ ê²½ë¡œ, íŒŒì¼ ëª… ì…ë ¥\n","*   ë…¸íŠ¸ë¶ì—ì„œ ì—…ë¡œë“œ -> ë²„íŠ¼ ëˆ„ë¥´ê³  ì—…ë¡œë“œ"],"metadata":{"id":"_0U-bok_PHN0"}},{"cell_type":"code","source":["# ==========================================\n","# 3-A. ğŸ“¹ ì…ë ¥ ì˜ìƒ ê²½ë¡œ ì„¤ì • (íŒŒì¼ ì—…ë¡œë“œ)\n","# ==========================================\n","from google.colab import files\n","\n","# ğŸš¨ INPUT_VIDEO_PATH ë³€ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n","INPUT_VIDEO_PATH = \"\"\n","\n","print(\"ğŸ“¹ ë¶„ì„í•  ë¹„ë””ì˜¤ íŒŒì¼(.mp4 ë˜ëŠ” .MOV)ì„ ì—…ë¡œë“œí•˜ê±°ë‚˜ Drive ê²½ë¡œë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì…ë ¥í•˜ì„¸ìš”.\")\n","print(\"  (Drive ê²½ë¡œë¥¼ ì‚¬ìš©í•  ê²½ìš°, ì•„ë˜ ì½”ë“œ ëŒ€ì‹  ì§ì ‘ ë³€ìˆ˜ì— ê²½ë¡œë¥¼ í• ë‹¹í•´ì•¼ í•©ë‹ˆë‹¤.)\")\n","\n","# íŒŒì¼ ì—…ë¡œë“œ ìœ„ì ¯ì„ ì‚¬ìš©í•˜ì—¬ ê²½ë¡œë¥¼ ì–»ëŠ” ë°©ì‹\n","try:\n","    uploaded = files.upload()\n","    if uploaded:\n","        # ì—…ë¡œë“œëœ íŒŒì¼ ì´ë¦„ì„ ê²½ë¡œë¡œ ì‚¬ìš©\n","        INPUT_VIDEO_PATH = list(uploaded.keys())[0]\n","        print(f\"\\nâœ… íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ. INPUT_VIDEO_PATH = '{INPUT_VIDEO_PATH}'\")\n","    else:\n","        # ì—…ë¡œë“œí•˜ì§€ ì•Šì€ ê²½ìš°, ìˆ˜ë™ìœ¼ë¡œ Drive ê²½ë¡œë¥¼ ì…ë ¥í•´ì•¼ í•¨\n","        print(\"\\nâš ï¸ íŒŒì¼ì´ ì—…ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. INPUT_VIDEO_PATH ë³€ìˆ˜ì— ìˆ˜ë™ìœ¼ë¡œ Drive ê²½ë¡œë¥¼ í• ë‹¹í•´ì£¼ì„¸ìš”.\")\n","        # ì´ì „ì— ì‚¬ìš©í•˜ë˜ Drive ê²½ë¡œë¥¼ ì„ì‹œë¡œ ë‚¨ê²¨ë‘ê±°ë‚˜ ì‚¬ìš©ìì—ê²Œ ì…ë ¥ì„ ë°›ë„ë¡ ì•ˆë‚´\n","        INPUT_VIDEO_PATH = '/content/drive/MyDrive/hand_project/enhanced_video/MOVI0004_enhanced.mp4'\n","\n","except Exception as e:\n","    print(f\"âŒ íŒŒì¼ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n","\n","# ìµœì¢…ì ìœ¼ë¡œ INPUT_VIDEO_PATHê°€ ì •ì˜ë˜ì—ˆëŠ”ì§€ í™•ì¸\n","if not INPUT_VIDEO_PATH:\n","    # ğŸš¨ğŸš¨ ì‚¬ìš©ìì—ê²Œ ìµœì¢…ì ìœ¼ë¡œ ê²½ë¡œë¥¼ ì…ë ¥í•˜ë„ë¡ ìœ ë„í•˜ëŠ” ì½”ë“œ (ì§ì ‘ ì…ë ¥ í›„ ì‹¤í–‰)\n","    INPUT_VIDEO_PATH = input(\"/content/drive/MyDrive/video.mp4) ì—”í„°ë¥¼ ëˆ„ë¥¸ ë’¤, ì´ ì…€ì„ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”: \")\n","\n","print(f\"\\nğŸš€ ìµœì¢… ì…ë ¥ ì˜ìƒ ê²½ë¡œ: {INPUT_VIDEO_PATH}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":152},"id":"dgLv8gtcbWcc","outputId":"dccd9804-7512-402f-ccd4-0dfb249304ca","executionInfo":{"status":"ok","timestamp":1764556337357,"user_tz":-540,"elapsed":4875,"user":{"displayName":"ì½”ì½”ëª½","userId":"10518974845406570474"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ“¹ ë¶„ì„í•  ë¹„ë””ì˜¤ íŒŒì¼(.mp4 ë˜ëŠ” .MOV)ì„ ì—…ë¡œë“œí•˜ê±°ë‚˜ Drive ê²½ë¡œë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì…ë ¥í•˜ì„¸ìš”.\n","  (Drive ê²½ë¡œë¥¼ ì‚¬ìš©í•  ê²½ìš°, ì•„ë˜ ì½”ë“œ ëŒ€ì‹  ì§ì ‘ ë³€ìˆ˜ì— ê²½ë¡œë¥¼ í• ë‹¹í•´ì•¼ í•©ë‹ˆë‹¤.)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-34296ae6-3788-456c-9483-4b6b1caacb0e\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-34296ae6-3788-456c-9483-4b6b1caacb0e\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","âš ï¸ íŒŒì¼ì´ ì—…ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. INPUT_VIDEO_PATH ë³€ìˆ˜ì— ìˆ˜ë™ìœ¼ë¡œ Drive ê²½ë¡œë¥¼ í• ë‹¹í•´ì£¼ì„¸ìš”.\n","\n","ğŸš€ ìµœì¢… ì…ë ¥ ì˜ìƒ ê²½ë¡œ: /content/drive/MyDrive/hand_project/enhanced_video/MOVI0004_enhanced.mp4\n"]}]},{"cell_type":"code","source":["# ==========================================\n","# 4. ì˜ìƒ ë¶„ì„ ì‹¤í–‰ ë° Fusion ì˜ˆì¸¡ (ì €ì¥/í™”ì§ˆ ê°œì„ )\n","# ==========================================\n","\n","cap = cv2.VideoCapture(INPUT_VIDEO_PATH)\n","\n","if not cap.isOpened():\n","    print(f\"âŒ íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {INPUT_VIDEO_PATH}\")\n","    cap.release()\n","    exit()\n","\n","width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","fps = int(cap.get(cv2.CAP_PROP_FPS))\n","total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","\n","original_filename = os.path.basename(INPUT_VIDEO_PATH).split('.')[0]\n","OUTPUT_VIDEO_PATH = os.path.join(SAVE_DIR, f\"{original_filename}_V3.mp4\")\n","PREDICTIONS_SAVE_PATH = os.path.join(SAVE_DIR, f\"{original_filename}_v3_strict_predictions.npy\")\n","\n","# ğŸš¨ğŸš¨ğŸš¨ ì½”ë±/í™”ì§ˆ ê°œì„ : XVID ìœ ì§€, ë˜ëŠ” MP4Vë¡œ ë³€ê²½ ì‹œë„ (Colab í˜¸í™˜ì„± ë¬¸ì œ ìˆìŒ) ğŸš¨ğŸš¨ğŸš¨\n","# XVID: ê°€ì¥ í˜¸í™˜ì„±ì´ ì¢‹ìœ¼ë‚˜ í™”ì§ˆì´ ê¹¨ì§ˆ ìˆ˜ ìˆìŒ. (ê¾¸ë¬¼ê±°ë¦¬ëŠ” í˜„ìƒì€ ì˜ˆì¸¡ ì†ë„ ë¬¸ì œ)\n","# MP4V: XVIDë³´ë‹¤ í™”ì§ˆì´ ì¢‹ìœ¼ë‚˜ Colab í™˜ê²½ì—ì„œ ì¬ìƒì´ ì•ˆ ë  ìˆ˜ ìˆìŒ.\n","fourcc = cv2.VideoWriter_fourcc(*'XVID')\n","out = cv2.VideoWriter(OUTPUT_VIDEO_PATH, fourcc, fps, (width, height))\n","\n","if not out.isOpened():\n","    print(f\"âŒ VideoWriter ì´ˆê¸°í™” ì‹¤íŒ¨! ê²½ë¡œë¥¼ í™•ì¸í•˜ê±°ë‚˜ ì½”ë±ì„ ë³€ê²½í•˜ì„¸ìš”: {OUTPUT_VIDEO_PATH}\")\n","    # ì½”ë±ì„ MP4Vë¡œ ì¬ì‹œë„\n","    fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n","    out = cv2.VideoWriter(OUTPUT_VIDEO_PATH, fourcc, fps, (width, height))\n","    if not out.isOpened():\n","        print(\"âŒ MP4V ì½”ë±ìœ¼ë¡œë„ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ ê²½ë¡œë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”.\")\n","        cap.release()\n","        exit()\n","\n","print(f\"ğŸ¬ ì˜ìƒ ë¶„ì„ ì‹œì‘ ({total_frames} í”„ë ˆì„, Skip Rate: 1 / {FRAME_SKIP_INTERVAL})\")\n","print(f\"ğŸ’¾ ì¶œë ¥ ê²½ë¡œ í™•ì¸: {OUTPUT_VIDEO_PATH}\")\n","pbar = tqdm(total=total_frames // FRAME_SKIP_INTERVAL)\n","\n","frame_counter = 0\n","prev_dist = None\n","stgcn_sequence = []\n","all_predictions = []\n","\n","while cap.isOpened():\n","    ret, frame = cap.read()\n","    if not ret:\n","        break\n","\n","    # ì˜ìƒ ë°©í–¥ ì •ìƒí™” (ìˆ˜ì§ ë°˜ì „)\n","    #frame = cv2.flip(frame, 0)\n","\n","    frame_idx_in_video = int(cap.get(cv2.CAP_PROP_POS_FRAMES))\n","    frame_counter += 1\n","    current_frame_prediction = 0\n","\n","    if frame_counter % FRAME_SKIP_INTERVAL != 0:\n","        out.write(frame)\n","        all_predictions.append(0)\n","        continue\n","\n","    # --- ë©”ì¸ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì§„ì… ---\n","    yolo_results = yolo_model(frame, verbose=False)\n","\n","    is_object_present_in_frame = False\n","    is_hand_detected_yolo = False\n","    current_keypoints_frame = np.zeros((2, NUM_JOINTS, 4), dtype=np.float32)\n","    final_kp_dnn = np.zeros(126)\n","\n","    cls_id_for_hand = CLS_ID_FOR_HAND # CLS_ID_FOR_HAND ì‚¬ìš©\n","    hand_boxes = []\n","\n","    for r in yolo_results:\n","        for i, cls_id in enumerate(r.boxes.cls.cpu().numpy().astype(int)):\n","            box = r.boxes.xyxy.cpu().numpy().astype(int)[i]\n","\n","            if cls_id == cls_id_for_hand:\n","                hand_boxes.append(box)\n","                is_hand_detected_yolo = True\n","            elif cls_id != 0:\n","                 is_object_present_in_frame = True\n","\n","    if not is_hand_detected_yolo:\n","        cv2.putText(frame, \"No Hand Detected (YOLO)\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n","        out.write(frame)\n","        all_predictions.append(0)\n","        pbar.update(1)\n","        continue\n","\n","    is_hand_detected_mediapipe = False\n","\n","    for box in hand_boxes:\n","        x1, y1, x2, y2 = box\n","        cropped_img = frame[y1:y2, x1:x2]\n","        if cropped_img.size == 0 or x2 <= x1 or y2 <= y1:\n","            continue\n","\n","        mp_results = hands.process(cv2.cvtColor(cropped_img, cv2.COLOR_BGR2RGB))\n","\n","        if mp_results.multi_hand_landmarks:\n","            is_hand_detected_mediapipe = True\n","\n","            for hand_idx, hand_landmarks in enumerate(mp_results.multi_hand_landmarks):\n","                if hand_idx >= 2: continue\n","\n","                kp_list_dnn = []\n","                for lm in hand_landmarks.landmark:\n","                    kp_list_dnn.extend([lm.x, lm.y, lm.z])\n","\n","                if hand_idx == 0:\n","                    final_kp_dnn[:len(kp_list_dnn)] = kp_list_dnn[:126]\n","\n","                lm_xyz = np.array([[lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark])\n","                thumb_tip = lm_xyz[4]\n","                index_tip = lm_xyz[8]\n","                dist = np.linalg.norm(thumb_tip - index_tip)\n","\n","                if dist < 0.25:\n","                    modified_dist = dist * 0.2\n","                else:\n","                    modified_dist = dist\n","\n","                dist_feat = np.full((NUM_JOINTS, 1), modified_dist)\n","                hand_kp_extended = np.concatenate((lm_xyz, dist_feat), axis=1)\n","                current_keypoints_frame[hand_idx] = hand_kp_extended\n","\n","                transformed_hand_landmarks = landmark_pb2.NormalizedLandmarkList()\n","                transformed_hand_landmarks.landmark.extend([\n","                    landmark_pb2.NormalizedLandmark(\n","                        x=(lm.x * (x2 - x1) + x1) / width,\n","                        y=(lm.y * (y2 - y1) + y1) / height,\n","                        z=lm.z\n","                    ) for lm in hand_landmarks.landmark\n","                ])\n","\n","                mp_drawing.draw_landmarks(frame, transformed_hand_landmarks, mp_hands.HAND_CONNECTIONS,\n","                                          mp_drawing_styles.get_default_hand_landmarks_style(),\n","                                          mp_drawing_styles.get_default_hand_connections_style())\n","\n","            if is_hand_detected_mediapipe:\n","                prev_dist = dist\n","\n","            final_prob = 0.0\n","            fusion_reason = \"DNN Only\"\n","            ai_prob_dnn = dnn_model.predict(final_kp_dnn.reshape(1, -1), verbose=0)[0][0]\n","\n","            if stgcn_model is None:\n","                final_prob = ai_prob_dnn\n","                fusion_reason = \"DNN Only (STGCN Fail)\"\n","            else:\n","                stgcn_sequence.append(current_keypoints_frame)\n","                if len(stgcn_sequence) > SEQ_LEN:\n","                    stgcn_sequence.pop(0)\n","\n","                if len(stgcn_sequence) == SEQ_LEN:\n","                    seq_arr = np.array(stgcn_sequence, dtype=np.float32)\n","                    seq_arr = seq_arr.reshape(SEQ_LEN, 2, NUM_JOINTS, 4)\n","                    input_data = np.transpose(seq_arr, (3, 0, 2, 1))\n","                    input_data = np.expand_dims(input_data, axis=0)\n","\n","                    wrist_id = 0\n","                    for m in range(2):\n","                        wrist_xyz = input_data[0, :3, :, wrist_id, m].copy()\n","                        input_data[0, :3, :, :, m] -= wrist_xyz[:, :, None]\n","\n","                    max_val = np.max(np.abs(input_data))\n","                    if max_val > 0:\n","                        input_data /= max_val\n","\n","                    velocity = np.zeros((1, 3, SEQ_LEN, NUM_JOINTS, 2), dtype=np.float32)\n","                    velocity[:, :, 1:, :, :] = input_data[:, :3, 1:, :, :] - input_data[:, :3, :-1, :, :]\n","                    final_input = np.concatenate((input_data, velocity), axis=1)\n","\n","                    input_tensor = torch.tensor(final_input, dtype=torch.float32).to(device)\n","\n","                    with torch.no_grad():\n","                        outputs = stgcn_model(input_tensor)\n","                        probabilities = F.softmax(outputs, dim=1)\n","                        prob_stgcn = probabilities[0, 1].item()\n","\n","                    final_prob = (ai_prob_dnn * WEIGHT_DNN) + (prob_stgcn * WEIGHT_STGCN)\n","                    fusion_reason = f\"Fusion ({WEIGHT_DNN:.1f}:{WEIGHT_STGCN:.1f})\"\n","                else:\n","                    final_prob = ai_prob_dnn\n","                    fusion_reason = \"DNN Only (Buffering)\"\n","\n","            is_closing = False\n","            if prev_dist is not None and (prev_dist - dist) > MIN_CLOSING_RATE:\n","                is_closing = True\n","\n","            label = \"Non-Pinch\"\n","            color = (255, 0, 0)\n","            reason_text = \"Non-Pinch\"\n","            current_frame_prediction = 0\n","\n","            if final_prob > 0.5:\n","                label = f\"FINAL: PINCH ({final_prob*100:.1f}%)\"\n","                color = (0, 255, 0)\n","                reason_text = fusion_reason\n","                current_frame_prediction = 1\n","\n","            elif dist < DISTANCE_THRESHOLD and is_closing and is_object_present_in_frame:\n","                label = f\"FINAL: PINCH (Logic)\"\n","                color = (0, 255, 255)\n","                reason_text = f\"Closing + Object\"\n","                current_frame_prediction = 1\n","\n","            display_text = f\"{label} ({reason_text})\"\n","            cv2.rectangle(frame, (x1, y2 - 40), (x1 + 550, y2), color, -1)\n","            cv2.putText(frame, display_text, (x1 + 10, y2 - 15), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2)\n","\n","    out.write(frame)\n","    pbar.update(1)\n","    all_predictions.append(current_frame_prediction)\n","\n","cap.release()\n","out.release()\n","pbar.close()\n","\n","np.save(PREDICTIONS_SAVE_PATH, np.array(all_predictions))\n","\n","print(f\"\\nâœ¨ ë¶„ì„ ì™„ë£Œ! ê²°ê³¼ íŒŒì¼: {OUTPUT_VIDEO_PATH}\")\n","print(f\"âœ… Fusion ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {PREDICTIONS_SAVE_PATH}\")"],"metadata":{"collapsed":true,"id":"OpXUop12PG7w","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764553371795,"user_tz":-540,"elapsed":105721,"user":{"displayName":"ì½”ì½”ëª½","userId":"10518974845406570474"}},"outputId":"97b5e1a4-6595-4498-cbd2-c39187517348"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ¬ ì˜ìƒ ë¶„ì„ ì‹œì‘ (397 í”„ë ˆì„, Skip Rate: 1 / 3)\n","ğŸ’¾ ì¶œë ¥ ê²½ë¡œ í™•ì¸: /content/drive/MyDrive/hand_project/video_check/MOVI0004_enhanced_V3.mp4\n"]},{"output_type":"stream","name":"stderr","text":["  2%|â–         | 2/132 [01:06<1:11:31, 33.01s/it]\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 132/132 [01:45<00:00,  1.25it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","âœ¨ ë¶„ì„ ì™„ë£Œ! ê²°ê³¼ íŒŒì¼: /content/drive/MyDrive/hand_project/video_check/MOVI0004_enhanced_V3.mp4\n","âœ… Fusion ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: /content/drive/MyDrive/hand_project/video_check/MOVI0004_enhanced_v3_strict_predictions.npy\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["ìˆ˜í‰ë°˜ì „"],"metadata":{"id":"17MgOYsaUWxn"}},{"cell_type":"code","source":["# ==========================================\n","# 4. ì˜ìƒ ë¶„ì„ ì‹¤í–‰ ë° Fusion ì˜ˆì¸¡ (ì €ì¥/í™”ì§ˆ ê°œì„ )\n","# (ìˆ˜í‰ ë°˜ì „ ë° Pinch í”„ë ˆì„ ìº¡ì²˜ ë¡œì§ í¬í•¨)\n","# ==========================================\n","\n","cap = cv2.VideoCapture(INPUT_VIDEO_PATH)\n","\n","if not cap.isOpened():\n","    print(f\"âŒ íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {INPUT_VIDEO_PATH}\")\n","    cap.release()\n","    exit()\n","\n","width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","fps = int(cap.get(cv2.CAP_PROP_FPS))\n","total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","\n","original_filename = os.path.basename(INPUT_VIDEO_PATH).split('.')[0]\n","OUTPUT_VIDEO_PATH = os.path.join(SAVE_DIR, f\"{original_filename}_V3_2.mp4\")\n","PREDICTIONS_SAVE_PATH = os.path.join(SAVE_DIR, f\"{original_filename}_v3_strict_predictions.npy\")\n","\n","# ğŸš¨ğŸš¨ğŸš¨ ì½”ë±/í™”ì§ˆ ê°œì„ : XVID ìœ ì§€\n","fourcc = cv2.VideoWriter_fourcc(*'XVID')\n","out = cv2.VideoWriter(OUTPUT_VIDEO_PATH, fourcc, fps, (width, height))\n","\n","if not out.isOpened():\n","    print(f\"âŒ VideoWriter ì´ˆê¸°í™” ì‹¤íŒ¨! ê²½ë¡œë¥¼ í™•ì¸í•˜ê±°ë‚˜ ì½”ë±ì„ ë³€ê²½í•˜ì„¸ìš”: {OUTPUT_VIDEO_PATH}\")\n","    # ì½”ë±ì„ MP4Vë¡œ ì¬ì‹œë„\n","    fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n","    out = cv2.VideoWriter(OUTPUT_VIDEO_PATH, fourcc, fps, (width, height))\n","    if not out.isOpened():\n","        print(\"âŒ MP4V ì½”ë±ìœ¼ë¡œë„ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ ê²½ë¡œë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”.\")\n","        cap.release()\n","        exit()\n","\n","print(f\"ğŸ¬ ì˜ìƒ ë¶„ì„ ì‹œì‘ ({total_frames} í”„ë ˆì„, Skip Rate: 1 / {FRAME_SKIP_INTERVAL})\")\n","print(f\"ğŸ’¾ ì¶œë ¥ ê²½ë¡œ í™•ì¸: {OUTPUT_VIDEO_PATH}\")\n","pbar = tqdm(total=total_frames // FRAME_SKIP_INTERVAL)\n","\n","frame_counter = 0\n","prev_dist = None\n","stgcn_sequence = []\n","all_predictions = []\n","\n","while cap.isOpened():\n","    ret, frame = cap.read()\n","    if not ret:\n","        break\n","\n","    # âœ… [ìˆ˜í‰ ë°˜ì „ ì ìš©] ë¯¸ëŸ¬ë§ ë¬¸ì œì™€ ì •í™•ë„ í•˜ë½ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ìˆ˜í‰ ë°˜ì „(flip 1) ì ìš©\n","    # frame = cv2.flip(frame, 0) # ê¸°ì¡´ ìˆ˜ì§ ë°˜ì „ ì œê±°\n","    frame = cv2.flip(frame, 1) # ìˆ˜í‰ ë°˜ì „(ì¢Œìš° ë°˜ì „) ì‹œë„\n","\n","    frame_idx_in_video = int(cap.get(cv2.CAP_PROP_POS_FRAMES))\n","    frame_counter += 1\n","    current_frame_prediction = 0\n","\n","    if frame_counter % FRAME_SKIP_INTERVAL != 0:\n","        out.write(frame)\n","        all_predictions.append(0)\n","        continue\n","\n","    # --- ë©”ì¸ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì§„ì… ---\n","    yolo_results = yolo_model(frame, verbose=False)\n","\n","    # ğŸš¨ğŸš¨ ê°ì²´ íƒì§€ í”Œë˜ê·¸ ì´ˆê¸°í™”\n","    is_object_present_in_frame = False\n","    is_hand_detected_yolo = False\n","    current_keypoints_frame = np.zeros((2, NUM_JOINTS, 4), dtype=np.float32)\n","    final_kp_dnn = np.zeros(126)\n","\n","    cls_id_for_hand = CLS_ID_FOR_HAND # CLS_ID_FOR_HAND ì‚¬ìš©\n","    hand_boxes = []\n","\n","    for r in yolo_results:\n","        for i, cls_id in enumerate(r.boxes.cls.cpu().numpy().astype(int)):\n","            box = r.boxes.xyxy.cpu().numpy().astype(int)[i]\n","\n","            if cls_id == cls_id_for_hand:\n","                hand_boxes.append(box)\n","                is_hand_detected_yolo = True\n","            # âœ… [ê°ì²´ íƒì§€ ë¡œì§] ì†(cls_id_for_hand) ì™¸ì˜ ë‹¤ë¥¸ ê°ì²´ê°€ íƒì§€ë˜ë©´ í”Œë˜ê·¸ ì„¤ì •\n","            elif cls_id != 0:\n","                 is_object_present_in_frame = True\n","\n","    if not is_hand_detected_yolo:\n","        cv2.putText(frame, \"No Hand Detected (YOLO)\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n","        out.write(frame)\n","        all_predictions.append(0)\n","        pbar.update(1)\n","        continue\n","\n","    is_hand_detected_mediapipe = False\n","\n","    for box in hand_boxes:\n","        x1, y1, x2, y2 = box\n","        cropped_img = frame[y1:y2, x1:x2]\n","        if cropped_img.size == 0 or x2 <= x1 or y2 <= y1:\n","            continue\n","\n","        mp_results = hands.process(cv2.cvtColor(cropped_img, cv2.COLOR_BGR2RGB))\n","\n","        if mp_results.multi_hand_landmarks:\n","            is_hand_detected_mediapipe = True\n","\n","            for hand_idx, hand_landmarks in enumerate(mp_results.multi_hand_landmarks):\n","                if hand_idx >= 2: continue\n","\n","                kp_list_dnn = []\n","                for lm in hand_landmarks.landmark:\n","                    kp_list_dnn.extend([lm.x, lm.y, lm.z])\n","\n","                if hand_idx == 0:\n","                    final_kp_dnn[:len(kp_list_dnn)] = kp_list_dnn[:126]\n","\n","                lm_xyz = np.array([[lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark])\n","                thumb_tip = lm_xyz[4]\n","                index_tip = lm_xyz[8]\n","                dist = np.linalg.norm(thumb_tip - index_tip)\n","\n","                if dist < 0.25:\n","                    modified_dist = dist * 0.2\n","                else:\n","                    modified_dist = dist\n","\n","                dist_feat = np.full((NUM_JOINTS, 1), modified_dist)\n","                hand_kp_extended = np.concatenate((lm_xyz, dist_feat), axis=1)\n","                current_keypoints_frame[hand_idx] = hand_kp_extended\n","\n","                transformed_hand_landmarks = landmark_pb2.NormalizedLandmarkList()\n","                transformed_hand_landmarks.landmark.extend([\n","                    landmark_pb2.NormalizedLandmark(\n","                        x=(lm.x * (x2 - x1) + x1) / width,\n","                        y=(lm.y * (y2 - y1) + y1) / height,\n","                        z=lm.z\n","                    ) for lm in hand_landmarks.landmark\n","                ])\n","\n","                mp_drawing.draw_landmarks(frame, transformed_hand_landmarks, mp_hands.HAND_CONNECTIONS,\n","                                          mp_drawing_styles.get_default_hand_landmarks_style(),\n","                                          mp_drawing_styles.get_default_hand_connections_style())\n","\n","            if is_hand_detected_mediapipe:\n","                prev_dist = dist\n","\n","            final_prob = 0.0\n","            fusion_reason = \"DNN Only\"\n","            ai_prob_dnn = dnn_model.predict(final_kp_dnn.reshape(1, -1), verbose=0)[0][0]\n","\n","            if stgcn_model is None:\n","                final_prob = ai_prob_dnn\n","                fusion_reason = \"DNN Only (STGCN Fail)\"\n","            else:\n","                stgcn_sequence.append(current_keypoints_frame)\n","                if len(stgcn_sequence) > SEQ_LEN:\n","                    stgcn_sequence.pop(0)\n","\n","                if len(stgcn_sequence) == SEQ_LEN:\n","                    seq_arr = np.array(stgcn_sequence, dtype=np.float32)\n","                    seq_arr = seq_arr.reshape(SEQ_LEN, 2, NUM_JOINTS, 4)\n","                    input_data = np.transpose(seq_arr, (3, 0, 2, 1))\n","                    input_data = np.expand_dims(input_data, axis=0)\n","\n","                    wrist_id = 0\n","                    for m in range(2):\n","                        wrist_xyz = input_data[0, :3, :, wrist_id, m].copy()\n","                        input_data[0, :3, :, :, m] -= wrist_xyz[:, :, None]\n","\n","                    max_val = np.max(np.abs(input_data))\n","                    if max_val > 0:\n","                        input_data /= max_val\n","\n","                    velocity = np.zeros((1, 3, SEQ_LEN, NUM_JOINTS, 2), dtype=np.float32)\n","                    velocity[:, :, 1:, :, :] = input_data[:, :3, 1:, :, :] - input_data[:, :3, :-1, :, :]\n","                    final_input = np.concatenate((input_data, velocity), axis=1)\n","\n","                    input_tensor = torch.tensor(final_input, dtype=torch.float32).to(device)\n","\n","                    with torch.no_grad():\n","                        outputs = stgcn_model(input_tensor)\n","                        probabilities = F.softmax(outputs, dim=1)\n","                        prob_stgcn = probabilities[0, 1].item()\n","\n","                    final_prob = (ai_prob_dnn * WEIGHT_DNN) + (prob_stgcn * WEIGHT_STGCN)\n","                    fusion_reason = f\"Fusion ({WEIGHT_DNN:.1f}:{WEIGHT_STGCN:.1f})\"\n","                else:\n","                    final_prob = ai_prob_dnn\n","                    fusion_reason = \"DNN Only (Buffering)\"\n","\n","            is_closing = False\n","            if prev_dist is not None and (prev_dist - dist) > MIN_CLOSING_RATE:\n","                is_closing = True\n","\n","            label = \"Non-Pinch\"\n","            color = (255, 0, 0)\n","            reason_text = \"Non-Pinch\"\n","            current_frame_prediction = 0\n","\n","            # 1. AI Fusionì„ í†µí•œ ê¼¬ì§‘ê¸° íƒì§€ (Primary)\n","            if final_prob > 0.5:\n","                label = f\"FINAL: PINCH ({final_prob*100:.1f}%)\"\n","                color = (0, 255, 0)\n","                reason_text = fusion_reason\n","                current_frame_prediction = 1\n","\n","                # ìº¡ì²˜: AI Fusion\n","                frame_filename = os.path.join(\n","                    CAPTURED_FRAMES_DIR,\n","                    f\"{original_filename}_Pinch_{frame_idx_in_video:06d}.jpg\"\n","                )\n","                cv2.imwrite(frame_filename, frame)\n","\n","            # 2. Logicì„ í†µí•œ ê¼¬ì§‘ê¸° íƒì§€ (Secondary)\n","            # ğŸš¨ğŸš¨ ê°ì²´ ì ‘ì´‰ ì¡°ê±´: is_object_present_in_frame\n","            elif dist < DISTANCE_THRESHOLD and is_closing and is_object_present_in_frame:\n","                label = f\"FINAL: PINCH (Logic)\"\n","                color = (0, 255, 255)\n","                reason_text = f\"Closing + Object\"\n","                current_frame_prediction = 1\n","\n","                # ìº¡ì²˜: Logic\n","                frame_filename = os.path.join(\n","                    CAPTURED_FRAMES_DIR,\n","                    f\"{original_filename}_LogicPinch_{frame_idx_in_video:06d}.jpg\"\n","                )\n","                cv2.imwrite(frame_filename, frame)\n","\n","            display_text = f\"{label} ({reason_text})\"\n","            cv2.rectangle(frame, (x1, y2 - 40), (x1 + 550, y2), color, -1)\n","            cv2.putText(frame, display_text, (x1 + 10, y2 - 15), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2)\n","\n","    out.write(frame)\n","    pbar.update(1)\n","    all_predictions.append(current_frame_prediction)\n","\n","cap.release()\n","out.release()\n","pbar.close()\n","\n","np.save(PREDICTIONS_SAVE_PATH, np.array(all_predictions))\n","\n","print(f\"\\nâœ¨ ë¶„ì„ ì™„ë£Œ! ê²°ê³¼ íŒŒì¼: {OUTPUT_VIDEO_PATH}\")\n","print(f\"âœ… Fusion ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {PREDICTIONS_SAVE_PATH}\")\n","print(f\"ğŸ“¸ 'PINCH'ë¡œ íƒì§€ëœ ìº¡ì²˜ í”„ë ˆì„ì€ {CAPTURED_FRAMES_DIR} í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aAi0DyQeUWf4","executionInfo":{"status":"ok","timestamp":1764553543347,"user_tz":-540,"elapsed":119974,"user":{"displayName":"ì½”ì½”ëª½","userId":"10518974845406570474"}},"outputId":"455539af-eaad-4e69-fd0d-cb358fddbcf5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ¬ ì˜ìƒ ë¶„ì„ ì‹œì‘ (397 í”„ë ˆì„, Skip Rate: 1 / 3)\n","ğŸ’¾ ì¶œë ¥ ê²½ë¡œ í™•ì¸: /content/drive/MyDrive/hand_project/video_check/MOVI0004_enhanced_V3_2.mp4\n"]},{"output_type":"stream","name":"stderr","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 132/132 [01:59<00:00,  1.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","âœ¨ ë¶„ì„ ì™„ë£Œ! ê²°ê³¼ íŒŒì¼: /content/drive/MyDrive/hand_project/video_check/MOVI0004_enhanced_V3_2.mp4\n","âœ… Fusion ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: /content/drive/MyDrive/hand_project/video_check/MOVI0004_enhanced_v3_strict_predictions.npy\n","ğŸ“¸ 'PINCH'ë¡œ íƒì§€ëœ ìº¡ì²˜ í”„ë ˆì„ì€ /content/drive/MyDrive/hand_project/Captured_Pinch_Frames_1 í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"]}]},{"cell_type":"markdown","source":["ì¶”ê°€í…ŒìŠ¤íŠ¸"],"metadata":{"id":"0ZQes1RNZoXT"}},{"cell_type":"code","source":["# ==========================================\n","# 4. ì˜ìƒ ë¶„ì„ ì‹¤í–‰ ë° Fusion ì˜ˆì¸¡ (ì˜¤íƒì§€ ê°œì„  ìµœì¢…íŒ)\n","# ==========================================\n","from tqdm.notebook import tqdm\n","from mediapipe.framework.formats import landmark_pb2\n","import torch.nn.functional as F\n","\n","cap = cv2.VideoCapture(INPUT_VIDEO_PATH)\n","\n","if not cap.isOpened():\n","    print(f\"âŒ íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {INPUT_VIDEO_PATH}\")\n","    cap.release()\n","    exit()\n","\n","# width, heightëŠ” ì´ ì‹œì ì—ì„œ ì •ì˜ë©ë‹ˆë‹¤.\n","width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","fps = int(cap.get(cv2.CAP_PROP_FPS))\n","total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","\n","# ğŸš¨ widthê°€ ì •ì˜ë˜ì—ˆìœ¼ë¯€ë¡œ DISTANCE_FROM_CENTER_THRESHOLDë¥¼ ì˜¬ë°”ë¥´ê²Œ ì¬ì •ì˜í•©ë‹ˆë‹¤.\n","DISTANCE_FROM_CENTER_THRESHOLD = width * 0.15\n","\n","original_filename = os.path.basename(INPUT_VIDEO_PATH).split('.')[0]\n","OUTPUT_VIDEO_PATH = os.path.join(SAVE_DIR, f\"{original_filename}_V3_final_C06.mp4\") # íŒŒì¼ëª… ë³€ê²½\n","PREDICTIONS_SAVE_PATH = os.path.join(SAVE_DIR, f\"{original_filename}_v3_strict_predictions.npy\")\n","\n","# ğŸš¨ğŸš¨ğŸš¨ ì½”ë±/í™”ì§ˆ ê°œì„ : XVID ìœ ì§€\n","fourcc = cv2.VideoWriter_fourcc(*'XVID')\n","out = cv2.VideoWriter(OUTPUT_VIDEO_PATH, fourcc, fps, (width, height))\n","\n","if not out.isOpened():\n","    print(f\"âŒ VideoWriter ì´ˆê¸°í™” ì‹¤íŒ¨! ê²½ë¡œë¥¼ í™•ì¸í•˜ê±°ë‚˜ ì½”ë±ì„ ë³€ê²½í•˜ì„¸ìš”: {OUTPUT_VIDEO_PATH}\")\n","    # ì½”ë±ì„ MP4Vë¡œ ì¬ì‹œë„\n","    fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n","    out = cv2.VideoWriter(OUTPUT_VIDEO_PATH, fourcc, fps, (width, height))\n","    if not out.isOpened():\n","        print(\"âŒ MP4V ì½”ë±ìœ¼ë¡œë„ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ ê²½ë¡œë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”.\")\n","        cap.release()\n","        exit()\n","\n","print(f\"ğŸ¬ ì˜ìƒ ë¶„ì„ ì‹œì‘ ({total_frames} í”„ë ˆì„, Skip Rate: 1 / {FRAME_SKIP_INTERVAL})\")\n","print(f\"ğŸ’¾ ì¶œë ¥ ê²½ë¡œ í™•ì¸: {OUTPUT_VIDEO_PATH}\")\n","pbar = tqdm(total=total_frames // FRAME_SKIP_INTERVAL)\n","\n","frame_counter = 0\n","prev_dist = None\n","stgcn_sequence = []\n","all_predictions = []\n","\n","while cap.isOpened():\n","    ret, frame = cap.read()\n","    if not ret:\n","        break\n","\n","    # âœ… [ìˆ˜í‰ ë°˜ì „ ì ìš©]\n","    frame = cv2.flip(frame, 1)\n","\n","    frame_idx_in_video = int(cap.get(cv2.CAP_PROP_POS_FRAMES))\n","    frame_counter += 1\n","    current_frame_prediction = 0\n","\n","    if frame_counter % FRAME_SKIP_INTERVAL != 0:\n","        out.write(frame)\n","        all_predictions.append(0)\n","        continue\n","\n","    # --- ë©”ì¸ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì§„ì… ---\n","    # YOLO ëª¨ë¸ì„ CPUì—ì„œ ì‹¤í–‰í•˜ë„ë¡ ëª…ì‹œí•˜ì—¬ GPU ë¶€í•˜ë¥¼ ì¤„ì…ë‹ˆë‹¤.\n","    yolo_results = yolo_model(frame, verbose=False, device=\"cpu\")\n","\n","    # ğŸš¨ğŸš¨ ê°ì²´ íƒì§€ í”Œë˜ê·¸ ë° ì‚¬ëŒ ì¤‘ì‹¬ì  ì´ˆê¸°í™”\n","    is_object_present_in_frame = False\n","    is_hand_detected_yolo = False\n","    current_keypoints_frame = np.zeros((2, NUM_JOINTS, 4), dtype=np.float32)\n","    final_kp_dnn = np.zeros(126)\n","\n","    person_boxes = []\n","    hand_boxes = []\n","\n","    for r in yolo_results:\n","        for i, cls_id in enumerate(r.boxes.cls.cpu().numpy().astype(int)):\n","            box = r.boxes.xyxy.cpu().numpy().astype(int)[i]\n","            x1, y1, x2, y2 = box\n","\n","            if cls_id == CLS_ID_FOR_HAND:\n","                hand_boxes.append(box)\n","                is_hand_detected_yolo = True\n","\n","            elif cls_id == CLS_ID_FOR_PERSON:\n","                person_boxes.append(box)\n","\n","            elif cls_id != CLS_ID_FOR_PERSON and cls_id != CLS_ID_FOR_HAND:\n","                 is_object_present_in_frame = True\n","\n","    if not is_hand_detected_yolo:\n","        cv2.putText(frame, \"No Hand Detected (YOLO)\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n","        out.write(frame)\n","        all_predictions.append(0)\n","        pbar.update(1)\n","        continue\n","\n","    is_hand_detected_mediapipe = False\n","\n","    for box in hand_boxes:\n","        x1_hand, y1_hand, x2_hand, y2_hand = box\n","        cropped_img = frame[y1_hand:y2_hand, x1_hand:x2_hand]\n","        if cropped_img.size == 0 or x2_hand <= x1_hand or y2_hand <= y1_hand:\n","            continue\n","\n","        # ğŸš¨ [Logic] ì†ì´ ëª¸ì—ì„œ ë©€ë¦¬ ë–¨ì–´ì ¸ ìˆëŠ”ì§€ í™•ì¸\n","        is_far_from_body = True\n","        if person_boxes:\n","            p_x1, p_y1, p_x2, p_y2 = person_boxes[0]\n","\n","            body_center_x = (p_x1 + p_x2) / 2\n","            body_upper_half_y = p_y1 + (p_y2 - p_y1) * 0.5\n","\n","            hand_center_x = (x1_hand + x2_hand) / 2\n","            hand_center_y = (y1_hand + y2_hand) / 2\n","\n","            dist_to_body_center = np.sqrt(\n","                (hand_center_x - body_center_x)**2 +\n","                (hand_center_y - body_upper_half_y)**2\n","            )\n","\n","            if dist_to_body_center < DISTANCE_FROM_CENTER_THRESHOLD:\n","                 is_far_from_body = False\n","\n","        # --- Mediapipe ì²˜ë¦¬ ì‹œì‘ ---\n","        mp_results = hands.process(cv2.cvtColor(cropped_img, cv2.COLOR_BGR2RGB))\n","\n","        # ğŸš¨ğŸš¨ MediaPipe ì‹ ë¢°ë„(Confidence) í•„í„°ë§ ë¡œì§ ì¶”ê°€ (0.8)\n","        if mp_results.multi_hand_landmarks:\n","\n","            valid_landmarks = []\n","\n","            if mp_results.multi_handedness:\n","                for hand_idx, hand_detection in enumerate(mp_results.multi_handedness):\n","                    score = hand_detection.classification[0].score\n","\n","                    # ğŸš¨ 0.8ë¡œ ì¬ì¡°ì •: ì˜¤ë§¤í•‘ ë°©ì§€\n","                    if score > 0.8:\n","                        valid_landmarks.append(mp_results.multi_hand_landmarks[hand_idx])\n","                        is_hand_detected_mediapipe = True\n","\n","            # ìœ íš¨í•œ ëœë“œë§ˆí¬ê°€ í•˜ë‚˜ë¼ë„ ì—†ìœ¼ë©´ í˜„ì¬ ë°•ìŠ¤ ì²˜ë¦¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.\n","            if not valid_landmarks:\n","                 continue\n","\n","            for hand_idx, hand_landmarks in enumerate(valid_landmarks):\n","                if hand_idx >= 2: continue\n","\n","                kp_list_dnn = []\n","                for lm in hand_landmarks.landmark:\n","                    kp_list_dnn.extend([lm.x, lm.y, lm.z])\n","\n","                if hand_idx == 0:\n","                    final_kp_dnn[:len(kp_list_dnn)] = kp_list_dnn[:126]\n","\n","                lm_xyz = np.array([[lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark])\n","                thumb_tip = lm_xyz[4]\n","                index_tip = lm_xyz[8]\n","                dist = np.linalg.norm(thumb_tip - index_tip)\n","\n","                if dist < 0.25:\n","                    modified_dist = dist * 0.2\n","                else:\n","                    modified_dist = dist\n","\n","                dist_feat = np.full((NUM_JOINTS, 1), modified_dist)\n","                hand_kp_extended = np.concatenate((lm_xyz, dist_feat), axis=1)\n","                current_keypoints_frame[hand_idx] = hand_kp_extended\n","\n","                transformed_hand_landmarks = landmark_pb2.NormalizedLandmarkList()\n","                transformed_hand_landmarks.landmark.extend([\n","                    landmark_pb2.NormalizedLandmark(\n","                        x=(lm.x * (x2_hand - x1_hand) + x1_hand) / width,\n","                        y=(lm.y * (y2_hand - y1_hand) + y1_hand) / height,\n","                        z=lm.z\n","                    ) for lm in hand_landmarks.landmark\n","                ])\n","\n","                mp_drawing.draw_landmarks(frame, transformed_hand_landmarks, mp_hands.HAND_CONNECTIONS,\n","                                          mp_drawing_styles.get_default_hand_landmarks_style(),\n","                                          mp_drawing_styles.get_default_hand_connections_style())\n","\n","            if is_hand_detected_mediapipe:\n","                prev_dist = dist\n","\n","            final_prob = 0.0\n","            fusion_reason = \"DNN Only\"\n","            ai_prob_dnn = dnn_model.predict(final_kp_dnn.reshape(1, -1), verbose=0)[0][0]\n","\n","            if stgcn_model is None:\n","                final_prob = ai_prob_dnn\n","                fusion_reason = \"DNN Only (STGCN Fail)\"\n","            else:\n","                stgcn_sequence.append(current_keypoints_frame)\n","                if len(stgcn_sequence) > SEQ_LEN:\n","                    stgcn_sequence.pop(0)\n","\n","                if len(stgcn_sequence) == SEQ_LEN:\n","                    seq_arr = np.array(stgcn_sequence, dtype=np.float32)\n","                    seq_arr = seq_arr.reshape(SEQ_LEN, 2, NUM_JOINTS, 4)\n","                    input_data = np.transpose(seq_arr, (3, 0, 2, 1))\n","                    input_data = np.expand_dims(input_data, axis=0)\n","\n","                    wrist_id = 0\n","                    for m in range(2):\n","                        wrist_xyz = input_data[0, :3, :, wrist_id, m].copy()\n","                        input_data[0, :3, :, :, m] -= wrist_xyz[:, :, None]\n","\n","                    max_val = np.max(np.abs(input_data))\n","                    if max_val > 0:\n","                        input_data /= max_val\n","\n","                    velocity = np.zeros((1, 3, SEQ_LEN, NUM_JOINTS, 2), dtype=np.float32)\n","                    velocity[:, :, 1:, :, :] = input_data[:, :3, 1:, :, :] - input_data[:, :3, :-1, :, :]\n","                    final_input = np.concatenate((input_data, velocity), axis=1)\n","\n","                    input_tensor = torch.tensor(final_input, dtype=torch.float32).to(device)\n","\n","                    with torch.no_grad():\n","                        outputs = stgcn_model(input_tensor)\n","                        probabilities = F.softmax(outputs, dim=1)\n","                        prob_stgcn = probabilities[0, 1].item()\n","\n","                    final_prob = (ai_prob_dnn * WEIGHT_DNN) + (prob_stgcn * WEIGHT_STGCN)\n","                    fusion_reason = f\"Fusion ({WEIGHT_DNN:.1f}:{WEIGHT_STGCN:.1f})\"\n","                else:\n","                    final_prob = ai_prob_dnn\n","                    fusion_reason = \"DNN Only (Buffering)\"\n","\n","            is_closing = False\n","            # ğŸš¨ Logic ì„ê³„ê°’ì€ #3-1ì—ì„œ ì„¤ì •ëœ ê°’ MIN_CLOSING_RATE ì‚¬ìš©\n","            if prev_dist is not None and (prev_dist - dist) > MIN_CLOSING_RATE:\n","                is_closing = True\n","\n","            label = \"Non-Pinch\"\n","            color = (255, 0, 0)\n","            reason_text = \"Non-Pinch\"\n","            current_frame_prediction = 0\n","\n","            # 1. AI Fusionì„ í†µí•œ ê¼¬ì§‘ê¸° íƒì§€ (Primary)\n","            # ğŸš¨ğŸš¨ AI Fusion ì„ê³„ê°’ì„ 0.6ìœ¼ë¡œ ìƒí–¥ ì¡°ì • (ì˜¤íƒì§€ ê°ì†Œ ëª©í‘œ)\n","            if final_prob > 0.6:\n","                label = f\"FINAL: PINCH ({final_prob*100:.1f}%)\"\n","                color = (0, 255, 0)\n","                reason_text = fusion_reason\n","                current_frame_prediction = 1\n","\n","                # ìº¡ì²˜: AI Fusion (ë¡œê·¸ ì£¼ì„ ì²˜ë¦¬)\n","                frame_filename = os.path.join(\n","                    CAPTURED_FRAMES_DIR,\n","                    f\"{original_filename}_Pinch_{frame_idx_in_video:06d}.jpg\"\n","                )\n","                cv2.imwrite(frame_filename, frame)\n","\n","            # 2. Logicì„ í†µí•œ ê¼¬ì§‘ê¸° íƒì§€ (Secondary - ê°•í™”ëœ 2ë‹¨ê³„ Logic)\n","            elif is_object_present_in_frame and is_far_from_body:\n","\n","                # ê¼¬ì§‘ëŠ” ì† í¬ì¦ˆë¥¼ ê°–ì¶”ê³  ìˆëŠ”ì§€ í™•ì¸\n","                if dist < DISTANCE_THRESHOLD:\n","\n","                    # 2-1: ë™ì ì¸ ê¼¬ì§‘ê¸° (is_closing ì¡°ê±´ì´ ë§Œì¡±ë  ë•Œ)\n","                    if is_closing:\n","                        label = f\"FINAL: PINCH (Logic - Dynamic)\"\n","                        color = (0, 255, 255)\n","                        reason_text = f\"Far + Closing + Object\"\n","                        current_frame_prediction = 1\n","\n","                    # 2-2: ì •ì ì¸ ê¼¬ì§‘ê¸° (ë¬¼ì²´ ê¼¬ì§‘ê³  ê°€ë§Œíˆ ìˆëŠ” ê²½ìš° - ì •ì§€ ìƒíƒœ)\n","                    else:\n","                        label = f\"FINAL: PINCH (Logic - Static Hold)\"\n","                        color = (0, 165, 255) # ì£¼í™©ìƒ‰ (ì •ì  ìƒíƒœ)\n","                        reason_text = f\"Far + Hold + Object\"\n","                        current_frame_prediction = 1\n","\n","                # ìœ íš¨í•œ Logic íƒì§€ì—ì„œë§Œ ìº¡ì²˜ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.\n","                if current_frame_prediction == 1:\n","                    frame_filename = os.path.join(\n","                        CAPTURED_FRAMES_DIR,\n","                        f\"{original_filename}_LogicPinch_{frame_idx_in_video:06d}.jpg\"\n","                    )\n","                    cv2.imwrite(frame_filename, frame)\n","\n","\n","            display_text = f\"{label} ({reason_text})\"\n","            cv2.rectangle(frame, (x1_hand, y2_hand - 40), (x1_hand + 550, y2_hand), color, -1)\n","            cv2.putText(frame, display_text, (x1_hand + 10, y2_hand - 15), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2)\n","\n","    out.write(frame)\n","    pbar.update(1)\n","    all_predictions.append(current_frame_prediction)\n","\n","cap.release()\n","out.release()\n","pbar.close()\n","\n","np.save(PREDICTIONS_SAVE_PATH, np.array(all_predictions))\n","\n","print(f\"\\nâœ¨ ë¶„ì„ ì™„ë£Œ! ê²°ê³¼ íŒŒì¼: {OUTPUT_VIDEO_PATH}\")\n","print(f\"âœ… Fusion ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {PREDICTIONS_SAVE_PATH}\")\n","print(f\"ğŸ“¸ 'PINCH'ë¡œ íƒì§€ëœ ìº¡ì²˜ í”„ë ˆì„ì€ {CAPTURED_FRAMES_DIR} í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":159,"referenced_widgets":["559cbe04ee0e4c79a6cb24295e7ba27b","d6c13ceec2f64b838bc91a6ce99cda35","c281d37569f54bc2beb63a8bbab73066","8ccdeceb2e5e495c854e9b864cfedad5","299258b6537647248084d9aa4b039002","0476ed0f5cae4c79b89131503975e43e","6222ee79d0c94179948c29c889363338","6ed4a2d18caa43f79ac6a244cf899b56","9660b7104ff143739eea36e9ff6cc857","7bdd2ca21f9e4cb7be2d93d6414ce6a0","0e07a4520c2a4c55a7dbfdcdcbb8619d"]},"id":"AhCYoZNPZq9i","executionInfo":{"status":"ok","timestamp":1764556577932,"user_tz":-540,"elapsed":229461,"user":{"displayName":"ì½”ì½”ëª½","userId":"10518974845406570474"}},"outputId":"b084da3d-330b-4ce2-bb9c-203f2d4df984"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ¬ ì˜ìƒ ë¶„ì„ ì‹œì‘ (397 í”„ë ˆì„, Skip Rate: 1 / 3)\n","ğŸ’¾ ì¶œë ¥ ê²½ë¡œ í™•ì¸: /content/drive/MyDrive/hand_project/video_check/MOVI0004_enhanced_V3_final_C06.mp4\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/132 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"559cbe04ee0e4c79a6cb24295e7ba27b"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","âœ¨ ë¶„ì„ ì™„ë£Œ! ê²°ê³¼ íŒŒì¼: /content/drive/MyDrive/hand_project/video_check/MOVI0004_enhanced_V3_final_C06.mp4\n","âœ… Fusion ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: /content/drive/MyDrive/hand_project/video_check/MOVI0004_enhanced_v3_strict_predictions.npy\n","ğŸ“¸ 'PINCH'ë¡œ íƒì§€ëœ ìº¡ì²˜ í”„ë ˆì„ì€ /content/drive/MyDrive/hand_project/Captured_Pinch_Frames_1 í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"]}]},{"cell_type":"markdown","source":["# **í”„ë ˆì„ ê¸°ì¤€ ë¶„ì„**\n","\n","ë°˜ì „ì€ ì˜ìƒ ê¸°ì¤€ì´ë¼ ëŒë ¤ë³´ê³  ë’¤ì§‘ì–´ì ¸ìˆìŒ ìƒëµí•´ë„ ë  ë“¯"],"metadata":{"id":"np3LeG_569-c"}},{"cell_type":"code","source":["# ==========================================\n","# í”„ë ˆì„ í´ë” ê²½ë¡œ ì„¤ì •\n","# ==========================================\n","# ğŸš¨ ì´ ë³€ìˆ˜ì— ë¶„í• ëœ í”„ë ˆì„ ì´ë¯¸ì§€ê°€ ë“¤ì–´ìˆëŠ” í´ë”ì˜ ê²½ë¡œë¥¼ í• ë‹¹í•˜ì„¸ìš”.\n","# ì˜ˆì‹œ: '/content/drive/MyDrive/Pinching_data/split_frames'\n","INPUT_FRAMES_DIR = input(\"í”„ë ˆì„ ì´ë¯¸ì§€ê°€ ì €ì¥ëœ í´ë” ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”: \")\n","\n","if not os.path.isdir(INPUT_FRAMES_DIR):\n","    print(f\"âŒ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ì˜¬ë°”ë¥¸ ê²½ë¡œê°€ ì•„ë‹™ë‹ˆë‹¤: {INPUT_FRAMES_DIR}\")\n","    exit()\n","\n","# INPUT_VIDEO_PATH ë³€ìˆ˜ëŠ” OUTPUT_VIDEO_PATH ìƒì„±ì„ ìœ„í•´ ì„ì‹œë¡œ í´ë”ëª…ìœ¼ë¡œ ì„¤ì •\n","INPUT_VIDEO_PATH = INPUT_FRAMES_DIR\n","\n","print(f\"ğŸš€ ë¶„ì„í•  í”„ë ˆì„ í´ë”: {INPUT_FRAMES_DIR}\")"],"metadata":{"id":"zx3gRWWG7BqP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ==========================================\n","# 4. ì˜ìƒ ë¶„ì„ ì‹¤í–‰ ë° Fusion ì˜ˆì¸¡ (í”„ë ˆì„ í´ë” ì…ë ¥)\n","# ==========================================\n","\n","# 1. íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ë° ì •ë ¬\n","if 'INPUT_FRAMES_DIR' not in locals():\n","    print(\"âŒ INPUT_FRAMES_DIR ë³€ìˆ˜ê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 3-B ë‹¨ê³„ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.\")\n","    exit()\n","\n","frame_files = sorted([f for f in os.listdir(INPUT_FRAMES_DIR) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n","total_frames = len(frame_files)\n","\n","if total_frames == 0:\n","    print(f\"âŒ í´ë” ë‚´ì—ì„œ ìœ íš¨í•œ ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {INPUT_FRAMES_DIR}\")\n","    exit()\n","\n","# 2. ì˜ìƒ ë©”íƒ€ë°ì´í„° (ê°€ì •)\n","# FPSì™€ WIDTH/HEIGHTëŠ” ì²« ë²ˆì§¸ ì´ë¯¸ì§€ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.\n","first_frame_path = os.path.join(INPUT_FRAMES_DIR, frame_files[0])\n","first_frame = cv2.imread(first_frame_path)\n","if first_frame is None:\n","    print(\"âŒ ì²« ë²ˆì§¸ ì´ë¯¸ì§€ë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n","    exit()\n","\n","height, width, _ = first_frame.shape\n","fps = 30 # ğŸš¨ í”„ë ˆì„ ë¶„ì„ ì‹œ FPSëŠ” 30ìœ¼ë¡œ ê³ ì •í•œë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤. (ì‹¤ì œ ê°’ìœ¼ë¡œ ìˆ˜ì • ê°€ëŠ¥)\n","\n","# 3. ì¶œë ¥ ê²½ë¡œ ë° VideoWriter ì´ˆê¸°í™”\n","original_filename = os.path.basename(INPUT_FRAMES_DIR).split(os.path.sep)[-1] # í´ë” ì´ë¦„ì„ íŒŒì¼ëª…ìœ¼ë¡œ ì‚¬ìš©\n","OUTPUT_VIDEO_PATH = os.path.join(SAVE_DIR, f\"{original_filename}_FRAMES_ANALYSIS.mp4\")\n","PREDICTIONS_SAVE_PATH = os.path.join(SAVE_DIR, f\"{original_filename}_frames_predictions.npy\")\n","\n","fourcc = cv2.VideoWriter_fourcc(*'XVID')\n","out = cv2.VideoWriter(OUTPUT_VIDEO_PATH, fourcc, fps, (width, height))\n","\n","if not out.isOpened():\n","    print(f\"âŒ VideoWriter ì´ˆê¸°í™” ì‹¤íŒ¨! ê²½ë¡œë¥¼ í™•ì¸í•˜ê±°ë‚˜ ì½”ë±ì„ ë³€ê²½í•˜ì„¸ìš”: {OUTPUT_VIDEO_PATH}\")\n","    fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n","    out = cv2.VideoWriter(OUTPUT_VIDEO_PATH, fourcc, fps, (width, height))\n","    if not out.isOpened():\n","        print(\"âŒ MP4V ì½”ë±ìœ¼ë¡œë„ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ ê²½ë¡œë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”.\")\n","        exit()\n","\n","print(f\"ğŸ¬ í”„ë ˆì„ ë¶„ì„ ì‹œì‘ (ì´ {total_frames} í”„ë ˆì„, Skip Rate: 1 / {FRAME_SKIP_INTERVAL})\")\n","print(f\"ğŸ’¾ ì¶œë ¥ ê²½ë¡œ í™•ì¸: {OUTPUT_VIDEO_PATH}\")\n","pbar = tqdm(total=total_frames // FRAME_SKIP_INTERVAL)\n","\n","frame_counter = 0\n","prev_dist = None\n","stgcn_sequence = []\n","all_predictions = []\n","\n","# 4. í”„ë ˆì„ ì´ë¯¸ì§€ ë£¨í”„ ì‹œì‘\n","for filename in frame_files:\n","    frame_path = os.path.join(INPUT_FRAMES_DIR, filename)\n","    frame = cv2.imread(frame_path)\n","    if frame is None:\n","        continue # ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ê±´ë„ˆë›°ê¸°\n","\n","    # ì˜ìƒ ë°©í–¥ ì •ìƒí™” (ìˆ˜ì§ ë°˜ì „ - í•„ìš”í•˜ë‹¤ë©´)\n","    frame = cv2.flip(frame, 0)\n","\n","    frame_counter += 1\n","    current_frame_prediction = 0\n","\n","    if frame_counter % FRAME_SKIP_INTERVAL != 0:\n","        out.write(frame)\n","        all_predictions.append(0)\n","        continue\n","\n","    # --- ë©”ì¸ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì§„ì… (ì´í•˜ ê¸°ì¡´ ë¡œì§ ìœ ì§€) ---\n","    yolo_results = yolo_model(frame, verbose=False)\n","\n","    is_object_present_in_frame = False\n","    is_hand_detected_yolo = False\n","    current_keypoints_frame = np.zeros((2, NUM_JOINTS, 4), dtype=np.float32)\n","    final_kp_dnn = np.zeros(126)\n","\n","    cls_id_for_hand = CLS_ID_FOR_HAND\n","    hand_boxes = []\n","\n","    for r in yolo_results:\n","        for i, cls_id in enumerate(r.boxes.cls.cpu().numpy().astype(int)):\n","            box = r.boxes.xyxy.cpu().numpy().astype(int)[i]\n","\n","            if cls_id == cls_id_for_hand:\n","                hand_boxes.append(box)\n","                is_hand_detected_yolo = True\n","            elif cls_id != 0:\n","                 is_object_present_in_frame = True\n","\n","    if not is_hand_detected_yolo:\n","        cv2.putText(frame, \"No Hand Detected (YOLO)\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n","        out.write(frame)\n","        all_predictions.append(0)\n","        pbar.update(1)\n","        continue\n","\n","    is_hand_detected_mediapipe = False\n","\n","    for box in hand_boxes:\n","        x1, y1, x2, y2 = box\n","        cropped_img = frame[y1:y2, x1:x2]\n","        if cropped_img.size == 0 or x2 <= x1 or y2 <= y1:\n","            continue\n","\n","        mp_results = hands.process(cv2.cvtColor(cropped_img, cv2.COLOR_BGR2RGB))\n","\n","        if mp_results.multi_hand_landmarks:\n","            is_hand_detected_mediapipe = True\n","\n","            for hand_idx, hand_landmarks in enumerate(mp_results.multi_hand_landmarks):\n","                if hand_idx >= 2: continue\n","\n","                kp_list_dnn = []\n","                for lm in hand_landmarks.landmark:\n","                    kp_list_dnn.extend([lm.x, lm.y, lm.z])\n","\n","                if hand_idx == 0:\n","                    final_kp_dnn[:len(kp_list_dnn)] = kp_list_dnn[:126]\n","\n","                lm_xyz = np.array([[lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark])\n","                thumb_tip = lm_xyz[4]\n","                index_tip = lm_xyz[8]\n","                dist = np.linalg.norm(thumb_tip - index_tip)\n","\n","                if dist < 0.25:\n","                    modified_dist = dist * 0.2\n","                else:\n","                    modified_dist = dist\n","\n","                dist_feat = np.full((NUM_JOINTS, 1), modified_dist)\n","                hand_kp_extended = np.concatenate((lm_xyz, dist_feat), axis=1)\n","                current_keypoints_frame[hand_idx] = hand_kp_extended\n","\n","                transformed_hand_landmarks = landmark_pb2.NormalizedLandmarkList()\n","                transformed_hand_landmarks.landmark.extend([\n","                    landmark_pb2.NormalizedLandmark(\n","                        x=(lm.x * (x2 - x1) + x1) / width,\n","                        y=(lm.y * (y2 - y1) + y1) / height,\n","                        z=lm.z\n","                    ) for lm in hand_landmarks.landmark\n","                ])\n","\n","                mp_drawing.draw_landmarks(frame, transformed_hand_landmarks, mp_hands.HAND_CONNECTIONS,\n","                                          mp_drawing_styles.get_default_hand_landmarks_style(),\n","                                          mp_drawing_styles.get_default_hand_connections_style())\n","\n","            if is_hand_detected_mediapipe:\n","                prev_dist = dist\n","\n","            final_prob = 0.0\n","            fusion_reason = \"DNN Only\"\n","            ai_prob_dnn = dnn_model.predict(final_kp_dnn.reshape(1, -1), verbose=0)[0][0]\n","\n","            if stgcn_model is None:\n","                final_prob = ai_prob_dnn\n","                fusion_reason = \"DNN Only (STGCN Fail)\"\n","            else:\n","                stgcn_sequence.append(current_keypoints_frame)\n","                if len(stgcn_sequence) > SEQ_LEN:\n","                    stgcn_sequence.pop(0)\n","\n","                if len(stgcn_sequence) == SEQ_LEN:\n","                    seq_arr = np.array(stgcn_sequence, dtype=np.float32)\n","                    seq_arr = seq_arr.reshape(SEQ_LEN, 2, NUM_JOINTS, 4)\n","                    input_data = np.transpose(seq_arr, (3, 0, 2, 1))\n","                    input_data = np.expand_dims(input_data, axis=0)\n","\n","                    wrist_id = 0\n","                    for m in range(2):\n","                        wrist_xyz = input_data[0, :3, :, wrist_id, m].copy()\n","                        input_data[0, :3, :, :, m] -= wrist_xyz[:, :, None]\n","\n","                    max_val = np.max(np.abs(input_data))\n","                    if max_val > 0:\n","                        input_data /= max_val\n","\n","                    velocity = np.zeros((1, 3, SEQ_LEN, NUM_JOINTS, 2), dtype=np.float32)\n","                    velocity[:, :, 1:, :, :] = input_data[:, :3, 1:, :, :] - input_data[:, :3, :-1, :, :]\n","                    final_input = np.concatenate((input_data, velocity), axis=1)\n","\n","                    input_tensor = torch.tensor(final_input, dtype=torch.float32).to(device)\n","\n","                    with torch.no_grad():\n","                        outputs = stgcn_model(input_tensor)\n","                        probabilities = F.softmax(outputs, dim=1)\n","                        prob_stgcn = probabilities[0, 1].item()\n","\n","                    final_prob = (ai_prob_dnn * WEIGHT_DNN) + (prob_stgcn * WEIGHT_STGCN)\n","                    fusion_reason = f\"Fusion ({WEIGHT_DNN:.1f}:{WEIGHT_STGCN:.1f})\"\n","                else:\n","                    final_prob = ai_prob_dnn\n","                    fusion_reason = \"DNN Only (Buffering)\"\n","\n","            is_closing = False\n","            if prev_dist is not None and (prev_dist - dist) > MIN_CLOSING_RATE:\n","                is_closing = True\n","\n","            label = \"Non-Pinch\"\n","            color = (255, 0, 0)\n","            reason_text = \"Non-Pinch\"\n","            current_frame_prediction = 0\n","\n","            if final_prob > 0.5:\n","                label = f\"FINAL: PINCH ({final_prob*100:.1f}%)\"\n","                color = (0, 255, 0)\n","                reason_text = fusion_reason\n","                current_frame_prediction = 1\n","\n","            elif dist < DISTANCE_THRESHOLD and is_closing and is_object_present_in_frame:\n","                label = f\"FINAL: PINCH (Logic)\"\n","                color = (0, 255, 255)\n","                reason_text = f\"Closing + Object\"\n","                current_frame_prediction = 1\n","\n","            display_text = f\"{label} ({reason_text})\"\n","            cv2.rectangle(frame, (x1, y2 - 40), (x1 + 550, y2), color, -1)\n","            cv2.putText(frame, display_text, (x1 + 10, y2 - 15), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2)\n","\n","    out.write(frame)\n","    all_predictions.append(current_frame_prediction)\n","    pbar.update(1)\n","\n","out.release()\n","pbar.close()\n","\n","np.save(PREDICTIONS_SAVE_PATH, np.array(all_predictions))\n","\n","print(f\"\\nâœ¨ ë¶„ì„ ì™„ë£Œ! ê²°ê³¼ íŒŒì¼: {OUTPUT_VIDEO_PATH}\")\n","print(f\"âœ… Fusion ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {PREDICTIONS_SAVE_PATH}\")"],"metadata":{"id":"Wrejg_Ps7Hij"},"execution_count":null,"outputs":[]}]}